[{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"simple-data-manipulation-for-data-frames","dir":"Articles","previous_headings":"","what":"Simple data manipulation for data-frames","title":"dUtility data-frame manipulations","text":"Renaming variables, Deleting variables Looking data Making new variales analysis Making factors (groupings) Working factors Making factor existing numeric variable vice versa key data-manipulation steps data-frame typically organize data R. read data R typically data-frame, can force data-frame. basic idea utility functions get simple easy type way making simple data-manipulation data-frame much like possible SAS STATA. functions, say, dcut, dfactor functions basically base R cut, factor , easier use context data-frames additional functionality. work melanoma data already read R data-frame.","code":"library(mets) data(melanoma) is.data.frame(melanoma) #> [1] TRUE"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"dutility-functions","dir":"Articles","previous_headings":"","what":"dUtility functions","title":"dUtility data-frame manipulations","text":"structure functions dfunction(dataframe,y~x|ifcond,…) use function y dataframe grouped x condition ifcond valid. basic functions Data processing * dsort * dreshape * dcut * drm, drename, ddrop, dkeep, dsubset * drelevel * dlag * dfactor, dnumeric Data aggregation * dby, dby2 * dscalar, deval, daggregate * dmean, dsd, dsum, dquantile, dcor * dtable, dcount Data summaries * dhead, dtail, * dsummary, * dprint, dlist, dlevels, dunique generic function daggregate, daggr, can called function argument daggregate(dataframe,y~x|ifcond,fun=function,…) without grouping variable (x) daggregate(dataframe,~y|ifcond,fun=function,…) useful feature y x well subset condition can specified using regular-expressions wildcards (default). illustrate , compute means certain variables. First just oveall now days>500 now sex days>500 finally quartiles days (via dcut function) summary variables starting “s” contains “”","code":"dmean(melanoma,~thick+I(log(thick))) #>         thick I(log(thick))  #>    291.985366      5.223341 dmean(melanoma,~thick+I(log(thick))|I(days>500)) #>         thick I(log(thick))  #>    271.582011      5.168691 dmean(melanoma,thick+I(log(thick))~sex|I(days>500)) #>   sex    thick I(log(thick)) #> 1   0 242.9580      5.060086 #> 2   1 320.2429      5.353321 dmean(melanoma,thick+I(log(thick))~I(dcut(days))) #>         I(dcut(days))    thick I(log(thick)) #> 1       [10,1.52e+03] 482.1731      5.799525 #> 2    (1.52e+03,2e+03] 208.5490      4.987652 #> 3    (2e+03,3.04e+03] 223.2941      4.974759 #> 4 (3.04e+03,5.56e+03] 250.1961      5.120129 dmean(melanoma,\"s*\"+\"*a*\"~sex|I(days>500)) #>   sex   status     days #> 1   0 1.831933 2399.143 #> 2   1 1.714286 2169.800"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"renaming-deleting-keeping-dropping-variables","dir":"Articles","previous_headings":"","what":"Renaming, deleting, keeping, dropping variables","title":"dUtility data-frame manipulations","text":"Deleting variables sas style alternatively can also keep certain variables can also done direct asignment dkeep function can also used re-ordering variables data-frame","code":"melanoma=drename(melanoma,tykkelse~thick) names(melanoma) #> [1] \"no\"       \"status\"   \"days\"     \"ulc\"      \"tykkelse\" \"sex\" data(melanoma) melanoma=drm(melanoma,~thick+sex) names(melanoma) #> [1] \"no\"     \"status\" \"days\"   \"ulc\" data(melanoma) melanoma=ddrop(melanoma,~thick+sex) names(melanoma) #> [1] \"no\"     \"status\" \"days\"   \"ulc\" data(melanoma) melanoma=dkeep(melanoma,~thick+sex+status+days) names(melanoma) #> [1] \"thick\"  \"sex\"    \"status\" \"days\" data(melanoma) ddrop(melanoma) <- ~thick+sex names(melanoma) #> [1] \"no\"     \"status\" \"days\"   \"ulc\" data(melanoma) names(melanoma) #> [1] \"no\"     \"status\" \"days\"   \"ulc\"    \"thick\"  \"sex\" melanoma=dkeep(melanoma,~days+status+.) names(melanoma) #> [1] \"days\"   \"status\" \"no\"     \"ulc\"    \"thick\"  \"sex\""},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"looking-at-the-data","dir":"Articles","previous_headings":"","what":"Looking at the data","title":"dUtility data-frame manipulations","text":"data can Rstudio seen data-table list certain parts data output window Getting summaries specfic variables Summaries different groups (sex) among thin-tumours females (sex==1) make complex conditions need use () Tables variables bivariate tables univariate tables new variables","code":"data(melanoma) dstr(melanoma) #> 'data.frame':    205 obs. of  6 variables: #>  $ no    : int  789 13 97 16 21 469 685 7 932 944 ... #>  $ status: int  3 3 2 3 1 1 1 1 3 1 ... #>  $ days  : int  10 30 35 99 185 204 210 232 232 279 ... #>  $ ulc   : int  1 0 0 0 1 1 1 1 1 1 ... #>  $ thick : int  676 65 134 290 1208 484 516 1288 322 741 ... #>  $ sex   : int  1 1 1 0 1 1 1 1 0 0 ... dlist(melanoma) #>     no  status days ulc thick sex #> 1   789 3       10  1    676  1   #> 2    13 3       30  0     65  1   #> 3    97 2       35  0    134  1   #> 4    16 3       99  0    290  0   #> 5    21 1      185  1   1208  1   #> ---                               #> 201 317 2      4492 1   706   1   #> 202 798 2      4668 0   612   0   #> 203 806 2      4688 0    48   0   #> 204 606 2      4926 0   226   0   #> 205 328 2      5565 0   290   0 dlist(melanoma, ~.|sex==1) #>     no  status days ulc thick #> 1   789 3       10  1    676  #> 2    13 3       30  0     65  #> 3    97 2       35  0    134  #> 5    21 1      185  1   1208  #> 6   469 1      204  1    484  #> ---                           #> 191 445 2      3909 1   806   #> 195 415 2      4119 0    65   #> 197 175 2      4207 0    65   #> 198 493 2      4310 0   210   #> 201 317 2      4492 1   706 dlist(melanoma, ~ulc+days+thick+sex|sex==1) #>     ulc days thick sex #> 1   1    10   676  1   #> 2   0    30    65  1   #> 3   0    35   134  1   #> 5   1   185  1208  1   #> 6   1   204   484  1   #> ---                    #> 191 1   3909 806   1   #> 195 0   4119  65   1   #> 197 0   4207  65   1   #> 198 0   4310 210   1   #> 201 1   4492 706   1 dsummary(melanoma) #>        no            status          days           ulc            thick      #>  Min.   :  2.0   Min.   :1.00   Min.   :  10   Min.   :0.000   Min.   :  10   #>  1st Qu.:222.0   1st Qu.:1.00   1st Qu.:1525   1st Qu.:0.000   1st Qu.:  97   #>  Median :469.0   Median :2.00   Median :2005   Median :0.000   Median : 194   #>  Mean   :463.9   Mean   :1.79   Mean   :2153   Mean   :0.439   Mean   : 292   #>  3rd Qu.:731.0   3rd Qu.:2.00   3rd Qu.:3042   3rd Qu.:1.000   3rd Qu.: 356   #>  Max.   :992.0   Max.   :3.00   Max.   :5565   Max.   :1.000   Max.   :1742   #>       sex         #>  Min.   :0.0000   #>  1st Qu.:0.0000   #>  Median :0.0000   #>  Mean   :0.3854   #>  3rd Qu.:1.0000   #>  Max.   :1.0000 dsummary(melanoma,~thick+status+sex) #>      thick          status          sex         #>  Min.   :  10   Min.   :1.00   Min.   :0.0000   #>  1st Qu.:  97   1st Qu.:1.00   1st Qu.:0.0000   #>  Median : 194   Median :2.00   Median :0.0000   #>  Mean   : 292   Mean   :1.79   Mean   :0.3854   #>  3rd Qu.: 356   3rd Qu.:2.00   3rd Qu.:1.0000   #>  Max.   :1742   Max.   :3.00   Max.   :1.0000 dsummary(melanoma,thick+days+status~sex) #> sex: 0 #>      thick             days          status      #>  Min.   :  10.0   Min.   :  99   Min.   :1.000   #>  1st Qu.:  97.0   1st Qu.:1636   1st Qu.:2.000   #>  Median : 162.0   Median :2059   Median :2.000   #>  Mean   : 248.6   Mean   :2283   Mean   :1.833   #>  3rd Qu.: 306.0   3rd Qu.:3131   3rd Qu.:2.000   #>  Max.   :1742.0   Max.   :5565   Max.   :3.000   #> ------------------------------------------------------------  #> sex: 1 #>      thick             days          status      #>  Min.   :  16.0   Min.   :  10   Min.   :1.000   #>  1st Qu.: 105.0   1st Qu.:1052   1st Qu.:1.000   #>  Median : 258.0   Median :1860   Median :2.000   #>  Mean   : 361.1   Mean   :1946   Mean   :1.722   #>  3rd Qu.: 484.0   3rd Qu.:2784   3rd Qu.:2.000   #>  Max.   :1466.0   Max.   :4492   Max.   :3.000 dsummary(melanoma,thick+days+status~sex|thick<97) #> sex: 0 #>      thick            days          status      #>  Min.   :10.00   Min.   : 355   Min.   :1.000   #>  1st Qu.:32.00   1st Qu.:1762   1st Qu.:2.000   #>  Median :64.00   Median :2227   Median :2.000   #>  Mean   :51.48   Mean   :2425   Mean   :2.034   #>  3rd Qu.:65.00   3rd Qu.:3185   3rd Qu.:2.000   #>  Max.   :81.00   Max.   :4688   Max.   :3.000   #> ------------------------------------------------------------  #> sex: 1 #>      thick            days          status      #>  Min.   :16.00   Min.   :  30   Min.   :1.000   #>  1st Qu.:30.00   1st Qu.:1820   1st Qu.:2.000   #>  Median :65.00   Median :2886   Median :2.000   #>  Mean   :55.75   Mean   :2632   Mean   :1.875   #>  3rd Qu.:81.00   3rd Qu.:3328   3rd Qu.:2.000   #>  Max.   :81.00   Max.   :4207   Max.   :3.000 dsummary(melanoma,thick+status~+1|sex==1) #>      thick            status      #>  Min.   :  16.0   Min.   :1.000   #>  1st Qu.: 105.0   1st Qu.:1.000   #>  Median : 258.0   Median :2.000   #>  Mean   : 361.1   Mean   :1.722   #>  3rd Qu.: 484.0   3rd Qu.:2.000   #>  Max.   :1466.0   Max.   :3.000 dsummary(melanoma,~thick+status|sex==1) #>      thick            status      #>  Min.   :  16.0   Min.   :1.000   #>  1st Qu.: 105.0   1st Qu.:1.000   #>  Median : 258.0   Median :2.000   #>  Mean   : 361.1   Mean   :1.722   #>  3rd Qu.: 484.0   3rd Qu.:2.000   #>  Max.   :1466.0   Max.   :3.000 dsummary(melanoma,thick+days+status~sex|I(thick<97 & sex==1)) #> sex: 1 #>      thick            days          status      #>  Min.   :16.00   Min.   :  30   Min.   :1.000   #>  1st Qu.:30.00   1st Qu.:1820   1st Qu.:2.000   #>  Median :65.00   Median :2886   Median :2.000   #>  Mean   :55.75   Mean   :2632   Mean   :1.875   #>  3rd Qu.:81.00   3rd Qu.:3328   3rd Qu.:2.000   #>  Max.   :81.00   Max.   :4207   Max.   :3.000 dtable(melanoma,~status+sex) #>  #>        sex  0  1 #> status           #> 1          28 29 #> 2          91 43 #> 3           7  7 dtable(melanoma,~status+sex+ulc,level=2) #>  #>    status #> sex  1  2  3 #>   0 28 91  7 #>   1 29 43  7 #>  #>    status #> ulc  1  2  3 #>   0 16 92  7 #>   1 41 42  7 #>  #>    sex #> ulc  0  1 #>   0 79 36 #>   1 47 43 dtable(melanoma,~status+sex+ulc,level=1) #>  #> status #>   1   2   3  #>  57 134  14  #>  #> sex #>   0   1  #> 126  79  #>  #> ulc #>   0   1  #> 115  90 dtable(melanoma,~status+sex+ulc+dcut(days)+I(days>300),level=1) #>  #> status #>   1   2   3  #>  57 134  14  #>  #> sex #>   0   1  #> 126  79  #>  #> ulc #>   0   1  #> 115  90  #>  #> dcut(days) #>       [10,1.52e+03]    (1.52e+03,2e+03]    (2e+03,3.04e+03] (3.04e+03,5.56e+03]  #>                  52                  51                  51                  51  #>  #> I(days > 300) #> FALSE  TRUE  #>    11   194"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"sorting-the-data","dir":"Articles","previous_headings":"","what":"Sorting the data","title":"dUtility data-frame manipulations","text":"sort data sort multiple variables increasing decreasing","code":"data(melanoma) mel= dsort(melanoma,~days) dsort(melanoma) <- ~days head(mel) #>    no status days ulc thick sex #> 1 789      3   10   1   676   1 #> 2  13      3   30   0    65   1 #> 3  97      2   35   0   134   1 #> 4  16      3   99   0   290   0 #> 5  21      1  185   1  1208   1 #> 6 469      1  204   1   484   1 dsort(melanoma) <- ~days-status head(melanoma) #>    no status days ulc thick sex #> 1 789      3   10   1   676   1 #> 2  13      3   30   0    65   1 #> 3  97      2   35   0   134   1 #> 4  16      3   99   0   290   0 #> 5  21      1  185   1  1208   1 #> 6 469      1  204   1   484   1"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"making-new-variales-for-the-analysis","dir":"Articles","previous_headings":"","what":"Making new variales for the analysis","title":"dUtility data-frame manipulations","text":"define bunch new covariates within data-frame definitions done using condition can achieved using dtransform function extends transform possible condition","code":"data(melanoma) melanoma= transform(melanoma, thick2=thick^2, lthick=log(thick) )  dhead(melanoma) #>    no status days ulc thick sex  thick2   lthick #> 1 789      3   10   1   676   1  456976 6.516193 #> 2  13      3   30   0    65   1    4225 4.174387 #> 3  97      2   35   0   134   1   17956 4.897840 #> 4  16      3   99   0   290   0   84100 5.669881 #> 5  21      1  185   1  1208   1 1459264 7.096721 #> 6 469      1  204   1   484   1  234256 6.182085 melanoma=dtransform(melanoma,ll=thick*1.05^ulc,sex==1)    melanoma=dtransform(melanoma,ll=thick,sex!=1)    dmean(melanoma,ll~sex+ulc) #>   sex ulc       ll #> 1   0   0 173.7342 #> 2   1   0 197.3611 #> 3   0   1 374.5532 #> 4   1   1 523.1198"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"making-factors-groupings","dir":"Articles","previous_headings":"","what":"Making factors (groupings)","title":"dUtility data-frame manipulations","text":"melanoma data variable thick gives thickness melanom tumour. analyses like make factor depending thickness. can done several different ways New variable named thickcat.0 default. see levels factors data-frame Checking group sizes adding data-frame directly new variable named thickcat.0 (first cut-point), get quartiles default names thick.cat.4 median groups, starting original data, control new names can also typed specifically","code":"melanoma=dcut(melanoma,~thick,breaks=c(0,200,500,800,2000)) dlevels(melanoma) #> thickcat.0 #levels=:4  #> [1] \"[0,200]\"     \"(200,500]\"   \"(500,800]\"   \"(800,2e+03]\" #> ----------------------------------------- dtable(melanoma,~thickcat.0) #>  #> thickcat.0 #>     [0,200]   (200,500]   (500,800] (800,2e+03]  #>         109          64          20          12 dcut(melanoma,breaks=c(0,200,500,800,2000)) <- gr.thick1~thick dlevels(melanoma) #> thickcat.0 #levels=:4  #> [1] \"[0,200]\"     \"(200,500]\"   \"(500,800]\"   \"(800,2e+03]\" #> ----------------------------------------- #> gr.thick1 #levels=:4  #> [1] \"[0,200]\"     \"(200,500]\"   \"(500,800]\"   \"(800,2e+03]\" #> ----------------------------------------- dcut(melanoma) <- ~ thick  # new variable is thickcat.4 dlevels(melanoma) #> thickcat.0 #levels=:4  #> [1] \"[0,200]\"     \"(200,500]\"   \"(500,800]\"   \"(800,2e+03]\" #> ----------------------------------------- #> gr.thick1 #levels=:4  #> [1] \"[0,200]\"     \"(200,500]\"   \"(500,800]\"   \"(800,2e+03]\" #> ----------------------------------------- #> thickcat.4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- data(melanoma) dcut(melanoma,breaks=2) <- ~ thick  # new variable is thick.2 dlevels(melanoma) #> thickcat.2 #levels=:2  #> [1] \"[10,194]\"       \"(194,1.74e+03]\" #> ----------------------------------------- data(melanoma) mela= dcut(melanoma,thickcat4+dayscat4~thick+days,breaks=4) dlevels(mela) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> dayscat4 #levels=:4  #> [1] \"[10,1.52e+03]\"       \"(1.52e+03,2e+03]\"    \"(2e+03,3.04e+03]\"    #> [4] \"(3.04e+03,5.56e+03]\" #> ----------------------------------------- data(melanoma) dcut(melanoma,breaks=4) <- thickcat4+dayscat4~thick+days dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> dayscat4 #levels=:4  #> [1] \"[10,1.52e+03]\"       \"(1.52e+03,2e+03]\"    \"(2e+03,3.04e+03]\"    #> [4] \"(3.04e+03,5.56e+03]\" #> ----------------------------------------- melanoma$gthick = cut(melanoma$thick,breaks=c(0,200,500,800,2000)) melanoma$gthick = cut(melanoma$thick,breaks=quantile(melanoma$thick),include.lowest=TRUE)"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"working-with-factors","dir":"Articles","previous_headings":"","what":"Working with factors","title":"dUtility data-frame manipulations","text":"see levels covariates data-frame relevel factor take third level list levels, , combine levels factor (first combinining first 3 groups one) combine groups 1 2 one group 3 4 another Changing order factor levels Combine levels now control factor-level names","code":"data(melanoma) dcut(melanoma,breaks=4) <- thickcat4~thick dlevels(melanoma)  #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- dtable(melanoma,~thickcat4) #>  #> thickcat4 #>        [10,97]       (97,194]      (194,356] (356,1.74e+03]  #>             56             53             45             51 melanoma = drelevel(melanoma,~thickcat4,ref=\"(194,356]\") dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.(194,356] #levels=:4  #> [1] \"(194,356]\"      \"[10,97]\"        \"(97,194]\"       \"(356,1.74e+03]\" #> ----------------------------------------- melanoma = drelevel(melanoma,~thickcat4,ref=2) dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.(194,356] #levels=:4  #> [1] \"(194,356]\"      \"[10,97]\"        \"(97,194]\"       \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.2 #levels=:4  #> [1] \"(97,194]\"       \"[10,97]\"        \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- melanoma = drelevel(melanoma,~thickcat4,newlevels=1:3) dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.(194,356] #levels=:4  #> [1] \"(194,356]\"      \"[10,97]\"        \"(97,194]\"       \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.2 #levels=:4  #> [1] \"(97,194]\"       \"[10,97]\"        \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.1:3 #levels=:2  #> [1] \"[10,97]-(194,356]\" \"(356,1.74e+03]\"    #> ----------------------------------------- dkeep(melanoma) <- ~thick+thickcat4 melanoma = drelevel(melanoma,gthick2~thickcat4,newlevels=list(1:2,3:4)) dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> gthick2 #levels=:2  #> [1] \"[10,97]-(97,194]\"         \"(194,356]-(356,1.74e+03]\" #> ----------------------------------------- dfactor(melanoma,levels=c(3,1,2,4)) <-  thickcat4.2~thickcat4 dlevel(melanoma,~ \"thickcat4*\") #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.2 #levels=:4  #> [1] \"(194,356]\"      \"[10,97]\"        \"(97,194]\"       \"(356,1.74e+03]\" #> ----------------------------------------- dtable(melanoma,~thickcat4+thickcat4.2) #>  #>                thickcat4.2 (194,356] [10,97] (97,194] (356,1.74e+03] #> thickcat4                                                            #> [10,97]                            0      56        0              0 #> (97,194]                           0       0       53              0 #> (194,356]                         45       0        0              0 #> (356,1.74e+03]                     0       0        0             51 melanoma=drelevel(melanoma,gthick3~thickcat4,newlevels=list(group1.2=1:2,group3.4=3:4)) dlevels(melanoma) #> thickcat4 #levels=:4  #> [1] \"[10,97]\"        \"(97,194]\"       \"(194,356]\"      \"(356,1.74e+03]\" #> ----------------------------------------- #> gthick2 #levels=:2  #> [1] \"[10,97]-(97,194]\"         \"(194,356]-(356,1.74e+03]\" #> ----------------------------------------- #> thickcat4.2 #levels=:4  #> [1] \"(194,356]\"      \"[10,97]\"        \"(97,194]\"       \"(356,1.74e+03]\" #> ----------------------------------------- #> gthick3 #levels=:2  #> [1] \"group1.2\" \"group3.4\" #> -----------------------------------------"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"making-a-factor-from-existing-numeric-variable-and-vice-versa","dir":"Articles","previous_headings":"","what":"Making a factor from existing numeric variable and vice versa","title":"dUtility data-frame manipulations","text":"numeric variable “status” values 1,2,3 factor gender factor values “M”, “F” can converted numerics ","code":"data(melanoma) melanoma = dfactor(melanoma,~status, labels=c(\"malignant-melanoma\",\"censoring\",\"dead-other\")) melanoma = dfactor(melanoma,sexl~sex,labels=c(\"females\",\"males\")) dtable(melanoma,~sexl+status.f) #>  #>         status.f malignant-melanoma censoring dead-other #> sexl                                                     #> females                          28        91          7 #> males                            29        43          7 melanoma = dnumeric(melanoma,~sexl) dstr(melanoma,\"sex*\") #> 'data.frame':    205 obs. of  3 variables: #>  $ sex   : int  1 1 1 0 1 1 1 1 0 0 ... #>  $ sexl  : Factor w/ 2 levels \"females\",\"males\": 2 2 2 1 2 2 2 2 1 1 ... #>  $ sexl.n: num  2 2 2 1 2 2 2 2 1 1 ... dtable(melanoma,~'sex*',level=2) #>  #>          sex #> sexl        0   1 #>   females 126   0 #>   males     0  79 #>  #>       sex #> sexl.n   0   1 #>      1 126   0 #>      2   0  79 #>  #>       sexl #> sexl.n females males #>      1     126     0 #>      2       0    79"},{"path":"http://kkholst.github.io/mets/articles/basic-dutils.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"dUtility data-frame manipulations","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Analysis of multivariate binomial data: family analysis","text":"looking multivariate binomial data aim learning dependence present, possibly correcting covariates many models methods available: Random-effects models logistic regression covered elsewhere (glmer lme4). mets package can fit Pairwise odds ratio model Bivariate Probit model random effects Special functionality polygenic random effects modelling ACE, ADE ,AE forth. Additive gamma random effects model Special functionality polygenic random effects modelling ACE, ADE ,AE forth. last three models fitted mets package using composite likelihoods based pairs within clusters. models can fitted specifically based specifying pairs one wants use composite score. models described futher details binomial-twin vignette.","code":""},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"simulated-family-data","dir":"Articles","previous_headings":"","what":"Simulated family data","title":"Analysis of multivariate binomial data: family analysis","text":"start simulating family data additive gamma structure ACE form. 10000 families consisting two parents two children. response ybin one covariate x. fit marginal models, find covariate effect 0.3 x. marginals can specified desired.","code":"library(mets)  library(timereg) #> Loading required package: survival #>  #> Attaching package: 'timereg' #> The following objects are masked from 'package:mets': #>  #>     Event, event.split, kmplot, plotConfregion  set.seed(100)  data <- simbinClaytonOakes.family.ace(500,2,1,beta=NULL,alpha=NULL)  data$number <- c(1,2,3,4)  data$child <- 1*(data$number==3)  head(data) #>   ybin x   type cluster number child #> 1    1 1 mother       1      1     0 #> 2    1 0 father       1      2     0 #> 3    1 0  child       1      3     1 #> 4    1 1  child       1      4     0 #> 5    1 1 mother       2      1     0 #> 6    1 0 father       2      2     0 aa <- margbin <- glm(ybin~x,data=data,family=binomial())  summary(aa) #>  #> Call: #> glm(formula = ybin ~ x, family = binomial(), data = data) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  0.49908    0.06387   7.814 5.52e-15 *** #> x            0.17531    0.09355   1.874   0.0609 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 2610.2  on 1999  degrees of freedom #> Residual deviance: 2606.7  on 1998  degrees of freedom #> AIC: 2610.7 #>  #> Number of Fisher Scoring iterations: 4"},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"additive-gamma-model","dir":"Articles","previous_headings":"","what":"Additive Gamma model","title":"Analysis of multivariate binomial data: family analysis","text":"Additive Gamma model set-random effects included family make ACE valid using special functions . model constructed one enviromental effect shared family 8 genetic random effects size 1/4 genetic variance. Looking first family see mother father share half genes children two children also share half genes specification. also show alternative specification model using pairs. can now fit model calling two-stage function","code":"# make ace random effects design out <- ace.family.design(data,member=\"type\",id=\"cluster\") out$pardes #>       [,1] [,2] #>  [1,] 0.25    0 #>  [2,] 0.25    0 #>  [3,] 0.25    0 #>  [4,] 0.25    0 #>  [5,] 0.25    0 #>  [6,] 0.25    0 #>  [7,] 0.25    0 #>  [8,] 0.25    0 #>  [9,] 0.00    1 head(out$des.rv,4) #>      m1 m2 m3 m4 f1 f2 f3 f4 env #> [1,]  1  1  1  1  0  0  0  0   1 #> [2,]  0  0  0  0  1  1  1  1   1 #> [3,]  1  1  0  0  1  1  0  0   1 #> [4,]  1  0  1  0  1  0  1  0   1 # fitting ace model for family structure ts <- binomial.twostage(margbin,data=data,clusters=data$cluster, theta=c(2,1),random.design=out$des.rv,theta.des=out$pardes) summary(ts) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                theta        se #> dependence1 2.817486 0.8981749 #> dependence2 1.137637 0.2806719 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err   2.5%  97.5%   P-value #> dependence1   0.7124 0.09262 0.5308 0.8939 1.457e-14 #> dependence2   0.2876 0.09262 0.1061 0.4692 1.899e-03 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    3.955  0.8668 2.256 5.654 5.051e-06 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # true variance parameters c(2,1) #> [1] 2 1 # total variance  3 #> [1] 3"},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"pairwise-fitting","dir":"Articles","previous_headings":"","what":"Pairwise fitting","title":"Analysis of multivariate binomial data: family analysis","text":"now specify model via extracting pairs. random effecs structure typically simpler just looking pairs, start specifying random effects jointly whole family. special function writes combinations pairs. 6 pairs within family, keep track belongs different families. first simply give pairs get result . Now pairs fit model random sample pairs given instead get estimates. specify model pairs available show specify model. use marginal “aa” make results comparable. marginal can also fitted based available data. start selecting data related pairs, sets new id’s start specify model using full design 9 random effects. show one can use random effects needed pair, typically simpler. Now fitting model data set now specify design specifically using pairs. random.design design parameters now given pair, 3 dimensional matrix. direct specification random.design design parameters theta.design. addition need also give number random effects pair. basic things constructed certain functions ACE design. pairs.new matrix columns 1:2 giving indeces data points columns 3:4 giving indeces random.design different pairs columns 5 giving indeces theta.des written rows columns 6 giving number random variables pair length rows theta.des maximum number random effects ×\\times number parameters. two numbers given call. case 4 ×\\times 2. theta.des rows length 88, possibly including 0’s rows relevant due fewer random effects, case pairs share genetic effects. pair 1 mother/farther pair, see share 1 environmental random effect size 1. also two genetic effects unshared two. total 3 random effects needed . theta.des relates 3 random effects possible relationships parameters. genetic effects full environmental effect. contrast also consider mother/child pair share half genes, now random effects (1/2) gene variance. need 4 random effects, 2 non-shared half-gene, 1 shared half-gene, one shared full environmental effect. Now fitting model, see lot quicker due fewer random effects needed pairs. need also specify number parameters case. model can specifed even simpler via kinship coefficient. speicification 4 random effects pair, variance 0. mother-father pair, shares random effect variance 0, two non-shared genetic effects full variance, addition fully shared environmental effect.","code":"mm <- familycluster.index(data$cluster) head(mm$familypairindex,n=20) #>  [1] 1 2 1 3 1 4 2 3 2 4 3 4 5 6 5 7 5 8 6 7 pairs <- mm$pairs dim(pairs) #> [1] 3000    2 head(pairs,12) #>       [,1] [,2] #>  [1,]    1    2 #>  [2,]    1    3 #>  [3,]    1    4 #>  [4,]    2    3 #>  [5,]    2    4 #>  [6,]    3    4 #>  [7,]    5    6 #>  [8,]    5    7 #>  [9,]    5    8 #> [10,]    6    7 #> [11,]    6    8 #> [12,]    7    8 tsp <- binomial.twostage(margbin,data=data,clusters=data$cluster,theta=c(2,1),detail=0,         random.design=out$des.rv,theta.des=out$pardes,pairs=pairs) summary(tsp) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                theta        se #> dependence1 2.817486 0.8981749 #> dependence2 1.137637 0.2806719 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err   2.5%  97.5%   P-value #> dependence1   0.7124 0.09262 0.5308 0.8939 1.457e-14 #> dependence2   0.2876 0.09262 0.1061 0.4692 1.899e-03 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    3.955  0.8668 2.256 5.654 5.051e-06 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" set.seed(100) ssid <- sort(sample(1:nrow(pairs),nrow(pairs)/2)) tsd <- binomial.twostage(aa,data=data,clusters=data$cluster,                theta=c(2,1),step=1.0,                random.design=out$des.rv,iid=1,Nit=10,                theta.des=out$pardes,pairs=pairs[ssid,]) summary(tsd) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 theta        se #> dependence1 3.0275402 1.6045129 #> dependence2 0.8135103 0.3895686 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err     2.5%  97.5%   P-value #> dependence1   0.7882  0.1511  0.49210 1.0843 1.816e-07 #> dependence2   0.2118  0.1511 -0.08431 0.5079 1.609e-01 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5% P-value #> p1    3.841   1.402 1.093 6.589 0.00615 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" head(pairs[ssid,]) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    1    3 #> [3,]    2    3 #> [4,]    2    4 #> [5,]    3    4 #> [6,]    6    8 ids <- sort(unique(c(pairs[ssid,])))  pairsids <- c(pairs[ssid,]) pair.new <- matrix(fast.approx(ids,c(pairs[ssid,])),ncol=2) head(pair.new) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    1    3 #> [3,]    2    3 #> [4,]    2    4 #> [5,]    3    4 #> [6,]    5    7  dataid <- dsort(data[ids,],\"cluster\") outid <- ace.family.design(dataid,member=\"type\",id=\"cluster\") outid$pardes #>       [,1] [,2] #>  [1,] 0.25    0 #>  [2,] 0.25    0 #>  [3,] 0.25    0 #>  [4,] 0.25    0 #>  [5,] 0.25    0 #>  [6,] 0.25    0 #>  [7,] 0.25    0 #>  [8,] 0.25    0 #>  [9,] 0.00    1 head(outid$des.rv) #>      m1 m2 m3 m4 f1 f2 f3 f4 env #> [1,]  1  1  1  1  0  0  0  0   1 #> [2,]  0  0  0  0  1  1  1  1   1 #> [3,]  1  1  0  0  1  1  0  0   1 #> [4,]  1  0  1  0  1  0  1  0   1 #> [5,]  0  0  0  0  1  1  1  1   1 #> [6,]  1  1  0  0  1  1  0  0   1 aa <- glm(ybin~x,data=dataid,family=binomial()) tsdid <- binomial.twostage(aa,data=dataid,clusters=dataid$cluster,          theta=c(2,1),random.design=outid$des.rv,theta.des=outid$pardes,pairs=pair.new) summary(tsdid) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 theta       se #> dependence1 3.1389859 1.649758 #> dependence2 0.8010544 0.399870 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err     2.5%  97.5%   P-value #> dependence1   0.7967  0.1492  0.50421 1.0892 9.358e-08 #> dependence2   0.2033  0.1492 -0.08917 0.4958 1.731e-01 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%  P-value #> p1     3.94   1.438 1.121 6.759 0.006153 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" pair.types <-  matrix(dataid[c(t(pair.new)),\"type\"],byrow=T,ncol=2) head(pair.new,7) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    1    3 #> [3,]    2    3 #> [4,]    2    4 #> [5,]    3    4 #> [6,]    5    7 #> [7,]    6    7 head(pair.types,7) #>      [,1]     [,2]     #> [1,] \"mother\" \"father\" #> [2,] \"mother\" \"child\"  #> [3,] \"father\" \"child\"  #> [4,] \"father\" \"child\"  #> [5,] \"child\"  \"child\"  #> [6,] \"father\" \"child\"  #> [7,] \"child\"  \"child\" ### theta.des  <- rbind( c(rbind(c(1,0),  c(1,0),  c(0,1),  c(0,0))),              c(rbind(c(0.5,0),c(0.5,0),c(0.5,0),c(0,1)))) random.des <- rbind(          c(1,0,1,0),c(0,1,1,0),         c(1,1,0,1),c(1,0,1,1)) mf <- 1*(pair.types[,1]==\"mother\" & pair.types[,2]==\"father\") ##          pair, rv related to pairs,  theta.des related to pair  pairs.new <- cbind(pair.new,(mf==1)*1+(mf==0)*3,(mf==1)*2+(mf==0)*4,(mf==1)*1+(mf==0)*2,(mf==1)*3+(mf==0)*4) # 3 rvs here  random.des #>      [,1] [,2] [,3] [,4] #> [1,]    1    0    1    0 #> [2,]    0    1    1    0 #> [3,]    1    1    0    1 #> [4,]    1    0    1    1 theta.des #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #> [1,]  1.0  1.0  0.0    0    0    0    1    0 #> [2,]  0.5  0.5  0.5    0    0    0    0    1  head(pairs.new) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    1    2    1    2    1    3 #> [2,]    1    3    3    4    2    4 #> [3,]    2    3    3    4    2    4 #> [4,]    2    4    3    4    2    4 #> [5,]    3    4    3    4    2    4 #> [6,]    5    7    3    4    2    4 tsdid2 <- binomial.twostage(aa,data=dataid,clusters=dataid$cluster, theta=c(2,1),            random.design=random.des,theta.des=theta.des,pairs=pairs.new,dim.theta=2) summary(tsdid2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 theta       se #> dependence1 3.1389859 1.649758 #> dependence2 0.8010544 0.399870 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err     2.5%  97.5%   P-value #> dependence1   0.7967  0.1492  0.50421 1.0892 9.358e-08 #> dependence2   0.2033  0.1492 -0.08917 0.4958 1.731e-01 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%  P-value #> p1     3.94   1.438 1.121 6.759 0.006153 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" kinship  <- rep(0.5,nrow(pair.types)) kinship[pair.types[,1]==\"mother\" & pair.types[,2]==\"father\"] <- 0 head(kinship,n=10) #>  [1] 0.0 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5  out <- make.pairwise.design(pair.new,kinship,type=\"ace\") tsdid3 <- binomial.twostage(aa,data=dataid,clusters=dataid$cluster,              theta=c(2,1)/9,random.design=out$random.design,              theta.des=out$theta.des,pairs=out$new.pairs,dim.theta=2) summary(tsdid3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 theta       se #> dependence1 3.1389859 1.649758 #> dependence2 0.8010544 0.399870 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err     2.5%  97.5%   P-value #> dependence1   0.7967  0.1492  0.50421 1.0892 9.358e-08 #> dependence2   0.2033  0.1492 -0.08917 0.4958 1.731e-01 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%  P-value #> p1     3.94   1.438 1.121 6.759 0.006153 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"pairwise-dependence-modelling","dir":"Articles","previous_headings":"Pairwise fitting","what":"Pairwise dependence modelling","title":"Analysis of multivariate binomial data: family analysis","text":"Now pairs fit model","code":"library(mets) set.seed(1000) data <- simbinClaytonOakes.family.ace(500,2,1,beta=NULL,alpha=NULL) head(data) #>   ybin x   type cluster #> 1    0 1 mother       1 #> 2    0 0 father       1 #> 3    0 1  child       1 #> 4    0 0  child       1 #> 5    1 0 mother       2 #> 6    1 0 father       2 data$number <- c(1,2,3,4) data$child <- 1*(data$number==3)  mm <- familycluster.index(data$cluster) head(mm$familypairindex,n=20) #>  [1] 1 2 1 3 1 4 2 3 2 4 3 4 5 6 5 7 5 8 6 7 pairs <- mm$pairs dim(pairs) #> [1] 3000    2 head(pairs,12) #>       [,1] [,2] #>  [1,]    1    2 #>  [2,]    1    3 #>  [3,]    1    4 #>  [4,]    2    3 #>  [5,]    2    4 #>  [6,]    3    4 #>  [7,]    5    6 #>  [8,]    5    7 #>  [9,]    5    8 #> [10,]    6    7 #> [11,]    6    8 #> [12,]    7    8 dtypes <- interaction( data[pairs[,1],\"type\"], data[pairs[,2],\"type\"])  dtypes <- droplevels(dtypes)  table(dtypes) #> dtypes #>   child.child  father.child  mother.child mother.father  #>           500          1000          1000           500  dm <- model.matrix(~-1+factor(dtypes)) aa <- glm(ybin~x,data=data,family=binomial())  tsp <- binomial.twostage(aa,data=data, clusters=data$cluster,          theta.des=dm,pairs=cbind(pairs,1:nrow(dm))) summary(tsp) #> Dependence parameter for Odds-Ratio (Plackett) model #> $estimates #>                                theta        se #> factor(dtypes)child.child   3.520109 0.7143594 #> factor(dtypes)father.child  3.871698 0.6055928 #> factor(dtypes)mother.child  3.922180 0.6233304 #> factor(dtypes)mother.father 1.328730 0.2657202 #>  #> $log.or #>                             Estimate Std.Err    2.5%  97.5%   P-value #> factor(dtypes)child.child     1.2585  0.2029  0.8607 1.6562 5.596e-10 #> factor(dtypes)father.....     1.3537  0.1564  1.0471 1.6603 4.952e-18 #> factor(dtypes)mother.....     1.3666  0.1589  1.0552 1.6781 8.017e-18 #> factor(dtypes)mother......1   0.2842  0.2000 -0.1077 0.6762 1.552e-01 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"pairwise-odds-ratio-model","dir":"Articles","previous_headings":"","what":"Pairwise odds ratio model","title":"Analysis of multivariate binomial data: family analysis","text":"fit pairwise odds-ratio model case pair-specification two options fitting model. One option set artificial data similar twin data pair-cluster-id (clusters) cluster-id get GEE type standard errors (se.cluster) can also use specify design via theta.des also matrix dimension pairs x design design POR model. Starting second option. need start specify design odds-ratio pair. set data find combinations within pairs. Subsequently, remove empty groups, grouping together factor levels 4:9, construct design. can fit pairwise model using pairs pair-design descrbing . results consistent ACE model mother-father lower dependence due environmental effects. combinations dependence also seem case. fit model generally recommended use var.link use parmetrization log-odd-ratio regression.","code":"tdp <-cbind( dataid[pair.new[,1],],dataid[pair.new[,2],]) names(tdp) <- c(paste(names(dataid),\"1\",sep=\"\"),         paste(names(dataid),\"2\",sep=\"\")) tdp <-transform(tdp,tt=interaction(type1,type2)) dlevel(tdp) #> tt #levels=:6  #> [1] \"child.child\"   \"father.child\"  \"mother.child\"  \"child.father\"  #> [5] \"father.father\" \"mother.father\" #> ----------------------------------------- drelevel(tdp,newlevels=list(mother.father=4:9)) <-  obs.types~tt dtable(tdp,~tt+obs.types) #>  #>               obs.types mother.father child.child father.child mother.child #> tt                                                                          #> child.child                         0         232            0            0 #> father.child                        0           0          525            0 #> mother.child                        0           0            0          510 #> child.father                        0           0            0            0 #> father.father                       0           0            0            0 #> mother.father                     233           0            0            0 tdp <- model.matrix(~-1+factor(obs.types),tdp) ###porpair <- binomial.twostage(aa,data=dataid,clusters=dataid$cluster, ###           theta.des=tdp,pairs=pair.new,model=\"or\",var.link=1) ###summary(porpair)"},{"path":"http://kkholst.github.io/mets/articles/binomial-family.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Analysis of multivariate binomial data: family analysis","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] timereg_2.0.7  survival_3.8-3 mets_1.3.9     #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] compiler_4.5.2      codetools_0.2-20    fs_1.6.6            #> [25] htmlwidgets_1.6.4   Rcpp_1.1.0          future_1.68.0       #> [28] lattice_0.22-7      systemfonts_1.3.1   digest_0.6.39       #> [31] R6_2.6.1            parallelly_1.46.0   parallel_4.5.2      #> [34] splines_4.5.2       Matrix_1.7-4        bslib_0.9.0         #> [37] tools_4.5.2         globals_0.18.0      pkgdown_2.2.0       #> [40] cachem_1.1.0        desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Analysis of bivariate binomial data: Twin analysis","text":"looking bivariate binomial data aim learning dependence present, possibly correcting covariates many models available. Random-effects models logistic regression covered elsewhere (glmer lme4). mets package can fit Pairwise odds ratio model Bivariate Probit model random effects Special functionality polygenic random effects modelling ACE, ADE ,AE forth. Additive gamma random effects model Special functionality polygenic random effects modelling ACE, ADE ,AE forth. Typically can hard impossible specify random effects models special structure among parameters random effects. possible models. concrete model structure assume paired binomial data (Y1,Y2,X1,X)2(Y_1, Y_2, X_1, X_)2 responses Y1,Y2Y_1, Y_2 covariates X1,X2X_1, X_2. start giving brief description different models. First bivariate data one can specify marginal probability using logistic regression models logit(P(Yi=1|Xi))=αi+XiTβi=1,2. logit(P(Y_i=1|X_i)) = \\alpha_i + X_i^T \\beta  =1,2.  model can estimated working independence . typical twin analysis typically consist looking Pairwise odds ratio model Bivariate Probit model additive gamma can used bivariate probit model restrictive terms dependence structure, nevertheless still valuable also check results bivariate probit model.","code":""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"biprobit-with-random-effects","dir":"Articles","previous_headings":"","what":"Biprobit with random effects","title":"Analysis of bivariate binomial data: Twin analysis","text":"model assume given random effects ZZ covariate vector V12V_{12} independent logistic regression models probit(P(Yi=1|Xi,Z))=αi+XiTβ+V12TZi=1,2. probit(P(Y_i=1|X_i, Z)) = \\alpha_i + X_i^T \\beta + V_{12}^T Z  =1,2.  Z$ bivariate normal distribution covariance Σ\\Sigma. general covariance structure Σ\\Sigma makes model flexible. note Paramters β\\beta subject specific Σ\\Sigma reflect dependence standard link function logitlogit rather probitprobit link often used implemented example . advantage one now gets odds-ratio interpretation subject specific effects, one needs numerical integration fit model.","code":""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"pairwise-odds-ratio-model","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Pairwise odds ratio model","title":"Analysis of bivariate binomial data: Twin analysis","text":"Now pairwise odds ratio model specifies given $ X_1, X_2 $ marginal models logit(P(Yi=1|Xi))=αi+XiTβi=1,2 logit(P(Y_i=1|X_i)) = \\alpha_i + X_i^T \\beta  =1,2 primary object interest odds ratio Y1Y_{1} Y2Y_{2}γ12=P(Yki=1,Ykj=1)P(Yki=0,Ykj=0)P(Yki=1,Ykj=0)P(Yki=0,Ykj=1) \\gamma_{12} = \\frac{ P(  Y_{ki} =1 , Y_{kj} =1) P(  Y_{ki} =0 , Y_{kj} =0) }{    P(  Y_{ki} =1 , Y_{kj} =0) P(  Y_{ki} =0 , Y_{kj} =1) }  given XkiX_{ki}, XkjX_{kj}, ZkjiZ_{kji}. model odds ratio regression γ12=exp(Z12Tλ) \\gamma_{12} = \\exp( Z_{12}^T \\lambda)  Z12Z_{12} covarites may influence odds-ratio Y1Y_{1} Y2Y_{2} contains marginal covariates, . odds-ratio given covariates well marginal covariates. odds-ratio marginals specify joint bivariate distribution via -called Placckett-distribution. One way fitting model ALR algoritm, alternating logistic regression ahd described several papers . simply estimate parameters two stage-procedure Estimating marginal parameters via GEE Using marginal estimates, estimate dependence parameters gives efficient estimates dependence parameters orthogonality, efficiency may gained marginal parameters using full likelihood iterative fitting ALR. pairwise odds-ratio model useful, one random effects model.","code":""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"additive-gamma-model","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Additive gamma model","title":"Analysis of bivariate binomial data: Twin analysis","text":"operate marginal logistic regression models logit(P(Yi=1|Xi))=αi+XiTβi=1,2 logit(P(Y_i=1|X_i)) = \\alpha_i + X_i^T \\beta  =1,2 First just one random effect ZZ assume conditional ZZ responses independent follow model logit(P(Yi=1|Xi,Z))=exp(−Z⋅Ψ−1(λ•,λ•,P(Yi=1|Xi))) logit(P(Y_i=1|X_i,Z)) = exp( -Z \\cdot \\Psi^{-1}(\\lambda_{\\bullet},\\lambda_{\\bullet},P(Y_i=1|X_i)) )    Ψ\\Psi laplace transform ZZ assume ZZ gamma distributed variance λ•−1\\lambda_{\\bullet}^{-1} mean 1. general Ψ(λ1,λ2)\\Psi(\\lambda_1,\\lambda_2) laplace transform Gamma distributed random effect ZZ mean λ1/λ2\\lambda_1/\\lambda_2 variance λ1/λ22\\lambda_1/\\lambda_2^2. fit model Estimating marginal parameters via GEE Using marginal estimates, estimate dependence parameters deal multiple random effects consider random effects Zii=1,...,dZ_i  =1,...,d ZiZ_i gamma distributed * mean: λj/λ•\\lambda_j/\\lambda_{\\bullet} * variance: λj/λ•2\\lambda_j/\\lambda_{\\bullet}^2, define scalar λ•\\lambda_{\\bullet} . Now given cluster-specific design vector V12V_{12} assume V12TZ V_{12}^T Z  gamma distributed mean 1 variance λ•−1\\lambda_{\\bullet}^{-1} critically random effect variance clusters. λ•=V12T(λ1,...,λd)T  \\lambda_{\\bullet} = V_{12}^T (\\lambda_1,...,\\lambda_d)^T   return specific models , show fit ACE AE model using set-. One last option model-specification specify parameters λ1,...,λd\\lambda_1,...,\\lambda_d related. thus can specify matrix MM dimension p×dp \\times d (λ1,...,λd)T=Mθ  (\\lambda_1,...,\\lambda_d)^T  = M \\theta  θ\\theta d-dimensional. MM diagonal restrictions parameters. parametrization obtained var.par=0 option thus estimates θ\\theta. DEFAULT parametrization instead estimates variances random effecs (var.par=1) via parameters ν\\nuMν=(λ1/λ•2,...,λd/λ•2)T  M \\nu = ( \\lambda_1/\\lambda_{\\bullet}^2, ...,\\lambda_d/\\lambda_{\\bullet}^2)^T basic modelling assumption now given random effects Z=(Z1,...,Zd)Z=(Z_1,...,Z_d) independent probabilites logit(P(Yi=1|Xi,Z))=exp(−V12,iTZ⋅Ψ−1(λ•,λ•,P(Yi=1|Xi)))=1,2 logit(P(Y_i=1|X_i,Z)) = exp( -V_{12,}^T Z \\cdot \\Psi^{-1}(\\lambda_{\\bullet},\\lambda_{\\bullet},P(Y_i=1|X_i)) )   =1,2 fit model Estimating marginal parameters via GEE Using marginal estimates, estimate dependence parameters Even though model formaly formulation allows negative correlation practice paramters can negative reflects negative correlation. advanatage numerical integration needed.","code":""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"the-twin-stutter-data","dir":"Articles","previous_headings":"Biprobit with random effects","what":"The twin-stutter data","title":"Analysis of bivariate binomial data: Twin analysis","text":"consider twin-stutter pairs twins either dizygotic monozygotic recorded whether twins stuttering consider MZ sex DZ twins. Looking data","code":"library(mets) data(twinstut) twinstut$binstut <- 1*(twinstut$stutter==\"yes\") twinsall <- twinstut twinstut <- subset(twinstut,zyg%in%c(\"mz\",\"dz\")) head(twinstut) #>    tvparnr zyg stutter    sex age nr binstut #> 1        1  mz      no female  71  1       0 #> 2        1  mz      no female  71  2       0 #> 3        2  dz      no female  71  1       0 #> 8        5  mz      no female  71  1       0 #> 9        5  mz      no female  71  2       0 #> 11       7  dz      no   male  71  1       0 twinstut <- subset(twinstut,tvparnr < 3000)"},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"pairwise-odds-ratio-model-1","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Pairwise odds ratio model","title":"Analysis of bivariate binomial data: Twin analysis","text":"start fitting overall dependence MZ DZ even though dependence expected different across zygosity. first step fit marginal model adjusting marginal covariates. note rather strong gender effect risk stuttering. Now estimating parameter. see strong dependence around 8 clearly significant. Now, interestingly, consider depends zygosity note MZ much larger DZ twins. type trait somewhat complicated interpret, clearly, one option genetic effect, alternatively might stronger environmental effect MZ twins. now consider regression modelling structure considering possible interactions sex zygozsity. see MZ much higher dependence males much lower dependence. tested interaction model significant.","code":"margbin <- glm(binstut~factor(sex)+age,data=twinstut,family=binomial()) summary(margbin) #>  #> Call: #> glm(formula = binstut ~ factor(sex) + age, family = binomial(),  #>     data = twinstut) #>  #> Coefficients: #>                 Estimate Std. Error z value Pr(>|z|)     #> (Intercept)      0.03033    2.16166   0.014   0.9888     #> factor(sex)male  0.80288    0.20272   3.961 7.48e-05 *** #> age             -0.05471    0.03295  -1.660   0.0968 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 940.43  on 2650  degrees of freedom #> Residual deviance: 921.01  on 2648  degrees of freedom #> AIC: 927.01 #>  #> Number of Fisher Scoring iterations: 6 bina <- binomial.twostage(margbin,data=twinstut,var.link=1,                        clusters=twinstut$tvparnr,detail=0) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>                theta       se #> dependence1 1.859712 0.481585 #>  #> $or #>             Estimate Std.Err   2.5% 97.5% P-value #> dependence1    6.422   3.093 0.3603 12.48 0.03785 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # design for OR dependence  theta.des <- model.matrix( ~-1+factor(zyg),data=twinstut) bin <- binomial.twostage(margbin,data=twinstut,var.link=1,                           clusters=twinstut$tvparnr,theta.des=theta.des) summary(bin) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>                   theta        se #> factor(zyg)dz 0.1624942 1.0050473 #> factor(zyg)mz 3.7460315 0.7259771 #>  #> $or #>               Estimate Std.Err    2.5%   97.5% P-value #> factor(zyg)dz    1.176   1.182  -1.141   3.494  0.3197 #> factor(zyg)mz   42.353  30.747 -17.910 102.616  0.1684 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" twinstut$cage <- scale(twinstut$age) theta.des <- model.matrix( ~-1+factor(zyg)+factor(sex),data=twinstut) bina <- binomial.twostage(margbin,data=twinstut,var.link=1,                           clusters=twinstut$tvparnr,theta.des=theta.des) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>                     theta       se #> factor(zyg)dz    1.238547 1.118919 #> factor(zyg)mz    6.078248 1.536564 #> factor(sex)male -3.233772 1.776972 #>  #> $or #>                  Estimate   Std.Err       2.5%     97.5% P-value #> factor(zyg)dz     3.45059   3.86094   -4.11670   11.0179  0.3715 #> factor(zyg)mz   436.26434 670.34797 -877.59353 1750.1222  0.5152 #> factor(sex)male   0.03941   0.07003   -0.09784    0.1767  0.5736 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"alternative-syntax","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Alternative syntax","title":"Analysis of bivariate binomial data: Twin analysis","text":"now demonstrate models can fitted jointly anohter syntax, ofcourse just fits marginal model subsequently fits pairwise model. First noticing MZ twins much higher dependence. Now considering data estimating separate effects opposite sex DZ twins sex twins. find os twins markedly different sex DZ twins.","code":"# refers to zygosity of first subject in eash pair : zyg1  # could also use zyg2 (since zyg2=zyg1 within twinpair's)  out <- easy.binomial.twostage(stutter~factor(sex)+age,data=twinstut,                 response=\"binstut\",id=\"tvparnr\",var.link=1,                 theta.formula=~-1+factor(zyg1)) summary(out) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>                    theta        se #> factor(zyg1)dz 0.1624942 1.0050473 #> factor(zyg1)mz 3.7460315 0.7259771 #>  #> $or #>                Estimate Std.Err    2.5%   97.5% P-value #> factor(zyg1)dz    1.176   1.182  -1.141   3.494  0.3197 #> factor(zyg1)mz   42.353  30.747 -17.910 102.616  0.1684 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # refers to zygosity of first subject in eash pair : zyg1  # could also use zyg2 (since zyg2=zyg1 within twinpair's))    desfs<-function(x,num1=\"zyg1\",num2=\"zyg2\")          c(x[num1]==\"dz\",x[num1]==\"mz\",x[num1]==\"os\")*1        margbinall <- glm(binstut~factor(sex)+age,data=twinsall,family=binomial())  out3 <- easy.binomial.twostage(binstut~factor(sex)+age,        data=twinsall,response=\"binstut\",id=\"tvparnr\",var.link=1,        theta.formula=desfs,desnames=c(\"dz\",\"mz\",\"os\"))  summary(out3) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>        theta        se #> dz 0.5278527 0.2396796 #> mz 3.4850037 0.1864190 #> os 0.7802940 0.2894394 #>  #> $or #>    Estimate Std.Err    2.5%  97.5%   P-value #> dz    1.695  0.4063  0.8989  2.492 3.016e-05 #> mz   32.623  6.0815 20.7031 44.542 8.128e-08 #> os    2.182  0.6316  0.9442  3.420 5.504e-04 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"bivariate-probit-model","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Bivariate Probit model","title":"Analysis of bivariate binomial data: Twin analysis","text":"First testing dependence MZ DZ recommend comparing correlations MZ DZ twins. Apart regression correction mean un-structured model, useful concordance casewise concordance estimates can reported analysis.","code":"library(mets) data(twinstut) twinstut <- subset(twinstut,zyg%in%c(\"mz\",\"dz\")) twinstut$binstut <- 1*(twinstut$stutter==\"yes\") head(twinstut) #>    tvparnr zyg stutter    sex age nr binstut #> 1        1  mz      no female  71  1       0 #> 2        1  mz      no female  71  2       0 #> 3        2  dz      no female  71  1       0 #> 8        5  mz      no female  71  1       0 #> 9        5  mz      no female  71  2       0 #> 11       7  dz      no   male  71  1       0 twinstut <- subset(twinstut,tvparnr < 2000) b1 <- bptwin(binstut~sex,data=twinstut,id=\"tvparnr\",zyg=\"zyg\",DZ=\"dz\",type=\"un\") #> Warning in environment(mytr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated #> Warning in environment(myinvtr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated summary(b1) #>  #>               Estimate  Std.Err        Z   p-value     #> (Intercept)   -2.04432  0.10246 -19.9532 < 2.2e-16 *** #> sexmale        0.41203  0.12392   3.3249 0.0008844 *** #> atanh(rho) MZ  1.11748  0.43188   2.5875 0.0096683 **  #> atanh(rho) DZ  0.23000  0.25889   0.8884 0.3743295     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  583/1192    212/354              #>  #>                            Estimate 2.5%     97.5%    #> Tetrachoric correlation MZ  0.80669  0.26456  0.96139 #> Tetrachoric correlation DZ  0.22603 -0.27052  0.62759 #>  #> MZ: #>                      Estimate 2.5%     97.5%    #> Concordance           0.00886  0.00272  0.02840 #> Casewise Concordance  0.43282  0.13214  0.79272 #> Marginal              0.02046  0.01258  0.03312 #> Rel.Recur.Risk       21.15321  2.70681 39.59960 #> log(OR)               4.15336  1.96005  6.34666 #> DZ: #>                      Estimate 2.5%     97.5%    #> Concordance           0.00128  0.00013  0.01240 #> Casewise Concordance  0.06251  0.00705  0.38497 #> Marginal              0.02046  0.01258  0.03312 #> Rel.Recur.Risk        3.05481 -3.12067  9.23029 #> log(OR)               1.20535 -1.09090  3.50161 #>  #>                          Estimate 2.5% 97.5% #> Broad-sense heritability   1      NaN  NaN"},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"polygenic-modelling","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Polygenic modelling","title":"Analysis of bivariate binomial data: Twin analysis","text":"now turn attention specific polygenic modelling special random effects used specify ACE, AE, ADE models forth. easy bptwin function. key parts output sizes genetic component environmental component, can compare results unstructed model . Also formally can test submodel acceptable likelihood ratio test.","code":"b1 <- bptwin(binstut~sex,data=twinstut,id=\"tvparnr\",zyg=\"zyg\",DZ=\"dz\",type=\"ace\") #> Warning in environment(mytr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated #> Warning in environment(myinvtr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated #> Warning in environment(dmytr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated summary(b1) #>  #>              Estimate   Std.Err       Z p-value    #> (Intercept)  -4.20958   1.55369 -2.7094 0.00674 ** #> sexmale       0.85990   0.36387  2.3632 0.01812 *  #> log(var(A))   1.17018   0.99695  1.1738 0.24049    #> log(var(C)) -23.53038       NaN     NaN     NaN    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  583/1192    212/354              #>  #>                    Estimate 2.5%     97.5%    #> A                   0.76318  0.41002  1.11633 #> C                   0.00000  0.00000  0.00000 #> E                   0.23682 -0.11633  0.58998 #> MZ Tetrachoric Cor  0.76318  0.15672  0.95170 #> DZ Tetrachoric Cor  0.38159  0.19280  0.54313 #>  #> MZ: #>                      Estimate 2.5%     97.5%    #> Concordance           0.00768  0.00201  0.02893 #> Casewise Concordance  0.37943  0.09711  0.77658 #> Marginal              0.02025  0.01241  0.03289 #> Rel.Recur.Risk       18.73516 -0.13385 37.60417 #> log(OR)               3.85127  1.58182  6.12072 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.00230  0.00077 0.00688 #> Casewise Concordance 0.11369  0.05266 0.22841 #> Marginal             0.02025  0.01241 0.03289 #> Rel.Recur.Risk       5.61371  2.19257 9.03484 #> log(OR)              1.92764  1.16374 2.69154 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.76318  0.41002 1.11633 b0 <- bptwin(binstut~sex,data=twinstut,id=\"tvparnr\",zyg=\"zyg\",DZ=\"dz\",type=\"ae\") #> Warning in environment(mytr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated #> Warning in environment(myinvtr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated #> Warning in environment(dmytr) <- baseenv(): setting environment(<primitive #> function>) is not possible and trying it is deprecated summary(b0) #>  #>             Estimate  Std.Err       Z p-value    #> (Intercept) -4.20958  1.55369 -2.7094 0.00674 ** #> sexmale      0.85990  0.36387  2.3632 0.01812 *  #> log(var(A))  1.17018  0.99695  1.1738 0.24049    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  583/1192    212/354              #>  #>                    Estimate 2.5%     97.5%    #> A                   0.76318  0.41002  1.11633 #> E                   0.23682 -0.11633  0.58998 #> MZ Tetrachoric Cor  0.76318  0.15672  0.95170 #> DZ Tetrachoric Cor  0.38159  0.19280  0.54313 #>  #> MZ: #>                      Estimate 2.5%     97.5%    #> Concordance           0.00768  0.00201  0.02893 #> Casewise Concordance  0.37943  0.09711  0.77658 #> Marginal              0.02025  0.01241  0.03289 #> Rel.Recur.Risk       18.73517 -0.13386 37.60419 #> log(OR)               3.85127  1.58182  6.12072 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.00230  0.00077 0.00688 #> Casewise Concordance 0.11369  0.05266 0.22841 #> Marginal             0.02025  0.01241 0.03289 #> Rel.Recur.Risk       5.61371  2.19257 9.03484 #> log(OR)              1.92764  1.16374 2.69154 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.76318  0.41002 1.11633"},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"additive-gamma-random-effects","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Additive gamma random effects","title":"Analysis of bivariate binomial data: Twin analysis","text":"Fitting first model different size random effects MZ DZ. note biprobit model dependence much stronger MZ twins. also test parametrizing model intercept. clearly shows significant difference.","code":"theta.des <- model.matrix( ~-1+factor(zyg),data=twinstut) margbin <- glm(binstut~sex,data=twinstut,family=binomial()) bintwin <- binomial.twostage(margbin,data=twinstut,model=\"gamma\",      clusters=twinstut$tvparnr,detail=0,theta=c(0.1)/1,var.link=1,      theta.des=theta.des) summary(bintwin) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>                     theta        se #> factor(zyg)dz -2.13348460 1.0821489 #> factor(zyg)mz -0.07740026 0.5790839 #>  #> $vargam #>               Estimate Std.Err    2.5%  97.5% P-value #> factor(zyg)dz   0.1184  0.1282 -0.1327 0.3696 0.35544 #> factor(zyg)mz   0.9255  0.5360 -0.1249 1.9760 0.08419 #>  #> $type #> [1] \"gamma\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  # test for same dependence in MZ and DZ  theta.des <- model.matrix( ~factor(zyg),data=twinstut) margbin <- glm(binstut~sex,data=twinstut,family=binomial()) bintwin <- binomial.twostage(margbin,data=twinstut,model=\"gamma\",      clusters=twinstut$tvparnr,detail=0,theta=c(0.1)/1,var.link=1,      theta.des=theta.des) summary(bintwin) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>                   theta       se #> (Intercept)   -2.133485 1.082149 #> factor(zyg)mz  2.056084 1.227348 #>  #> $vargam #>               Estimate Std.Err     2.5%   97.5% P-value #> (Intercept)     0.1184  0.1282  -0.1327  0.3696  0.3554 #> factor(zyg)mz   7.8153  9.5921 -10.9849 26.6155  0.4152 #>  #> $type #> [1] \"gamma\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"polygenic-modelling-1","dir":"Articles","previous_headings":"Biprobit with random effects","what":"Polygenic modelling","title":"Analysis of bivariate binomial data: Twin analysis","text":"First setting random effects design random effects relationship variance parameters. see genetic random effect size one MZ 0.5 DZ subjects, shared non-shared genetic components variance 0.5 total genetic variance subjects. shared environmental effect samme . Thus two parameters bands. Now, fitting ACE model, see variance genetic, component, 1.5 environmental variance -0.5. Thus suggesting ACE model fit data. random design given automatically use gamma fralty model. model estimate concordance casewise concordance well marginal rates stuttering females. E component consistent fit data now consider instead AE model. , concordance can computed:","code":"out <- twin.polygen.design(twinstut,id=\"tvparnr\",zygname=\"zyg\",zyg=\"dz\",type=\"ace\") head(cbind(out$des.rv,twinstut$tvparnr),10) #>    MZ DZ DZns1 DZns2 env    #> 1   1  0     0     0   1  1 #> 2   1  0     0     0   1  1 #> 3   0  1     1     0   1  2 #> 8   1  0     0     0   1  5 #> 9   1  0     0     0   1  5 #> 11  0  1     1     0   1  7 #> 12  0  1     1     0   1  8 #> 13  0  1     0     1   1  8 #> 15  0  1     1     0   1 10 #> 18  0  1     1     0   1 12 out$pardes #>      [,1] [,2] #> [1,]  1.0    0 #> [2,]  0.5    0 #> [3,]  0.5    0 #> [4,]  0.5    0 #> [5,]  0.0    1 margbin <- glm(binstut~sex,data=twinstut,family=binomial()) bintwin1 <- binomial.twostage(margbin,data=twinstut,      clusters=twinstut$tvparnr,detail=0,theta=c(0.1)/1,var.link=0,      random.design=out$des.rv,theta.des=out$pardes) summary(bintwin1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                  theta        se #> dependence1  1.1700713 0.9350041 #> dependence2 -0.2552978 0.5910697 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err     2.5%  97.5% P-value #> dependence1   1.2791  0.6039  0.09552 2.4626 0.03416 #> dependence2  -0.2791  0.6039 -1.46265 0.9045 0.64397 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err    2.5% 97.5% P-value #> p1   0.9148  0.5352 -0.1343 1.964 0.08744 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" concordanceTwinACE(bintwin1,type=\"ace\") #> $MZ #>                      Estimate  Std.Err     2.5%   97.5%   P-value #> concordance          0.009738 0.004010 0.001878 0.01760 1.517e-02 #> casewise concordance 0.476141 0.190613 0.102547 0.84974 1.249e-02 #> marginal             0.020452 0.005083 0.010489 0.03041 5.733e-05 #>  #> $DZ #>                      Estimate  Std.Err       2.5%   97.5%   P-value #> concordance          0.001301 0.001158 -0.0009681 0.00357 2.611e-01 #> casewise concordance 0.063604 0.057255 -0.0486137 0.17582 2.666e-01 #> marginal             0.020452 0.005083  0.0104894 0.03041 5.733e-05 out <- twin.polygen.design(twinstut,id=\"tvparnr\",zygname=\"zyg\",zyg=\"dz\",type=\"ae\")  bintwin <- binomial.twostage(margbin,data=twinstut,      clusters=twinstut$tvparnr,detail=0,theta=c(0.1)/1,var.link=0,      random.design=out$des.rv,theta.des=out$pardes) summary(bintwin) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 theta        se #> dependence1 0.8485392 0.4941264 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err 2.5% 97.5% P-value #> dependence1        1       0    1     1       0 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err    2.5% 97.5% P-value #> p1   0.8485  0.4941 -0.1199 1.817 0.08593 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" concordanceTwinACE(bintwin,type=\"ae\") #> $MZ #>                      Estimate  Std.Err     2.5%   97.5%   P-value #> concordance          0.009236 0.003778 0.001831 0.01664 1.450e-02 #> casewise concordance 0.451605 0.189131 0.080916 0.82229 1.695e-02 #> marginal             0.020452 0.005083 0.010489 0.03041 5.733e-05 #>  #> $DZ #>                      Estimate   Std.Err      2.5%    97.5%   P-value #> concordance          0.001966 0.0007099 0.0005741 0.003357 5.628e-03 #> casewise concordance 0.096106 0.0196561 0.0575803 0.134631 1.012e-06 #> marginal             0.020452 0.0050831 0.0104894 0.030415 5.733e-05"},{"path":"http://kkholst.github.io/mets/articles/binomial-twin.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Analysis of bivariate binomial data: Twin analysis","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            ucminf_1.2.2        htmlwidgets_1.6.4   #> [28] Rcpp_1.1.0          future_1.68.0       lattice_0.22-7      #> [31] systemfonts_1.3.1   digest_0.6.39       R6_2.6.1            #> [34] parallelly_1.46.0   parallel_4.5.2      splines_4.5.2       #> [37] Matrix_1.7-4        bslib_0.9.0         tools_4.5.2         #> [40] globals_0.18.0      survival_3.8-3      pkgdown_2.2.0       #> [43] cachem_1.1.0        desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/binreg-TRS.html","id":"two-stage-randomization-for-for-competing-risks-and-survival-outcomes","dir":"Articles","previous_headings":"","what":"Two-Stage Randomization for for Competing risks and Survival outcomes","title":"Two-Stage Randomization for for Competing risks and Survival outcomes","text":"two-stage randomization can estimate average treatment effect E(Y(,k‾))E(Y(,\\bar k)) treatment regime (,k‾)(,\\bar k). responses, randomization A1 = (k_1), treatment k_1 response*A1 = (k_1, k_2), treatment k_1 response 1, treatment k_2 response 2. estimator can agumented different ways: using two randomizations dynamic censoring augmentatation. Estimating μi,k‾=P(Y(,k‾,ϵ=v)<=t)\\mu_{,\\bar k} = P(Y(,\\bar k,\\epsilon=v) <= t), restricted mean E(min(Y(,k‾),τ))E( \\min(Y(,\\bar k),\\tau)) years lost E((ϵ=v)⋅(τ−min(Y(,k‾),τ)))E( (\\epsilon=v) \\cdot (\\tau  -  \\min(Y(,\\bar k),\\tau))) using IPCW weighted estimating equations : \\ solved estimating eqution ∑iI(min(Ti,t)<Gi)Gc(min(Ti,t))(T≤t,ϵ=1)−AUG0−AUG1+AUGC−p(,j))=0\\begin{align*}  \\sum_i  \\frac{(min(T_i,t) < G_i)}{G_c(min(T_i ,t))} (T \\leq t, \\epsilon=1 ) - AUG_0 - AUG_1 + AUG_C  -  p(,j)) = 0   \\end{align*} using covariates augmentR0 augment AUG0=A0()−π0()π0()X0γ0\\begin{align*}  AUG_0 = \\frac{A_0() - \\pi_0()}{ \\pi_0()} X_0 \\gamma_0  \\end{align*} using covariates augmentR1 augment AUG1=A0()π0()A1(j)−π1(j)π1(j)X1γ1\\begin{align*}  AUG_1 = \\frac{A_0()}{\\pi_0()} \\frac{A_1(j) - \\pi_1(j)}{ \\pi_1(j)} X_1 \\gamma_1  \\end{align*} censoring augmenting AUGC=∫0tγc(s)T(e(s)−e‾(s))1Gc(s)dMc(s)\\begin{align*}   AUG_C =  \\int_0^t \\gamma_c(s)^T (e(s) - \\bar e(s))  \\frac{1}{G_c(s) } dM_c(s)   \\end{align*} γc(s)\\gamma_c(s) chosen minimize variance given dynamic covariates specified augmentC. treatment’s must given factors. Treatment probabilities estimated default uncertainty adjusted . Randomization augmentation 1’st 2’nd randomization possible. Censoring model possibly stratified observed covariates (time 0). Censoring augmentation done dynamically time time-dependent covariates. Standard errors estimated using influence function estimators tests differences can therefore computed subsequently. Data must given start,stop,status survival format one code status indicating response, 2nd randomization codes defines outcome interest","code":"library(mets)  set.seed(100)  n <- 200 ddf <- mets:::gsim(n,covs=1,null=0,cens=1,ce=1,betac=c(0.3,1)) true <- apply(ddf$TTt<2,2,mean) true #> [1] 0.740 0.715 0.340 0.350 datat <- ddf$datat ## set-random response on data, only relevant after status==2  response <- rbinom(n,1,0.5) datat$response <- as.factor(response[datat$id]*datat$Count2) datat$A000 <- as.factor(1) datat$A111 <- as.factor(1)  bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,         treat.model0=A0.f~+1, treat.model1=A1.f~A0.f,         augmentR1=~X11+X12+TR, augmentR0=~X01+X02,         augmentC=~X01+X02+A11t+A12t+X11+X12+TR, cens.model=~strata(A0.f)) bb #> Simple estimator : #>                              coef            #> A0.f=1, response*A1.f=1 0.5723748 0.19721139 #> A0.f=1, response*A1.f=2 0.7354573 0.08755875 #> A0.f=2, response*A1.f=1 0.2834829 0.10028201 #> A0.f=2, response*A1.f=2 0.4790373 0.10007200 #>  #> First Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.5900338 0.21419118 #> A0.f=1, response*A1.f=2 0.7428143 0.08889887 #> A0.f=2, response*A1.f=1 0.2721992 0.10288280 #> A0.f=2, response*A1.f=2 0.4671886 0.10273188 #>  #> Second Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.5564110 0.24533756 #> A0.f=1, response*A1.f=2 0.7185240 0.09841671 #> A0.f=2, response*A1.f=1 0.2778309 0.09021056 #> A0.f=2, response*A1.f=2 0.4685339 0.09839184 #>  #> 1st and 2nd Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.5981472 0.26485855 #> A0.f=1, response*A1.f=2 0.7302633 0.10170090 #> A0.f=2, response*A1.f=1 0.2699247 0.09093482 #> A0.f=2, response*A1.f=2 0.4620365 0.09859532  estimate(coef=bb$riskG$riskG01[,1],vcov=crossprod(bb$riskG.iid$riskG01)) #>                         Estimate Std.Err    2.5%  97.5%   P-value #> A0.f=1, response*A1.f=1   0.5981 0.26486 0.07903 1.1173 2.392e-02 #> A0.f=1, response*A1.f=2   0.7303 0.10170 0.53093 0.9296 6.946e-13 #> A0.f=2, response*A1.f=1   0.2699 0.09093 0.09170 0.4482 2.994e-03 #> A0.f=2, response*A1.f=2   0.4620 0.09860 0.26879 0.6553 2.783e-06 estimate(coef=bb$riskG$riskG01[,1],vcov=crossprod(bb$riskG.iid$riskG01),f=function(p) c(p[1]/p[2],p[3]/p[4])) #>                         Estimate Std.Err   2.5% 97.5%  P-value #> A0.f=1, response*A1.f=1   0.8191  0.3600 0.1135 1.525 0.022884 #> A0.f=2, response*A1.f=1   0.5842  0.2249 0.1433 1.025 0.009399 estimate(coef=bb$riskG$riskG01[,1],vcov=crossprod(bb$riskG.iid$riskG01),f=function(p) c(p[1]-p[2],p[3]-p[4])) #>                         Estimate Std.Err    2.5%   97.5% P-value #> A0.f=1, response*A1.f=1  -0.1321   0.266 -0.6534 0.38920  0.6194 #> A0.f=2, response*A1.f=1  -0.1921   0.129 -0.4450 0.06076  0.1365 ## 2 levels for each response , fixed weights  datat$response.f <- as.factor(datat$response) bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,         treat.model0=A0.f~+1, treat.model1=A1.f~A0.f*response.f,         augmentR0=~X01+X02, augmentR1=~X11+X12,         augmentC=~X01+X02+A11t+A12t+X11+X12+TR, cens.model=~strata(A0.f),         estpr=c(0,0),pi0=0.5,pi1=0.5) bb #> Simple estimator : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.5195752 0.18171540 #> A0.f=1, response.f*A1.f=2,1 0.5264182 0.18212146 #> A0.f=1, response.f*A1.f=1,2 0.6343225 0.09083112 #> A0.f=1, response.f*A1.f=2,2 0.6411655 0.08810024 #> A0.f=2, response.f*A1.f=1,1 0.3023142 0.11081204 #> A0.f=2, response.f*A1.f=2,1 0.3199096 0.09719853 #> A0.f=2, response.f*A1.f=1,2 0.5294417 0.13399244 #> A0.f=2, response.f*A1.f=2,2 0.5470372 0.13251846 #>  #> First Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.6359737 0.21658343 #> A0.f=1, response.f*A1.f=2,1 0.6425345 0.21728891 #> A0.f=1, response.f*A1.f=1,2 0.7129831 0.06901176 #> A0.f=1, response.f*A1.f=2,2 0.7195438 0.06513763 #> A0.f=2, response.f*A1.f=1,1 0.2561355 0.12249210 #> A0.f=2, response.f*A1.f=2,1 0.2790365 0.10488801 #> A0.f=2, response.f*A1.f=1,2 0.4557345 0.14361723 #> A0.f=2, response.f*A1.f=2,2 0.4786356 0.14226926 #>  #> Second Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.4046498 0.19742912 #> A0.f=1, response.f*A1.f=2,1 0.5687969 0.23376677 #> A0.f=1, response.f*A1.f=1,2 0.5910450 0.09523643 #> A0.f=1, response.f*A1.f=2,2 0.6498579 0.08872953 #> A0.f=2, response.f*A1.f=1,1 0.3039991 0.10638060 #> A0.f=2, response.f*A1.f=2,1 0.3311109 0.09138384 #> A0.f=2, response.f*A1.f=1,2 0.5569428 0.11332978 #> A0.f=2, response.f*A1.f=2,2 0.5164913 0.12796725 #>  #> 1st and 2nd Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.5212352 0.21098387 #> A0.f=1, response.f*A1.f=2,1 0.6994484 0.26830813 #> A0.f=1, response.f*A1.f=1,2 0.6692919 0.07617047 #> A0.f=1, response.f*A1.f=2,2 0.7328127 0.06615399 #> A0.f=2, response.f*A1.f=1,1 0.2586853 0.11531506 #> A0.f=2, response.f*A1.f=2,1 0.2935352 0.09702298 #> A0.f=2, response.f*A1.f=1,2 0.4809089 0.11457277 #> A0.f=2, response.f*A1.f=2,2 0.4602799 0.13349781  ## 2 levels for each response ,  estimated treat probabilities bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,         treat.model0=A0.f~+1, treat.model1=A1.f~A0.f*response.f,         augmentR0=~X01+X02, augmentR1=~X11+X12,         augmentC=~X01+X02+A11t+A12t+X11+X12+TR, cens.model=~strata(A0.f),estpr=c(1,1)) bb #> Simple estimator : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.5733301 0.25088070 #> A0.f=1, response.f*A1.f=2,1 0.6022300 0.25101611 #> A0.f=1, response.f*A1.f=1,2 0.6893786 0.07700325 #> A0.f=1, response.f*A1.f=2,2 0.7182785 0.08178121 #> A0.f=2, response.f*A1.f=1,1 0.2851197 0.09997901 #> A0.f=2, response.f*A1.f=2,1 0.2841367 0.07893846 #> A0.f=2, response.f*A1.f=1,2 0.4821020 0.10343782 #> A0.f=2, response.f*A1.f=2,2 0.4811190 0.10002228 #>  #> First Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.5934059 0.27131225 #> A0.f=1, response.f*A1.f=2,1 0.6224367 0.27204244 #> A0.f=1, response.f*A1.f=1,2 0.6962560 0.07772829 #> A0.f=1, response.f*A1.f=2,2 0.7252868 0.08319005 #> A0.f=2, response.f*A1.f=1,1 0.2738115 0.10244557 #> A0.f=2, response.f*A1.f=2,1 0.2767634 0.08081569 #> A0.f=2, response.f*A1.f=1,2 0.4661741 0.10481333 #> A0.f=2, response.f*A1.f=2,2 0.4691260 0.10238718 #>  #> Second Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.4372672 0.23706174 #> A0.f=1, response.f*A1.f=2,1 0.5854429 0.26329507 #> A0.f=1, response.f*A1.f=1,2 0.6685369 0.07730893 #> A0.f=1, response.f*A1.f=2,2 0.7320359 0.08145477 #> A0.f=2, response.f*A1.f=1,1 0.2745442 0.10426421 #> A0.f=2, response.f*A1.f=2,1 0.2988823 0.07578047 #> A0.f=2, response.f*A1.f=1,2 0.5019477 0.09984288 #> A0.f=2, response.f*A1.f=2,2 0.4745778 0.10218550 #>  #> 1st and 2nd Randomization Augmentation : #>                                  coef            #> A0.f=1, response.f*A1.f=1,1 0.4702327 0.25293879 #> A0.f=1, response.f*A1.f=2,1 0.6127080 0.28587900 #> A0.f=1, response.f*A1.f=1,2 0.6759149 0.07701506 #> A0.f=1, response.f*A1.f=2,2 0.7433289 0.08370622 #> A0.f=2, response.f*A1.f=1,1 0.2634297 0.10565890 #> A0.f=2, response.f*A1.f=2,1 0.2948997 0.07656016 #> A0.f=2, response.f*A1.f=1,2 0.4865367 0.09860797 #> A0.f=2, response.f*A1.f=2,2 0.4699980 0.10193373   ## 2 and 3 levels for each response , fixed weights  datat$A1.23.f <- as.numeric(datat$A1.f) dtable(datat,~A1.23.f+response) #>  #>         response   0   1 #> A1.23.f                  #> 1                120  23 #> 2                120  25 datat <- dtransform(datat,A1.23.f=2+rbinom(nrow(datat),1,0.5),             Count2==1 & A1.23.f==2 & response==0) dtable(datat,~A1.23.f+response) #>  #>         response   0   1 #> A1.23.f                  #> 1                120  23 #> 2                111  25 #> 3                  9   0 datat$A1.23.f <- as.factor(datat$A1.23.f) dtable(datat,~A1.23.f+response|Count2==1) #>  #>         response  0  1 #> A1.23.f                #> 1                21 23 #> 2                10 25 #> 3                 9  0 ### bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,     treat.model0=A0.f~+1,treat.model1=A1.23.f~A0.f*response.f,     augmentR0=~X01+X02, augmentR1=~X11+X12,     augmentC=~X01+X02+A11t+A12t+X11+X12+TR, cens.model=~strata(A0.f),         estpr=c(1,0),pi1=c(0.3,0.5)) bb #> Simple estimator : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.5928568 0.20464868 #> A0.f=1, response.f*A1.23.f=2,1 0.6055291 0.20241441 #> A0.f=1, response.f*A1.23.f=3,1 0.5539792 0.19865003 #> A0.f=1, response.f*A1.23.f=1,2 0.7203538 0.09728960 #> A0.f=1, response.f*A1.23.f=2,2 0.7330260 0.08618558 #> A0.f=1, response.f*A1.23.f=3,2 0.6814762 0.07905734 #> A0.f=2, response.f*A1.23.f=1,1 0.3487105 0.14756981 #> A0.f=2, response.f*A1.23.f=2,1 0.2724117 0.09182351 #> A0.f=2, response.f*A1.23.f=3,1 0.2669705 0.10319223 #> A0.f=2, response.f*A1.23.f=1,2 0.5551901 0.15399346 #> A0.f=2, response.f*A1.23.f=2,2 0.4788913 0.12263546 #> A0.f=2, response.f*A1.23.f=3,2 0.4734501 0.12853150 #>  #> First Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.6091794 0.22507560 #> A0.f=1, response.f*A1.23.f=2,1 0.6246762 0.22364858 #> A0.f=1, response.f*A1.23.f=3,1 0.5756603 0.21982886 #> A0.f=1, response.f*A1.23.f=1,2 0.7242102 0.10066515 #> A0.f=1, response.f*A1.23.f=2,2 0.7397070 0.08813234 #> A0.f=1, response.f*A1.23.f=3,2 0.6906911 0.07986256 #> A0.f=2, response.f*A1.23.f=1,1 0.3348012 0.15239160 #> A0.f=2, response.f*A1.23.f=2,1 0.2718558 0.09060009 #> A0.f=2, response.f*A1.23.f=3,1 0.2532931 0.10673462 #> A0.f=2, response.f*A1.23.f=1,2 0.5362379 0.15754640 #> A0.f=2, response.f*A1.23.f=2,2 0.4732925 0.12378699 #> A0.f=2, response.f*A1.23.f=3,2 0.4547298 0.13192021 #>  #> Second Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.4707095 0.21804098 #> A0.f=1, response.f*A1.23.f=2,1 0.6287053 0.22907037 #> A0.f=1, response.f*A1.23.f=3,1 0.6778953 0.30311581 #> A0.f=1, response.f*A1.23.f=1,2 0.6499002 0.12488660 #> A0.f=1, response.f*A1.23.f=2,2 0.7656237 0.07140976 #> A0.f=1, response.f*A1.23.f=3,2 0.6875010 0.08312068 #> A0.f=2, response.f*A1.23.f=1,1 0.2656521 0.17396985 #> A0.f=2, response.f*A1.23.f=2,1 0.2706333 0.08715854 #> A0.f=2, response.f*A1.23.f=3,1 0.3235400 0.07994612 #> A0.f=2, response.f*A1.23.f=1,2 0.4759907 0.16071698 #> A0.f=2, response.f*A1.23.f=2,2 0.4671860 0.11714753 #> A0.f=2, response.f*A1.23.f=3,2 0.5050170 0.11067832 #>  #> 1st and 2nd Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.5068915 0.22640669 #> A0.f=1, response.f*A1.23.f=2,1 0.6601925 0.25328437 #> A0.f=1, response.f*A1.23.f=3,1 0.7128045 0.32561624 #> A0.f=1, response.f*A1.23.f=1,2 0.6681178 0.12092323 #> A0.f=1, response.f*A1.23.f=2,2 0.7709766 0.07310528 #> A0.f=1, response.f*A1.23.f=3,2 0.7088981 0.08171051 #> A0.f=2, response.f*A1.23.f=1,1 0.2472033 0.17493179 #> A0.f=2, response.f*A1.23.f=2,1 0.2703258 0.08650158 #> A0.f=2, response.f*A1.23.f=3,1 0.3183141 0.08043082 #> A0.f=2, response.f*A1.23.f=1,2 0.4540746 0.15704473 #> A0.f=2, response.f*A1.23.f=2,2 0.4667557 0.11617870 #> A0.f=2, response.f*A1.23.f=3,2 0.4960792 0.11127903   ## 2 and 3 levels for each response , estimated  bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,     treat.model0=A0.f~+1, treat.model1=A1.23.f~A0.f*response.f,     augmentR0=~X01+X02, augmentR1=~X11+X12,     augmentC=~X01+X02+A11t+A12t+X11+X12+TR,cens.model=~strata(A0.f),     estpr=c(1,1)) bb #> Simple estimator : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.5733301 0.25088123 #> A0.f=1, response.f*A1.23.f=2,1 0.6486249 0.25863637 #> A0.f=1, response.f*A1.23.f=3,1 0.5558352 0.25058134 #> A0.f=1, response.f*A1.23.f=1,2 0.6893788 0.07700333 #> A0.f=1, response.f*A1.23.f=2,2 0.7646736 0.11893993 #> A0.f=1, response.f*A1.23.f=3,2 0.6718839 0.07549715 #> A0.f=2, response.f*A1.23.f=1,1 0.2851198 0.09997906 #> A0.f=2, response.f*A1.23.f=2,1 0.2795959 0.10086233 #> A0.f=2, response.f*A1.23.f=3,1 0.2893264 0.12751179 #> A0.f=2, response.f*A1.23.f=1,2 0.4821024 0.10343788 #> A0.f=2, response.f*A1.23.f=2,2 0.4765785 0.12129977 #> A0.f=2, response.f*A1.23.f=3,2 0.4863090 0.13928327 #>  #> First Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.5934060 0.27131280 #> A0.f=1, response.f*A1.23.f=2,1 0.6665511 0.27965619 #> A0.f=1, response.f*A1.23.f=3,1 0.5783224 0.27132992 #> A0.f=1, response.f*A1.23.f=1,2 0.6962562 0.07772837 #> A0.f=1, response.f*A1.23.f=2,2 0.7694013 0.12078013 #> A0.f=1, response.f*A1.23.f=3,2 0.6811727 0.07593118 #> A0.f=2, response.f*A1.23.f=1,1 0.2738116 0.10244561 #> A0.f=2, response.f*A1.23.f=2,1 0.2791785 0.10064910 #> A0.f=2, response.f*A1.23.f=3,1 0.2740035 0.13235513 #> A0.f=2, response.f*A1.23.f=1,2 0.4661745 0.10481340 #> A0.f=2, response.f*A1.23.f=2,2 0.4715413 0.12263585 #> A0.f=2, response.f*A1.23.f=3,2 0.4663664 0.14396909 #>  #> Second Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.4372664 0.23706211 #> A0.f=1, response.f*A1.23.f=2,1 0.5867572 0.24940274 #> A0.f=1, response.f*A1.23.f=3,1 0.6818858 0.34853309 #> A0.f=1, response.f*A1.23.f=1,2 0.6685368 0.07730905 #> A0.f=1, response.f*A1.23.f=2,2 0.7678408 0.07768404 #> A0.f=1, response.f*A1.23.f=3,2 0.6794316 0.07967452 #> A0.f=2, response.f*A1.23.f=1,1 0.2745441 0.10426433 #> A0.f=2, response.f*A1.23.f=2,1 0.2695053 0.09394894 #> A0.f=2, response.f*A1.23.f=3,1 0.3345432 0.09931154 #> A0.f=2, response.f*A1.23.f=1,2 0.5019478 0.09984303 #> A0.f=2, response.f*A1.23.f=2,2 0.4603587 0.11962056 #> A0.f=2, response.f*A1.23.f=3,2 0.5047271 0.12508846 #>  #> 1st and 2nd Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.23.f=1,1 0.4702319 0.25293915 #> A0.f=1, response.f*A1.23.f=2,1 0.6117617 0.26980849 #> A0.f=1, response.f*A1.23.f=3,1 0.7132649 0.36786766 #> A0.f=1, response.f*A1.23.f=1,2 0.6759149 0.07701519 #> A0.f=1, response.f*A1.23.f=2,2 0.7733904 0.07957475 #> A0.f=1, response.f*A1.23.f=3,2 0.7066309 0.07677686 #> A0.f=2, response.f*A1.23.f=1,1 0.2634296 0.10565903 #> A0.f=2, response.f*A1.23.f=2,1 0.2691166 0.09330578 #> A0.f=2, response.f*A1.23.f=3,1 0.3296791 0.10056744 #> A0.f=2, response.f*A1.23.f=1,2 0.4865368 0.09860813 #> A0.f=2, response.f*A1.23.f=2,2 0.4605674 0.11777635 #> A0.f=2, response.f*A1.23.f=3,2 0.4971161 0.12478451   ## 2 and 1 level for each response  datat$A1.21.f <- as.numeric(datat$A1.f) dtable(datat,~A1.21.f+response|Count2==1) #>  #>         response  0  1 #> A1.21.f                #> 1                21 23 #> 2                19 25 datat <- dtransform(datat,A1.21.f=1,Count2==1 & response==1) dtable(datat,~A1.21.f+response|Count2==1) #>  #>         response  0  1 #> A1.21.f                #> 1                21 48 #> 2                19  0 datat$A1.21.f <- as.factor(datat$A1.21.f) dtable(datat,~A1.21.f+response|Count2==1) #>  #>         response  0  1 #> A1.21.f                #> 1                21 48 #> 2                19  0 bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,     treat.model0=A0.f~+1, treat.model1=A1.21.f~A0.f*response.f,     augmentR0=~X01+X02, augmentR1=~X11+X12,     augmentC=~X01+X02+A11t+A12t+X11+X12+TR,cens.model=~strata(A0.f),     estpr=c(1,1)) bb #> Simple estimator : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6352226 0.11813045 #> A0.f=1, response.f*A1.21.f=2,1 0.6641226 0.12002926 #> A0.f=2, response.f*A1.21.f=1,1 0.3865954 0.09118205 #> A0.f=2, response.f*A1.21.f=2,1 0.3856124 0.07700508 #>  #> First Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6482593 0.12794220 #> A0.f=1, response.f*A1.21.f=2,1 0.6772901 0.13063539 #> A0.f=2, response.f*A1.21.f=1,1 0.3729074 0.09195877 #> A0.f=2, response.f*A1.21.f=2,1 0.3758593 0.07808807 #>  #> Second Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6435175 0.11802262 #> A0.f=1, response.f*A1.21.f=2,1 0.6882148 0.11332360 #> A0.f=2, response.f*A1.21.f=1,1 0.3954927 0.09148962 #> A0.f=2, response.f*A1.21.f=2,1 0.3782504 0.08450470 #>  #> 1st and 2nd Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6685224 0.13033444 #> A0.f=1, response.f*A1.21.f=2,1 0.7138566 0.12655030 #> A0.f=2, response.f*A1.21.f=1,1 0.3839378 0.08986106 #> A0.f=2, response.f*A1.21.f=2,1 0.3758166 0.08434474  ## known weights  bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),datat,time=2,cause=c(1),response.code=2,     treat.model0=A0.f~+1, treat.model1=A1.21.f~A0.f*response.f,     augmentR0=~X01+X02, augmentR1=~X11+X12,     augmentC=~X01+X02+A11t+A12t+X11+X12+TR,     cens.model=~strata(A0.f),estpr=c(1,0),pi1=c(0.5,1)) bb #> Simple estimator : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6410543 0.12005600 #> A0.f=1, response.f*A1.21.f=2,1 0.6486576 0.11864403 #> A0.f=2, response.f*A1.21.f=1,1 0.3780709 0.09171015 #> A0.f=2, response.f*A1.21.f=2,1 0.3940667 0.08377301 #>  #> First Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6532872 0.12997608 #> A0.f=1, response.f*A1.21.f=2,1 0.6625853 0.12899925 #> A0.f=2, response.f*A1.21.f=1,1 0.3647406 0.09331039 #> A0.f=2, response.f*A1.21.f=2,1 0.3842369 0.08422015 #>  #> Second Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6435110 0.12122751 #> A0.f=1, response.f*A1.21.f=2,1 0.6751763 0.11432293 #> A0.f=2, response.f*A1.21.f=1,1 0.3957743 0.08413935 #> A0.f=2, response.f*A1.21.f=2,1 0.3772145 0.09110553 #>  #> 1st and 2nd Randomization Augmentation : #>                                     coef            #> A0.f=1, response.f*A1.21.f=1,1 0.6709750 0.13165583 #> A0.f=1, response.f*A1.21.f=2,1 0.7043078 0.12732297 #> A0.f=2, response.f*A1.21.f=1,1 0.3861201 0.08279377 #> A0.f=2, response.f*A1.21.f=2,1 0.3757694 0.09080739"},{"path":"http://kkholst.github.io/mets/articles/binreg-TRS.html","id":"two-stage-randomization-calgb-9823","dir":"Articles","previous_headings":"","what":"Two-Stage Randomization CALGB-9823","title":"Two-Stage Randomization for for Competing risks and Survival outcomes","text":"illustrate analysis one SMART conducted Cancer Leukemia Group B Protocol 8923 (CALGB 8923), Stone others (2001). 388 patients randomized initial treatment GM-CSF (A1 ) standard chemotherapy (A2 ). Patients complete remission informed consent second stage re-randomized cytarabine (B1 ) cytarabine plus mitoxantrone (B2 ). first compute weighted risk-set estimator based estimated weights ΛA1,B1(t)=∑∫0twi(s)Yw(s)dNi(s)\\begin{align*} \\Lambda_{A1,B1}(t) & = \\sum_i \\int_0^t \\frac{w_i(s)}{Y^w(s)} dN_i(s) \\end{align*} wi(s)=(A0i=A1)+(t>TR)(A1i=B1)/π1(Xi)w_i(s) = (A0_i=A1) + (t>T_R) (A1_i=B1)/\\pi_1(X_i), 1 start treatment A1A1 changes B1B1 time TRT_R scaled proportion . equivalent IPTW (inverse probability treatment weighted estimator). estimate treatment regimes A1,B1A1, B1 A2,B1A2, B1 letting A10A10 indicate consistent ending B1B1. A10A10 starts 11 becomes 00 subject treated B2B2, stays 11 subject treated B1B1. can look two strata A0=0,A10=1A0=0,A10=1 A0=1,A10=1A0=1,A10=1. Similary, end consistent B2B2. Thus defining A11A11 start 11, stays 11 B2B2 taken, becomes 00 second randomization B1B1. treatment models time-points, unless weight.var variable given (1 treatments, 0 otherwise) accomodate general start,stop format treatment model may also depend response value standard errors based influence functions also computed baseline use propensity score model P(A1=B1|A0)P(A1=B1|A0) uses observed frequencies arm B1B1 among starting either A1A1 A2A2.  propensity score mode can extended use covariates get increased efficiency. Note also propensity scores A0A0 cancel different strata.","code":"data(calgb8923) calgt <- calgb8923  tm=At.f~factor(Count2)+age+sex+wbc tm=At.f~factor(Count2) tm=At.f~factor(Count2)*A0.f  head(calgt) #>   id V X Z   TR R     U delta  stop age   wbc sex race      time status start #> 1  1 0 0 0 0.00 0 13.33     1 13.33  64 128.0   1    1 13.338219      1  0.00 #> 2  2 1 1 0 0.00 0 17.80     1 17.80  71   4.3   2    1 17.802995      1  0.00 #> 3  3 1 0 0 0.00 0  1.27     1  1.27  71  43.6   2    1  1.271527      1  0.00 #> 4  4 1 0 1 0.00 0 24.77     1 24.77  63  72.3   2    1  0.730000      2  0.00 #> 5  4 1 0 1 0.73 1 24.77     1 24.77  63  72.3   2    1 24.772515      1  0.73 #> 6  5 0 1 0 0.00 0 10.37     1 10.37  65   1.4   1    1 10.374479      1  0.00 #>   A0.f A0 A1 A11 A12 A1.f A10 At.f lbnr__id Count1 Count2 consent trt2 trt1 #> 1    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 2    1  1  0   1   0    0   0    1        1      0      0      -1   -1    2 #> 3    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 4    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 5    0  0  1   1   1    1   1    1        2      0      1       1    1    1 #> 6    1  1  0   1   0    0   0    1        1      0      0      -1   -1    2 ll0 <- phreg_IPTW(Event(start,time,status==1)~strata(A0,A10)+cluster(id),calgt,treat.model=tm) pll0 <- predict(ll0,expand.grid(A0=0:1,A10=0,id=1)) ll1 <- phreg_IPTW(Event(start,time,status==1)~strata(A0,A11)+cluster(id),calgt,treat.model=tm) pll1 <- predict(ll1,expand.grid(A0=0:1,A11=1,id=1)) plot(pll0,se=1,lwd=2,col=1:2,lty=1,xlab=\"time (months)\",xlim=c(0,30)) plot(pll1,add=TRUE,col=3:4,se=1,lwd=2,lty=1,xlim=c(0,30)) abline(h=0.25) legend(\"topright\",c(\"A1B1\",\"A2B1\",\"A1B2\",\"A2B2\"),col=c(1,2,3,4),lty=1) summary(pll1,times=12) #> $pred #> [1] 0.4556676 0.5029214 #>  #> $se.pred #> [1] 0.04212703 0.04165848 #>  #> $lower #> [1] 0.3801487 0.4275556 #>  #> $upper #> [1] 0.5461888 0.5915721 #>  #> $times #> [1] 12 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\" summary(pll0,times=12) #> $pred #> [1] 0.3950461 0.4279272 #>  #> $se.pred #> [1] 0.04263714 0.04342426 #>  #> $lower #> [1] 0.3197260 0.3507467 #>  #> $upper #> [1] 0.4881098 0.5220911 #>  #> $times #> [1] 12 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\""},{"path":"http://kkholst.github.io/mets/articles/binreg-TRS.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Two-Stage Randomization for for Competing risks and Survival outcomes","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/binreg-ate.html","id":"average-treatment-effect","dir":"Articles","previous_headings":"","what":"Average treatment effect","title":"Average treatment effect (ATE) for Competing risks and binary outcomes","text":"First simulate data mimics Kumar et al 2012. data multiple myeloma patients treated allogeneic stem cell transplantation Center International Blood Marrow Transplant Research (CIBMTR) Kumar et al (2012), “Trends allogeneic stem cell transplantation multiple myeloma: CIBMTR analysis”. data used paper consist patients transplanted 1995 2005, compared outcomes transplant periods: 2001-2005 (N=488) versus 1995-2000 (N=375). two competing events relapse (cause 2) treatment-related mortality (TRM, cause 1) defined death without relapse. considered following risk covariates: transplant time period (gp (main interest study): 1 transplanted 2001-2005 versus 0 transplanted 1995-2000), donor type (dnr: 1 Unrelated related donor (N=280) versus 0 HLA-identical sibling (N=584)), prior autologous transplant (preauto: 1 Auto+Allo transplant (N=399) versus 0 allogeneic transplant alone (N=465)) time transplant (ttt24: 1 24 months (N=289) versus 0 less equal 24 months (N=575))). generate similar data assuming two cumlative incidence curves logistic censoring depends covariates via Cox model. wrapped kumarsim function. simulation deal possible violations bound F1+F2<1F_1+F_2 < 1. increase sample size still see recover parameters cause 2. note estimates found using large censoring model different using simple Kaplan-Meier weights severely biased data. due strong censoring dependence. average treatment around 0.17=E(Y(1)−Y(0))0.17 = E(Y(1) - Y(0)) time 60 transplant period, standard causal assumptions. 1/0 treatment variable used causal computation found right hand side (rhs) treat.model first argument rhs response model. binregATE default uses binreg default fit working model recommended, logitIPCW logitIPCWATE can also used GLM-type IPCW weighted models (see binreg help page/vignette). cluster argument used logitIPCWATE, works binregATE.","code":"library(mets)  set.seed(100) ### n <- 400 kumar <- kumarsim(n,depcens=1) kumar$cause <- kumar$status kumar$ttt24 <- kumar[,6] dtable(kumar,~cause) #>  #> cause #>   0   1   2  #> 182  72 146 dfactor(kumar) <- gp.f~gp kumar$id <- 1:n kumar$idc <- sample(100,n,TRUE) kumar$ids <- sample(n,n) kumar$id2 <- sample(n,n) kumar2 <- kumar[order(kumar$id2),] kumar$int <- interaction(kumar$gp,kumar$dnr) kumar2$int <- interaction(kumar2$gp,kumar2$dnr) clust <- 0  b2 <- binregATE(Event(time,cause)~gp.f+dnr+preauto+ttt24,kumar,cause=2,     treat.model=gp.f~dnr+preauto+ttt24,time=40,cens.model=~strata(gp,dnr)) summary(b2) #>    n events #>  400    137 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.310227  0.221121 -1.743616 -0.876838  0.0000 #> gp.f1        0.898030  0.271691  0.365526  1.430535  0.0009 #> dnr          0.323059  0.271545 -0.209160  0.855278  0.2342 #> preauto      0.269177  0.278181 -0.276049  0.814402  0.3332 #> ttt24        0.496202  0.276045 -0.044836  1.037240  0.0722 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.26976 0.17489 0.4161 #> gp.f1        2.45476 1.44127 4.1809 #> dnr          1.38135 0.81127 2.3520 #> preauto      1.30889 0.75878 2.2578 #> ttt24        1.64247 0.95615 2.8214 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    0.293021 0.039148 0.216292 0.369750   0e+00 #> treat1    0.496481 0.041744 0.414665 0.578297   0e+00 #> treat:1-0 0.203460 0.060294 0.085285 0.321635   7e-04 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    0.311984 0.042914 0.227875 0.396094   0e+00 #> treat1    0.512310 0.042454 0.429103 0.595518   0e+00 #> treat:1-0 0.200326 0.060585 0.081582 0.319070   9e-04  b5 <- binregATE(Event(time,cause)~int+preauto+ttt24,kumar,cause=2,         treat.model=int~preauto+ttt24,cens.code=0,time=60) summary(b5) #>    n events #>  400    142 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.115768  0.285147 -1.674646 -0.556890  0.0001 #> int1.0       0.674582  0.333967  0.020018  1.329146  0.0434 #> int0.1       0.034751  0.494229 -0.933920  1.003421  0.9439 #> int1.1       1.340278  0.431989  0.493594  2.186961  0.0019 #> preauto      0.298967  0.272324 -0.234779  0.832713  0.2723 #> ttt24        0.476755  0.286955 -0.085667  1.039177  0.0966 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.32766 0.18737 0.5730 #> int1.0       1.96321 1.02022 3.7778 #> int0.1       1.03536 0.39301 2.7276 #> int1.1       3.82010 1.63819 8.9081 #> preauto      1.34846 0.79075 2.2995 #> ttt24        1.61084 0.91790 2.8269 #>  #> Average Treatment effects (G-formula) : #>                 Estimate    Std.Err       2.5%      97.5% P-value #> treat0.0       0.3124362  0.0586931  0.1973999  0.4274724  0.0000 #> treat1.0       0.4676985  0.0422018  0.3849846  0.5504125  0.0000 #> treat0.1       0.3197899  0.0896402  0.1440983  0.4954815  0.0004 #> treat1.1       0.6274521  0.0726051  0.4851488  0.7697554  0.0000 #> treat:1.0-0.0  0.1552624  0.0738341  0.0105502  0.2999745  0.0355 #> treat:0.1-0.0  0.0073537  0.1048541 -0.1981565  0.2128640  0.9441 #> treat:1.1-0.0  0.3150160  0.0978046  0.1233225  0.5067094  0.0013 #> treat:0.1-1.0 -0.1479086  0.1003040 -0.3445009  0.0486837  0.1403 #> treat:1.1-1.0  0.1597536  0.0845518 -0.0059649  0.3254721  0.0588 #> treat:1.1-0.1  0.3076622  0.1178093  0.0767602  0.5385643  0.0090 #>  #> Average Treatment effects (double robust) : #>                 Estimate    Std.Err       2.5%      97.5% P-value #> treat0.0       0.3580588  0.0674640  0.2258318  0.4902858  0.0000 #> treat1.0       0.4871368  0.0501492  0.3888461  0.5854274  0.0000 #> treat0.1       0.3102783  0.1210473  0.0730299  0.5475266  0.0104 #> treat1.1       0.7645588  0.1887893  0.3945387  1.1345790  0.0001 #> treat:1.0-0.0  0.1290779  0.0831680 -0.0339284  0.2920843  0.1207 #> treat:0.1-0.0 -0.0477806  0.1400295 -0.3222333  0.2266722  0.7329 #> treat:1.1-0.0  0.4065000  0.2040550  0.0065596  0.8064404  0.0464 #> treat:0.1-1.0 -0.1768585  0.1315359 -0.4346641  0.0809471  0.1788 #> treat:1.1-1.0  0.2774221  0.1937486 -0.1023182  0.6571624  0.1522 #> treat:1.1-0.1  0.4542806  0.2102654  0.0421680  0.8663931  0.0307 ib2 <- logitIPCWATE(Event(time,cause)~gp.f+dnr+preauto+ttt24,kumar,cause=2,     treat.model=gp.f~dnr+preauto+ttt24,time=40,cens.model=~strata(gp,dnr)) summary(ib2) #>    n events #>  400    137 #>  #>  400 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept) -1.27557  0.21917 -1.70514 -0.84601  0.0000 #> gp.f1        0.83337  0.25904  0.32566  1.34108  0.0013 #> dnr          0.36815  0.27656 -0.17390  0.91020  0.1831 #> preauto      0.48521  0.30772 -0.11791  1.08832  0.1148 #> ttt24        0.18958  0.30947 -0.41698  0.79614  0.5401 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.27927 0.18175 0.4291 #> gp.f1        2.30105 1.38494 3.8232 #> dnr          1.44506 0.84038 2.4848 #> preauto      1.62452 0.88878 2.9693 #> ttt24        1.20874 0.65904 2.2170 #>  #> Average Treatment effects (G-formula) : #>         Estimate  Std.Err     2.5%    97.5% P-value #> treat-1 0.493059 0.040018 0.414625 0.571494  0.0000 #> treat-0 0.302695 0.040274 0.223759 0.381632  0.0000 #> p1      0.190364 0.058248 0.076200 0.304527  0.0011 #>  #> Average Treatment effects (double robust) : #>         Estimate  Std.Err     2.5%    97.5% P-value #> treat-1 0.524459 0.043164 0.439860 0.609058   0e+00 #> treat-0 0.309211 0.043507 0.223938 0.394483   0e+00 #> p1      0.215248 0.061268 0.095165 0.335331   4e-04  ib5 <- logitIPCW(Event(time,cause)~gp.f+dnr+preauto+ttt24,kumar,cause=2,cens.code=0,      time=60,cens.model=~strata(gp,dnr)) summary(ib5) #>    n events #>  400    142 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.142446  0.225267 -1.583960 -0.700931  0.0000 #> gp.f1        0.716372  0.267369  0.192339  1.240405  0.0074 #> dnr          0.551548  0.325499 -0.086418  1.189515  0.0902 #> preauto      0.748078  0.343802  0.074239  1.421917  0.0296 #> ttt24       -0.184844  0.373708 -0.917298  0.547610  0.6209 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.31904 0.20516 0.4961 #> gp.f1        2.04699 1.21208 3.4570 #> dnr          1.73594 0.91721 3.2855 #> preauto      2.11294 1.07706 4.1451 #> ttt24        0.83123 0.39960 1.7291  ibs <- logitIPCW(Event(time,cause)~gp.f+dnr+preauto+ttt24,kumar,cause=2,cens.code=0,time=60) summary(ibs) #>    n events #>  400    142 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.294250  0.220215 -1.725864 -0.862636  0.0000 #> gp.f1        1.633772  0.294651  1.056267  2.211277  0.0000 #> dnr          0.020551  0.309235 -0.585538  0.626639  0.9470 #> preauto      0.547275  0.305442 -0.051381  1.145931  0.0732 #> ttt24        0.288357  0.309124 -0.317515  0.894228  0.3509 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.27410 0.17802 0.4220 #> gp.f1        5.12316 2.87562 9.1274 #> dnr          1.02076 0.55681 1.8713 #> preauto      1.72854 0.94992 3.1454 #> ttt24        1.33423 0.72796 2.4454  check <- 0 if (check==1) { require(riskRegression) e.wglm <- wglm( regressor.event=~gp.f+dnr+preauto+ttt24, formula.censor = Surv(time,cause==0)~+1, times = 60, data = kumar, product.limit=TRUE,cause=2) summary(e.wglm)$coef estimate(ibs)  es.wglm <- wglm( regressor.event=~gp.f+dnr+preauto+ttt24,  formula.censor = Surv(time,cause==0)~strata(gp,dnr), times = 60,  data = kumar, product.limit=TRUE,cause=2) summary(es.wglm)$coef estimate(ib5) }"},{"path":"http://kkholst.github.io/mets/articles/binreg-ate.html","id":"average-treatment-for-competing-risks-data","dir":"Articles","previous_headings":"","what":"Average treatment for Competing risks data","title":"Average treatment effect (ATE) for Competing risks and binary outcomes","text":"binreg function direct binomial regression one time-point, tt, fitting model P(T≤t,ϵ=1|X)=expit(XTβ),\\begin{align*} P(T \\leq t, \\epsilon=1 | X )  & = \\mbox{expit}( X^T \\beta), \\end{align*} possible right censored data. estimation procedure based IPCW adjusted estimating equation (EE) U(β)=X(Δ(t)(T≤t,ϵ=1)/Gc(T∧t)−expit(XTbeta))=0\\begin{align*}  U(\\beta) = &  X \\left( \\Delta(t) (T \\leq t, \\epsilon=1 )/G_c(T \\wedge t) - \\mbox{expit}( X^T beta) \\right) = 0  \\end{align*} Gc(t)=P(C>t)G_c(t)=P(C > t), censoring survival distribution, Δ(t)=(C>T∧t)\\Delta(t) = ( C > T \\wedge t) indicator uncensored time tt. function logitIPCW instead considers EE EE U(β)=XΔ(t)Gc(T∧t)((T≤t,ϵ=1)−expit(XTbeta))=0.\\begin{align*}  U(\\beta) = &  X  \\frac{\\Delta(t)}{G_c(T \\wedge t)} \\left( (T \\leq t, \\epsilon=1 ) - \\mbox{expit}( X^T beta) \\right) = 0. \\end{align*} two score equations quite similar, exactly censoring model fully-nonparametric given XX. seems binreg estimating equations often preferable use, estimating equation used also augmented default implementation (see binreg vignette). Additional functions logitATE, binregATE computes average treatment effect. demonstrate use . functions binregATE (recommended) logitATE also works censoring thus simple binary outcome. Variance based sandwich formula IPCW adjustment, naive.var variance known censoring model. influence functions stored output. , standard errors can cluster corrected specifying relevant cluster working outcome model. Using working logistic model resonse (possibly cluster specification) binregATE can also handle factor two levels uses mlogit multinomial regression function (mets). Using working model censoring given covariates, must stratified Kaplan-Meier. censoring censoring weights simply set 1. average treatment effect E(Y(1)−Y(0))\\begin{align*}  E(Y(1) - Y(0))  \\end{align*} using counterfactual outcomes. compute simple G-estimator ∑ma(Xi)\\begin{align*} \\sum  m_a(X_i)  \\end{align*} estimate risk E(Y())E(Y()). DR-estimator instead uses estimating equations double robust wrt working logistic model resonse working logistic model treatment given covariates estimated using estimator ∑[AiYiπA(Xi)−Ai−πA(Xi)πA(Xi)m1(Xi)]−[(1−Ai)Yi1−πA(Xi)+Ai−πA(Xi)1−πA(Xi)m0(Xi)]\\begin{align*} \\sum \\left[ \\frac{A_i Y_i}{\\pi_A(X_i)}-\\frac{A_i - \\pi_A(X_i)}{\\pi_A(X_i)} m_1(X_i) \\right]    - \\left[ \\frac{(1-A_i) Y_i}{1-\\pi_A(X_i)}+\\frac{A_i - \\pi_A(X_i)}{1-\\pi_A(X_i)} m_0(X_i) \\right] \\end{align*} AiA_i treatment indicator πA(Xi)=P(Ai=1|Xi)\\pi_A(X_i) = P(A_i=1|X_i) treatment model YiY_i outcome, case censoring censoring adjusted ỸiΔ(t)/Gc(Ti−∧t)\\tilde Y_i \\Delta(t) /G_c(T_i- \\wedge t) Ỹ=(Ti≤t,ϵi=1)\\tilde Y_i = (T_i \\leq t, \\epsilon_i=1) oucome censoring. mj(Xi)=P(Yi=1|Ai=j,Xi)m_j(X_i)=P(Y_i=1| A_i=j,X_i) outcome model, using binomial regression. standard errors based iid decomposition using taylor-expansions parameters treatment-model outcome-model, censoring probability. need censoring model correct, can important use sufficiently large censorng model also illustrate . censoring model can specified strata (used phreg also compute standard marginalization average treatment effect (called differenceG) ∑[m1(Xi)−m0(Xi)]\\begin{align*} \\sum \\left[  m_1(X_i) - m_0(X_i) \\right] \\end{align*} standard errors based related influcence functions also returned. large data 2 treatment groups computations can memory extensive many covariates due multinomial-regression model used propensity scores. Otherwise function (binregATE) run large data. ATE functions need treatment given first variable right hand side outcome model factor. variable also indentified left hand side treatment model (treat.model), per default assumes treatment depend covariates.","code":""},{"path":"http://kkholst.github.io/mets/articles/binreg-ate.html","id":"average-treatment-effect-for-binary-or-continuous-responses","dir":"Articles","previous_headings":"","what":"Average treatment effect for binary or continuous responses","title":"Average treatment effect (ATE) for Competing risks and binary outcomes","text":"binary case binary outcome specified instead survival outcome, consequence -censoring adjustment done binary/numeric outcome must variable data-frame Running code (can also use binregATE koding cause without censorings values, setting cens.code=2, time large) continuous response using normal estimating equations","code":"kumar$cause2 <- 1*(kumar$cause==2)  b3 <- logitATE(cause2~gp.f+dnr+preauto+ttt24,kumar,treat.model=gp.f~dnr+preauto+ttt24) summary(b3) #>    n events #>  400    400 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.219960  0.200830 -1.613580 -0.826341  0.0000 #> gp.f1        0.387595  0.243738 -0.090123  0.865314  0.1118 #> dnr          0.633992  0.241410  0.160837  1.107147  0.0086 #> preauto      0.139356  0.248680 -0.348049  0.626761  0.5752 #> ttt24        0.449527  0.243475 -0.027675  0.926730  0.0648 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.29524 0.19917 0.4376 #> gp.f1        1.47343 0.91382 2.3758 #> dnr          1.88512 1.17449 3.0257 #> preauto      1.14953 0.70606 1.8715 #> ttt24        1.56757 0.97270 2.5262 #>  #> Average Treatment effects (G-formula) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.316084  0.037954  0.241695  0.390472  0.0000 #> treat1     0.400809  0.033395  0.335356  0.466262  0.0000 #> treat:1-0  0.084726  0.052651 -0.018468  0.187919  0.1076 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.343451  0.042284  0.260577  0.426325  0.0000 #> treat1     0.421615  0.033616  0.355729  0.487500  0.0000 #> treat:1-0  0.078164  0.053961 -0.027599  0.183926  0.1475  ###library(targeted) ###b3a <- ate(cause2~gp.f|dnr+preauto+ttt24| dnr+preauto+ttt24,kumar,family=binomial) ###summary(b3a)  ## calculate also relative risk estimate(coef=b3$riskDR,vcov=b3$var.riskDR,f=function(p) p[1]/p[2]) #>        Estimate Std.Err   2.5% 97.5%   P-value #> treat0   0.8146  0.1194 0.5807 1.049 8.831e-12 b3 <- normalATE(time~gp.f+dnr+preauto+ttt24,kumar,treat.model=gp.f~dnr+preauto+ttt24) summary(b3) #>    n events #>  400    400 #>  #>  400 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)  43.4758   4.0796  35.4800  51.4716  0.0000 #> gp.f1       -27.2211   4.1379 -35.3313 -19.1109  0.0000 #> dnr           2.9485   4.4768  -5.8259  11.7228  0.5101 #> preauto      -2.8172   3.6191  -9.9105   4.2760  0.4363 #> ttt24        -1.9787   4.1285 -10.0704   6.1131  0.6318 #>  #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0     42.3170   3.8362  34.7982  49.8357       0 #> treat1     15.0958   1.3284  12.4922  17.6994       0 #> treat:1-0 -27.2211   4.1379 -35.3313 -19.1109       0 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0     42.3200   4.0417  34.3985  50.2415       0 #> treat1     15.1545   1.2580  12.6888  17.6201       0 #> treat:1-0 -27.1655   4.2405 -35.4768 -18.8543       0"},{"path":"http://kkholst.github.io/mets/articles/binreg-ate.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Average treatment effect (ATE) for Competing risks and binary outcomes","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"binomial-regression-for-censored-data","dir":"Articles","previous_headings":"","what":"Binomial Regression for censored data","title":"Binomial Regression for Survival and Competing Risks Data","text":"binreg function can fit logistic link model IPCW adjustment specific time-point, can thus used describing survival competing risks data. function can used large data completely scalable, , linear data. nice feature influcence functions computed available, can thus used settings based parameters. addition summarize censoring weights can strata dependent predictions can computed standard errors including standard errors clusters can given cluster corrected standard errors computed","code":""},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"details","dir":"Articles","previous_headings":"","what":"Details","title":"Binomial Regression for Survival and Competing Risks Data","text":"binreg function direct binomial regression one time-point, tt, fitting model P(T≤t,ϵ=1|X)=expit(XTβ)=F1(t,X,β)\\begin{align*} P(T \\leq t, \\epsilon=1 | X )  & = \\mbox{expit}( X^T \\beta)  = F_1(t,X,\\beta) \\end{align*} IPCW adjusted estmating equation (EE) response Y(t)=(T≤t,ϵ=1)Y(t)=(T \\leq t, \\epsilon=1 )U(β,Ĝc)=X(Y(t)Δ(t)Ĝc(Ti∧t)−expit(XTβ))=0,\\begin{align*}  U(\\beta,\\hat G_c) = &  X ( Y(t) \\frac{ \\Delta(t) }{\\hat G_c(T_i \\wedge t)} - \\mbox{expit}( X^T \\beta)) = 0, \\end{align*} Gc(t)=P(C>t)G_c(t)=P(C>t), censoring survival distribution, Δ(t)=(Ci>Ti∧t)\\Delta(t) = ( C_i > T_i \\wedge t) indicator uncensored time tt (type=“”). default type=“II” augment censoring term, solve U(β,Ĝc)+∫0tXÊ(Y(t)|T>u)Ĝc(u)dM̂c(u)=0\\begin{align*}   &  U(\\beta,\\hat G_c) + \\int_0^t X \\frac{\\hat E(Y(t)| T>u)}{\\hat G_c(u)} d\\hat M_c(u) =0  \\end{align*} Mc(u)M_c(u) censoring martingale, typically improves performance. equivlent pseudo-value approach (see Overgaard (2025)). influence function type=“II” estimator U(β,Gc)+∫0tXE(Y|T>u)Gc(u)dMc(u)−∫0tE(X|T>u)E(Y|T>u)Gc(u)dMc(u)−∫0tE(XY|T>u)Gc(u)dMc(u)\\begin{align*}     U(\\beta,G_c) + \\int_0^t X \\frac{E(Y| T>u)}{G_c(u)} d M_c(u)       - \\int_0^t  \\frac{E(X| T>u) E(Y| T>u)}{G_c(u)} d M_c(u) - \\int_0^t \\frac{E( X Y| T>u)}{G_c(u)} d M_c(u)   \\end{align*} type=“” U(β)+∫0tE(XY|T>u)Gc(u)dMc(u).\\begin{align*}   &  U(\\beta) + \\int_0^t \\frac{E( X Y| T>u)}{G_c(u)} d M_c(u). \\end{align*} means E(XY(t)|T>u)E(X Y(t) | T>u) E(Y(t)|T>u)E(Y(t)| T>u) estimated IPCW estimators among survivors get estimates influence functions. function logitIPCW instead considers Uglm(β,Ĝc)=Δ(t)Ĝc(Ti∧t)X(Y(t)−expit(XTβ))=0.\\begin{align*}  U^{glm}(\\beta,\\hat G_c) = & \\frac{ \\Delta(t) }{\\hat G_c(T_i \\wedge t)}  X  ( Y(t) - \\mbox{expit}( X^T \\beta)) = 0. \\end{align*} score equation quite similar binreg, exactly censoring model fully-nonparametric. logitIPCW influence function Uglm(β,Gc)+∫0tE(X(Y−F1(t,β))|T>u)Gc(u)dMc(u)\\begin{align*}   &  U^{glm}(\\beta,G_c) + \\int_0^t \\frac{E( X ( Y - F_1(t,\\beta)) | T>u)}{G_c(u)} d M_c(u)   \\end{align*} estimator performs best depends censoring distribution seems binreg type=“II” performs overall quite nicely (see Blanche et al (2023) Overgaard (2024)). full estimated censoring model estimators influence function (see Blanche et al (2023)). Additional functions logitATE, binregATE computes average treatment effect. demonstrate another vignette. functions logitATE/binregATE can used censoring thus simple binary outcome. variance based sandwich formula IPCW adjustment (using influence functions), naive.var variance known censoring model. influence functions stored output. Clusters can specified get cluster corrected standard errors.","code":""},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Binomial Regression for Survival and Competing Risks Data","text":"can also compute predictions using estimates censoring model can depend strata","code":"library(mets)  options(warn=-1)  set.seed(1000) # to control output in random noise just below.  data(bmt)  bmt$time <- bmt$time+runif(nrow(bmt))*0.01  bmt$id <- 1:408   # logistic regresion with IPCW binomial regression   out <- binreg(Event(time,cause)~tcell+platelet,bmt,time=50)  summary(out) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.180338  0.126748 -0.428760  0.068084  0.1548 #> tcell       -0.418545  0.345480 -1.095675  0.258584  0.2257 #> platelet    -0.437644  0.240978 -0.909952  0.034665  0.0694 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.83499 0.65132 1.0705 #> tcell        0.65800 0.33431 1.2951 #> platelet     0.64556 0.40254 1.0353 predict(out,data.frame(tcell=c(0,1),platelet=c(1,1)),se=TRUE) #>        pred         se     lower     upper #> 1 0.3502406 0.04847582 0.2552280 0.4452533 #> 2 0.2618207 0.06969334 0.1252217 0.3984196 outs <- binreg(Event(time,cause)~tcell+platelet,bmt,time=50,cens.model=~strata(tcell,platelet))  summary(outs) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.180697  0.127414 -0.430424  0.069030  0.1561 #> tcell       -0.365928  0.350632 -1.053154  0.321299  0.2967 #> platelet    -0.433494  0.240270 -0.904415  0.037428  0.0712 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.83469 0.65023 1.0715 #> tcell        0.69355 0.34884 1.3789 #> platelet     0.64824 0.40478 1.0381"},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"absolute-risk-differences-and-ratio","dir":"Articles","previous_headings":"","what":"Absolute risk differences and ratio","title":"Binomial Regression for Survival and Competing Risks Data","text":"Now illustrations wish consider absolute risk difference depending tcell risk difference Getting standard errors easy enough since two-groups independent. case addition adjusted covariates, however, need apply delta-theorem thus using relevant covariances along lines ","code":"outs <- binreg(Event(time,cause)~tcell,bmt,time=50,cens.model=~strata(tcell))  summary(outs) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept) -0.30054  0.11153 -0.51914 -0.08194  0.0070 #> tcell       -0.51741  0.33981 -1.18342  0.14860  0.1278 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.74042 0.59503 0.9213 #> tcell        0.59606 0.30623 1.1602 ps <-  predict(outs,data.frame(tcell=c(0,1)),se=TRUE) ps #>        pred         se     lower     upper #> 1 0.4254253 0.02726306 0.3719897 0.4788609 #> 2 0.3061988 0.06819019 0.1725461 0.4398516 sum( c(1,-1) * ps[,1]) #> [1] 0.1192264 dd <- data.frame(tcell=c(0,1)) p <- predict(outs,dd)  riskdifratio <- function(p,contrast=c(1,-1)) {    outs$coef <- p    p <- predict(outs,dd)[,1]    pd <- sum(contrast*p)    r1 <- p[1]/p[2]    r2 <- p[2]/p[1]    return(c(pd,r1,r2)) }       estimate(outs,f=riskdifratio,dd,null=c(0,1,1)) #>      Estimate Std.Err     2.5%  97.5% P-value #> [p1]   0.1192 0.07344 -0.02471 0.2632 0.10448 #> [p2]   1.3894 0.32197  0.75833 2.0204 0.22652 #> [p3]   0.7197 0.16679  0.39284 1.0467 0.09291 #>  #>  Null Hypothesis:  #>   [p1] = 0 #>   [p2] = 1 #>   [p3] = 1  #>   #> chisq = 12.0249, df = 3, p-value = 0.007298 run <- 0 if (run==1) { library(prodlim) pl <- prodlim(Hist(time,cause)~tcell,bmt) spl <- summary(pl,times=50,asMatrix=TRUE) spl }"},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"augmenting-the-binomial-regression","dir":"Articles","previous_headings":"","what":"Augmenting the Binomial Regression","title":"Binomial Regression for Survival and Competing Risks Data","text":"Rather using larger censoring model can also compute augmentation term fit binomial regression model based augmentation term. compute augmentation based stratified non-parametric estimates F1(t,S(X))F_1(t,S(X)), S(X)S(X) gives strata based XX working model. Computes augmentation term individual well sum =∫0tH(u,X)1S*(u,s)1Gc(u)dMc(u)\\begin{align*}  & = \\int_0^t H(u,X) \\frac{1}{S^*(u,s)} \\frac{1}{G_c(u)} dM_c(u) \\end{align*} H(u,X)=F1*(t,S(X))−F1*(u,S(X))\\begin{align*}  H(u,X) & = F_1^*(t,S(X)) - F_1^*(u,S(X)) \\end{align*} using KM Gc(t)G_c(t) working model cumulative baseline related F1*(t,s)F_1^*(t,s) ss strata, S*(t,s)=1−F1*(t,s)−F2*(t,s)S^*(t,s) = 1 - F_1^*(t,s) - F_2^*(t,s). Standard errors computed assumption correct estimated Gc(s)G_c(s) model.","code":"data(bmt)  dcut(bmt,breaks=2) <- ~age   out1<-BinAugmentCifstrata(Event(time,cause)~platelet+agecat.2+               strata(platelet,agecat.2),data=bmt,cause=1,time=40)  summary(out1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>                      Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)          -0.51295  0.17090 -0.84791 -0.17799  0.0027 #> platelet             -0.63011  0.23585 -1.09237 -0.16785  0.0075 #> agecat.2(0.203,1.94]  0.55926  0.21211  0.14353  0.97500  0.0084 #>  #> exp(coeffients): #>                      Estimate    2.5%  97.5% #> (Intercept)           0.59873 0.42831 0.8370 #> platelet              0.53253 0.33542 0.8455 #> agecat.2(0.203,1.94]  1.74938 1.15434 2.6512   out2<-BinAugmentCifstrata(Event(time,cause)~platelet+agecat.2+      strata(platelet,agecat.2)+strataC(platelet),data=bmt,cause=1,time=40)  summary(out2) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>                      Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)          -0.51346  0.17109 -0.84879 -0.17814  0.0027 #> platelet             -0.63636  0.23653 -1.09996 -0.17276  0.0071 #> agecat.2(0.203,1.94]  0.56280  0.21229  0.14672  0.97889  0.0080 #>  #> exp(coeffients): #>                      Estimate    2.5%  97.5% #> (Intercept)           0.59842 0.42793 0.8368 #> platelet              0.52922 0.33288 0.8413 #> agecat.2(0.203,1.94]  1.75559 1.15803 2.6615"},{"path":"http://kkholst.github.io/mets/articles/binreg.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Binomial Regression for Survival and Competing Risks Data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"fine-gray-model","dir":"Articles","previous_headings":"","what":"Fine-Gray model","title":"Cumulative Incidence Regression","text":"considered cumulative incidence form F1(t,X)=P(T≤t,ϵ=1)=1−exp(−Λ0(t)exp(XTβ)).\\begin{align*}  F_1(t,X) & = P(T \\leq t, \\epsilon=1) = 1 - exp( - \\Lambda_0(t) \\exp(X^T \\beta)). \\end{align*} case independent right-censoring censoring distribtion Gc(t,X)=P(C>t|S(X))G_c(t,X) = P(C > t | S(X)) S(X)S(X) set strata defined XX, ubiased estimating equation given UnFG(β)=∑=0n∫0+∞(Xi−En(t,β))wi(t,Xi)dN1,(t) En(t,β)=S̃1(t,β)S̃0(t,β),\\begin{align*}   U^{FG}_{n}(\\beta) = \\sum_{=0}^{n} \\int_0^{+\\infty} \\left( X_i- E_n(t,\\beta) \\right) w_i(t,X_i) dN_{1,}(t)  \\text{ }      E_n(t,\\beta)=\\frac{\\tilde S_1(t,\\beta) }{\\tilde S_0(t,\\beta)}, \\end{align*} wi(t,Xi)=Gc(t,Xi)Gc(Ti∧t,Xi)(Ci>Ti∧t)w_i(t,X_i) = \\frac{G_c(t,X_i)}{G_c(T_i \\wedge t,X_i)} ( C_i > T_i \\wedge t ) ,S̃k(t,β)=∑j=1nXjkexp(XjTβ)Y1,j(t)\\tilde S_k(t,\\beta) = \\sum_{j=1}^n X_j^k \\exp(X_j^T\\beta) Y_{1,j}(t) k=0,1k=0,1, Ỹ1,(t)=Y1,(t)wi(t,Xi)\\tilde Y_{1,}(t) = Y_{1,}(t) w_i(t,X_i) =1,...,ni=1,...,n. wi(t)w_i(t) needs replaced estimator censoring distribution, since depend XX ŵ(t)=Ĝc(t,Xi)Ĝc(Ti∧t,Xi)(Ci>Ti∧t)\\hat w_i(t) = \\frac{\\hat G_c(t,X_i)}{\\hat G_c(T_i \\wedge t,X_i)} (C_i > T_i \\wedge t) Ĝc\\hat G_c Kaplan-Meier estimator censoring distribution. First simulate competing risks data using utility functions. simulate data two causes based Fine-Gray model: F1(t,X)=P(T≤t,ϵ=1|X)=(1−exp(−Λ1(t)exp(XTβ1)))F2(t,X)=P(T≤t,ϵ=2|X)=(1−exp(−Λ2(t)exp(XTβ2)))⋅(1−F1(∞,X))\\begin{align} F_1(t,X) &  = P(T\\leq t, \\epsilon=1|X)=( 1 - exp(-\\Lambda_1(t) \\exp(X^T \\beta_1))) \\\\ F_2(t,X) & = P(T\\leq t, \\epsilon=2|X)= ( 1 - exp(-\\Lambda_2(t) \\exp(X^T \\beta_2))) \\cdot (1 - F_1(\\infty,X))   \\end{align} baselines given Λj(t)=ρj(1−exp(−t/νj))\\Lambda_j(t) = \\rho_j (1- exp(-t/\\nu_j)) j=1,2j=1,2, XX two independent binomials. Alternatively, one can also replace FG-model logistic link expit(Λj(t)+exp(XTβj))\\mbox{expit}( \\Lambda_j(t) + \\exp(X^T \\beta_j)). advantage model easy fit get standard errors, quite flexible essentially Cox-model. downside coefficients must interpreted cloglogcloglog-scale. Specifically, log(−log(1−F1(t,X1+1,X2)))−log(−log(1−F1(t,X1,X2)))=β1,\\begin{align} \\log(-\\log( 1-F_1(t,X_1+1,X_2))) - \\log(-\\log( 1-F_1(t,X_1,X_2))) & =  \\beta_1, \\end{align} effect increase X1X_1 β1\\beta_1 leads 1−F1(t,X)1-F_1(t,X) cloglogcloglog scale. look non-parametric cumulative incidence curves  Now fitting Fine-Gray model  GOF based cumulative residuals (Li et al. 2015) showing problem proportionality model.","code":"library(mets)  options(warn=-1)  set.seed(1000) # to control output in simulatins for p-values below.   rho1 <- 0.2; rho2 <- 10  n <- 400  beta=c(0.0,-0.1,-0.5,0.3)  ## beta1=c(0.0,-0.1); beta2=c(-0.5,0.3)  dats <- simul.cifs(n,rho1,rho2,beta,rc=0.5,rate=7)  dtable(dats,~status) #>  #> status #>   0   1   2  #> 127  12 261  dsort(dats) <- ~time par(mfrow=c(1,2))  cifs1 <- cif(Event(time,status)~strata(Z1,Z2),dats,cause=1)  plot(cifs1)   cifs2 <- cif(Event(time,status)~strata(Z1,Z2),dats,cause=2)  plot(cifs2) fg <- cifregFG(Event(time,status)~Z1+Z2,data=dats,cause=1)  summary(fg) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.69686  0.38760  0.38882  0.0722 #> Z2 -0.85929  0.62453  0.61478  0.1689 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  2.00744 0.93911 4.2911 #> Z2  0.42346 0.12451 1.4402   dd <- expand.grid(Z1=c(-1,1),Z2=0:1)  pfg <- predict(fg,dd)  plot(pfg,ylim=c(0,0.2)) gofFG(Event(time,status)~Z1+Z2,data=dats,cause=1) #> Cumulative score process test for Proportionality: #>    Sup|U(t)|  pval #> Z1  3.011461 0.124 #> Z2  1.373513 0.227"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"ses-for-the-baseline-and-predictions-of-fg","dir":"Articles","previous_headings":"","what":"SE’s for the baseline and predictions of FG","title":"Cumulative Incidence Regression","text":"standard errors reported FG-estimator based ..d decompostion (influence functions) estimator give later. similar decompostion exist baseline needed standard errors predictions computed. bit harder compute time-points simultaneously, can obtained specific timepoints jointly iid decomposition regression coefficients used get standard errors predictions. plot predictions jittered confidence intervals predictions time point 5  iid decompostions stored inside Biid, addition note iid decompostions β̂−β0\\hat \\beta - \\beta_0 obtained command iid()","code":"### predictions with CI based on iid decomposition of baseline and beta fg <- cifregFG(Event(time,status)~Z1+Z2,data=dats,cause=1) Biid <- iidBaseline(fg,time=5) pfgse <- FGprediid(Biid,dd) pfgse #>            pred    se-log       lower     upper #> [1,] 0.04253879 0.7418354 0.009938793 0.1820692 #> [2,] 0.16069100 0.3946377 0.074143886 0.3482633 #> [3,] 0.01823957 0.9410399 0.002884032 0.1153531 #> [4,] 0.07149610 0.4611261 0.028958169 0.1765199 plot(pfg,ylim=c(0,0.2)) for (i in 1:4) lines(c(5,5)+i/10,pfgse[i,3:4],col=i,lwd=2)"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"comparison","dir":"Articles","previous_headings":"","what":"Comparison","title":"Cumulative Incidence Regression","text":"compare cmprsk function, gives exactly , without running avoid dependencies: comparing results coxph based setting data using finegray function, get estimates note standard errors coxph missing term therefore slightly different. comparing estimates coxph missing additional censoring term see get also standard errors also remove censorings data compare estimates based coxph, observe estimates well standard errors agree cmprsk also gives ","code":"run <- 0 if (run==1) { library(cmprsk) mm <- model.matrix(~Z1+Z2,dats)[,-1] cr <- with(dats,crr(time,status,mm)) cbind(cr$coef,diag(cr$var)^.5,fg$coef,fg$se.coef,cr$coef-fg$coef,diag(cr$var)^.5-fg$se.coef) #          [,1]      [,2]       [,3]      [,4]          [,5]          [,6] # Z1  0.6968603 0.3876029  0.6968603 0.3876029 -2.442491e-15 -2.553513e-15 # Z2 -0.8592892 0.6245258 -0.8592892 0.6245258 -2.997602e-15  1.776357e-15 } if (run==1) {  library(survival)  dats$id <- 1:nrow(dats)  dats$event <- factor(dats$status,0:2, labels=c(\"censor\", \"death\", \"other\"))  fgdats <- finegray(Surv(time,event)~.,data=dats)  coxfg <- survival::coxph(Surv(fgstart, fgstop, fgstatus) ~ Z1+Z2 + cluster(id), weight=fgwt, data=fgdats)   fg0 <- cifreg(Event(time,status)~Z1+Z2,data=dats,cause=1,propodds=NULL)  cbind( coxfg$coef,fg0$coef, coxfg$coef-fg0$coef) #          [,1]       [,2]          [,3] # Z1  0.6968603  0.6968603 -1.110223e-16 # Z2 -0.8592892 -0.8592892 -1.110223e-15  cbind(diag(coxfg$var)^.5,fg0$se.coef,diag(coxfg$var)^.5-fg0$se.coef) #           [,1]      [,2]          [,3] # [1,] 0.3889129 0.3876029  0.0013099915 # [2,] 0.6241225 0.6245258 -0.0004033148  cbind(diag(coxfg$var)^.5,fg0$se1.coef,diag(coxfg$var)^.5-fg0$se1.coef) #           [,1]      [,2]          [,3] # [1,] 0.3889129 0.3889129 -2.331468e-15 # [2,] 0.6241225 0.6241225  2.553513e-15 } datsnc <- dtransform(dats,status=2,status==0) dtable(datsnc,~status) #>  #> status #>   1   2  #>  12 388 datsnc$id <- 1:n datsnc$entry <- 0 max <- max(dats$time)+1 ## for cause 2 add risk interaval  datsnc2 <- subset(datsnc,status==2) datsnc2 <- transform(datsnc2,entry=time) datsnc2 <- transform(datsnc2,time=max) datsncf <- rbind(datsnc,datsnc2) # cifnc <- cifreg(Event(time,status)~Z1+Z2,data=datsnc,cause=1,propodds=NULL) cc <- phreg(Surv(entry,time,status==1)~Z1+Z2+cluster(id),datsncf) cbind(cc$coef-cifnc$coef, diag(cc$var)^.5-diag(cifnc$var)^.5) #>            [,1]          [,2] #> Z1 1.221245e-15 -1.609823e-15 #> Z2 3.996803e-15  1.887379e-15 #            [,1]          [,2] # Z1 1.332268e-15 -4.440892e-16 # Z2 4.218847e-15  2.220446e-16 if (run==1) {  library(cmprsk)  mm <- model.matrix(~Z1+Z2,datsnc)[,-1]  cr <- with(datsnc,crr(time,status,mm))  cbind(cc$coef-cr$coef, diag(cr$var)^.5-diag(cc$var)^.5) #             [,1]         [,2] # Z1 -4.218847e-15 1.443290e-15 # Z2  7.549517e-15 1.110223e-16 }"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"strata-dependent-censoring-weights","dir":"Articles","previous_headings":"","what":"Strata dependent Censoring weights","title":"Cumulative Incidence Regression","text":"can improve efficiency avoid bias allowing censoring weights depend covariates note standard errors slightly smaller efficient estimator. influence functions FG-estimator given ,ϕiFG=∫(Xi−e(t))w̃(t)dMi1(t,Xi)+∫q(t)π(t)dMic(t),=ϕiFG,1+ϕiFG,2,\\begin{align*} \\phi_i^{FG}  & = \\int (X_i- e(t))  \\tilde w_i(t) dM_{i1}(t,X_i)  + \\int \\frac{q(t)}{\\pi(t)} dM_{ic}(t),  \\\\              & = \\phi_i^{FG,1}  + \\phi_i^{FG,2}, \\end{align*} first term achieved known censoring distribution, second term due variability Kaplan-Meier estimator. Mic(t)=Nic(t)−∫0tYi(s)dΛc(s)M_{ic}(t) = N_{ic}(t) - \\int_0^t Y_i(s) d\\Lambda_c (s) MicM_{ic} standard censoring martingale. function q(t)q(t) reflects censoring affects terms related cause “2” jumps, can written (see Appendix B2) q(t)=E(H(t,X)(T≤t,ϵ=2)(C>T)/Gc(T))=E(H(t,X)F2(t,X)),\\begin{align*}      q(t) & =   E( H(t,X) (T  \\leq t, \\epsilon=2) (C > T)/G_c(T)) = E( H(t,X) F_2(t,X) ),  \\end{align*} H(t,X)=∫t∞(X−e(s))G(s)dΛ1(s,X)H(t,X) =  \\int_t^{\\infty} (X- e(s))  G(s) d \\Lambda_1(s,X) since π(t)=E(Y(t))=S(t)Gc(t)\\pi(t)=E(Y(t))=S(t) G_c(t). case censoring weights stratified (based XX) get influence functions related censoring term q(t,X)=E(H(t,X)(T≤t,ϵ=2)(T<C)/Gc(T,X)|X)=H(t,X)F2(t,X),\\begin{align*}      q(t,X) & =   E( H(t,X) (T  \\leq t, \\epsilon=2) (T < C)/G_c(T,X) | X) = H(t,X) F_2(t,X),  \\end{align*} influence function becomes ∫(X−e(t))w(t)dM1(t,X)+∫H(t,X)F2(t,X)S(t,X)1Gc(t,X)dMc(t,X).\\begin{align*} \\int (X-e(t)) w(t) dM_1(t,X) +  \\int H(t,X) \\frac{F_2(t,X)}{S(t,X)} \\frac{1}{G_c(t,X)} dM_c(t,X). \\end{align*} H(t,X)=∫t∞(X−e(s))G(s,X)dΛ1(s,X)H(t,X) =  \\int_t^{\\infty} (X- e(s))  G(s,X) d \\Lambda_1(s,X).","code":"fgcm <- cifregFG(Event(time,status)~Z1+Z2,data=dats,cause=1,cens.model=~strata(Z1,Z2))  summary(fgcm) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.54277  0.37188  0.39352  0.1444 #> Z2 -0.91846  0.61886  0.61447  0.1378 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  1.72077 0.83019 3.5667 #> Z2  0.39913 0.11867 1.3424  summary(fg) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.69686  0.38760  0.38882  0.0722 #> Z2 -0.85929  0.62453  0.61478  0.1689 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  2.00744 0.93911 4.2911 #> Z2  0.42346 0.12451 1.4402"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"augmenting-the-fg-estimator","dir":"Articles","previous_headings":"","what":"Augmenting the FG-estimator","title":"Cumulative Incidence Regression","text":"Rather using larger censoring model can also compute augmentation term directly fit FG-model based augmentation term couple iterations note slightly smaller standard errors augmenting estimator. function compute augmentation term fixed E(t)E(t) based current β̂\\hat \\betaUnA=∑=1n∫0+∞F2(t,Xi)S(t,Xi)Gc(t,Xi)H(t,Xi,E,Gc,Λ1)dMci(t)\\begin{align*}  U_n^{} = \\sum_{=1}^n \\int_{0}^{+\\infty}  \\frac{F_2(t,X_i)}{S(t,X_i)G_c(t,X_i)}       H(t,X_i,E,G_c,\\Lambda_1)  dM_{ci}(t) \\end{align*} using working models based stratification get F1sF_1^s F2sF_2^s strata given strata()strata() call. fits FG model solve UnA(βp)+UnFG(β)=0.\\begin{align*}  U_n^{}(\\beta_p)  + U^{FG}_{n}(\\beta) = 0.  \\end{align*} may iterate get solution augmented score equation UnA(β∞)+UnFG(β∞)=0.\\begin{align*}  U_n^{}(\\beta_\\infty)  + U^{FG}_{n}(\\beta_\\infty) = 0. \\end{align*} censoring model one overall Kaplan-Meier. influence funtion augmented estimator ∫(X−e(t))w(t)dM1(t,X)+∫H(t,X)F2(t,X)S(t,X)1Gc(t)dMc.\\begin{align*} \\int (X-e(t)) w(t) dM_1(t,X) +  \\int H(t,X) \\frac{F_2(t,X)}{S(t,X)} \\frac{1}{G_c(t)} dM_c. \\end{align*} standard errors based formula.","code":"fgaugS <- FG_AugmentCifstrata(Event(time,status)~Z1+Z2+strata(Z1,Z2),data=dats,cause=1,E=fg$E)   summary(fgaugS) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.69686  0.34898  0.38882  0.0458 #> Z2 -0.85929  0.60243  0.61478  0.1538 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  2.00744 1.01296 3.9783 #> Z2  0.42346 0.13002 1.3791    fgaugS2 <- FG_AugmentCifstrata(Event(time,status)~Z1+Z2+strata(Z1,Z2),data=dats,cause=1,E=fgaugS$E)   summary(fgaugS2) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.69686  0.34898  0.38882  0.0458 #> Z2 -0.85929  0.60243  0.61478  0.1538 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  2.00744 1.01296 3.9783 #> Z2  0.42346 0.13002 1.3791    fgaugS3 <- FG_AugmentCifstrata(Event(time,status)~Z1+Z2+strata(Z1,Z2),data=dats,cause=1,E=fgaugS2$E)   summary(fgaugS3) #>  #>    n events #>  400     12 #>  #>  400 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1  0.69686  0.34898  0.38882  0.0458 #> Z2 -0.85929  0.60243  0.61478  0.1538 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  2.00744 1.01296 3.9783 #> Z2  0.42346 0.13002 1.3791"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"logistic-link","dir":"Articles","previous_headings":"","what":"Logistic-link","title":"Cumulative Incidence Regression","text":"model logit(F1(t,X))=α(t)+XTβ\\begin{align*}  \\mbox{logit}(F_1(t,X)) & =  \\alpha(t) + X^T \\beta \\end{align*} leads interpretation F1F_1, can also fitted easily, however, standard errors harder compute approximative (assuming censoring weights known) gives typically small error. ${{\\bf timereg}}$-package model can fitted using different estimators efficient using different weights much slower. Fitting model getting ’s","code":"rho1 <- 0.2; rho2 <- 10  n <- 400  beta=c(0.0,-0.1,-0.5,0.3)  dats <- simul.cifs(n,rho1,rho2,beta,rc=0.5,rate=7,type=\"logistic\")  dtable(dats,~status) #>  #> status #>   0   1   2  #> 166  16 218  dsort(dats) <- ~time or <- cifreg(Event(time,status)~Z1+Z2,data=dats,cause=1)  summary(or) #>  #>    n events #>  400     16 #>  #>  400 clusters #> coeffients: #>    Estimate    S.E. dU^-1/2 P-value #> Z1  0.10017 0.25562 0.25215  0.6952 #> Z2  0.21763 0.50407 0.50346  0.6659 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  1.10535 0.66976 1.8242 #> Z2  1.24313 0.46287 3.3387"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"administrative-censoring","dir":"Articles","previous_headings":"","what":"Administrative Censoring","title":"Cumulative Incidence Regression","text":"case administrative censoring can simply provide risk-set given administrative censoring times Fine-Gray logistic link cumulative incidence regression models. Fine-Gray model can similarly estimated using modified risk-set phreg function","code":"library(mets) rho1 <- 0.3; rho2 <- 5.9 set.seed(100) n <- 100 beta=c(0.3,-0.3,-0.5,0.3) rc <- 0.9 ### dats <- mets:::simul.cifsRA(n,rho1,rho2,beta,bin=1,rc=rc,rate=c(3,7)) dats$status07 <- dats$status dats$status07[dats$status %in% c(0,7)] <- 0 tt <- seq(0,6,by=0.1) base1 <- rho1*(1-exp(-tt/3))  ccA  <-  cifregFG(Event(timeA,statusA)~Z1+Z2,dats,           adm.cens.time=dats$censorA,no.codes=7) estimate(ccA) #>    Estimate Std.Err    2.5%  97.5% P-value #> Z1  0.08665  0.2116 -0.3280 0.5014  0.6821 #> Z2  0.40535  0.4276 -0.4328 1.2435  0.3432 dats$entry <- 0 dats$id <- 1:n datA <- dats datA2 <- subset(datA,statusA==2) datA2$entry <- datA2$timeA datA2$timeA <- datA2$censorA datA2$statusA <- 0 datA <- rbind(datA,datA2) ddA <- phreg(Event(entry,timeA,statusA==1)~Z1+Z2+cluster(id),datA) estimate(ddA) #>    Estimate Std.Err    2.5%  97.5% P-value #> Z1  0.08665  0.2116 -0.3280 0.5014  0.6821 #> Z2  0.40535  0.4276 -0.4328 1.2435  0.3432  ## also checking the cumulative baseline  ###plotl(tt,base1)  ###plot(ccA,add=TRUE,col=3) ###plot(ddA,col=2,add=TRUE)"},{"path":"http://kkholst.github.io/mets/articles/cifreg.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Cumulative Incidence Regression","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"WIP: Cooking survival data, 5 minute recipes","text":"Simulation survival data important theoretical practical work. practical setting might wish validate standard errors valid even rather small sample, validate complicated procedure intended. Therefore useful simple tools generating survival data looks much possible like particular data. theoretical setting often interested evaluating finite sample properties new procedure different settings often motivated specific practical problem. aim provide tools. Bender et al. nice paper discussed generate survival data based Cox model, restricted attention many useful parametric survival models (weibull, exponential). use piecewise linear baseline functions make easy simulate data follows closely baseline given data using semi nonparametric models. makes easy capture important aspects data. Different survival models can cooked, give recipes hazard cumulative incidence based simulations. recipes given vignette recurrent events. hazard based. cumulative incidence. recurrent events (see recurrent events vignette).","code":"library(mets)  options(warn=-1)  set.seed(10) # to control output in simulations"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"hazard-based-cox-models","dir":"Articles","previous_headings":"","what":"Hazard based, Cox models","title":"WIP: Cooking survival data, 5 minute recipes","text":"Given survival time TT cumulative hazard Λ(t)=∫0tλ(s)ds\\Lambda(t)=\\int_0^t \\lambda(s) ds, follows E∼Exp(1)E \\sim Exp(1) (exponential rate 1), Λ−1(E)\\Lambda^{-1}(E) distribution TT. provides basis simulations survival times given hazard consequence simple calculation P(Λ−1(E)>t)=P(E>Λ(t))=exp(−Λ(t))=P(T>t).   P(\\Lambda^{-1}(E) > t) = P(E > \\Lambda(t)) = \\exp( - \\Lambda(t)) = P(T > t). Similarly TT given XX hazard Cox form λ0(t)exp(XTβ)   \\lambda_0(t) \\exp( X^T \\beta)  β\\beta pp-dimensional regression coefficient λ0(t)\\lambda_0(t) baseline hazard funcion, useful observe also Λ−1(E/HR)\\Lambda^{-1}(E/HR) HR=exp(XTβ)HR=\\exp(X^T \\beta) distribution TT given XX. Therefore inverse cumulative hazard can computed can generate survival specified hazard function. One useful observation note piecewise linear continuous cumulative hazard interval [0,τ][0,\\tau]Λl(t)\\Lambda_l(t) easy compute inverse. , can approximate cumulative hazard piecewise linear continous cumulative hazard simulate data according approximation. Recall fitting Cox model data give piecewise constant cumulative hazard regression coefficients hand can first approximate piecewise constant “Breslow”-estimator linear upper (lower bound) simply connecting values straight lines.","code":""},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"delayed-entry","dir":"Articles","previous_headings":"","what":"Delayed entry","title":"WIP: Cooking survival data, 5 minute recipes","text":"TT given XX hazard Cox form λ0(t)exp(XTβ)   \\lambda_0(t) \\exp( X^T \\beta)  wish generate data according hazard alive time ss, draw distribution TT given T>sT>s (given XX ), note thatΛ0−1(Λ0(s)+E/HR)) \\Lambda_0^{-1}( \\Lambda_0(s) + E/HR))   HR=exp(XTβ))HR=\\exp(X^T \\beta)) E∼Exp(1)E \\sim Exp(1) distributiion . consequence simple calculation PX(Λ−1(Λ(s)+E/HR)>t)=PX(E>HR(Λ(t)−Λ(s)))=PX(T>t|T>s)   P_X(\\Lambda^{-1}(\\Lambda(s)+ E/HR) > t) = P_X(E > HR( \\Lambda(t) - \\Lambda(s)) ) = P_X(T>t | T>s) engine simulate data given linear cumulative hazard. First generating survival data based cumulative hazard cumhaz:j Now looking simple cox model   Multiple Cox models cause specific hazards can combined, start drawing covariates manually, just call sim.phregs function draws covariates data,  Now fully nonparametric model stratified baselines  now fit cause-specific hazard models 3 causes (censoring one ) generate competing risks data hazards taken fitted Cox models. situation stratified baselines models: sim.phreg phreg, can deal strata sim.phregs cause specific hazards phreg form One example fully non-parametric","code":"nsim <- 1000  chaz <-  c(0,1,1.5,2,2.1)  breaks <- c(0,10,   20,  30,   40)  cumhaz <- cbind(breaks,chaz)  X <- rbinom(nsim,1,0.5)  beta <- 0.2  rrcox <- exp(X * beta)    pctime <- rchaz(cumhaz,n=nsim)  pctimecox <- rchaz(cumhaz,rrcox) library(mets)  n <- nsim  data(bmt)  bmt$bmi <- rnorm(408)  dcut(bmt) <- gage~age  data <- bmt  cox1 <- phreg(Surv(time,cause==1)~tcell+platelet+age,data=bmt)   dd <- sim.phreg(cox1,n,data=bmt)  dtable(dd,~status) #>  #> status #>   0   1  #> 529 471  scox1 <- phreg(Surv(time,status==1)~tcell+platelet+age,data=dd)  cbind(coef(cox1),coef(scox1)) #>                [,1]       [,2] #> tcell    -0.6517920 -0.4564152 #> platelet -0.5207454 -0.5113844 #> age       0.4083098  0.3860139  par(mfrow=c(1,1))  plot(scox1,col=2); plot(cox1,add=TRUE,col=1) ## changing the parameters   cox10 <- cox1  cox10$coef <- c(0,0.4,0.3)  dd <- sim.phreg(cox10,n,data=bmt)  dtable(dd,~status) #>  #> status #>   0   1  #> 427 573  scox1 <- phreg(Surv(time,status==1)~tcell+platelet+age,data=dd)  cbind(coef(cox10),coef(scox1)) #>          [,1]       [,2] #> tcell     0.0 0.05615982 #> platelet  0.4 0.34930321 #> age       0.3 0.42496872  par(mfrow=c(1,1))  plot(scox1,col=2); plot(cox10,add=TRUE,col=1) data(bmt);   cox1 <- phreg(Surv(time,cause==1)~tcell+platelet,data=bmt)  cox2 <- phreg(Surv(time,cause==2)~tcell+platelet,data=bmt)   X1 <- bmt[,c(\"tcell\",\"platelet\")]  n <- nsim  xid <- sample(1:nrow(X1),n,replace=TRUE)  Z1 <- X1[xid,]  Z2 <- X1[xid,]  rr1 <- exp(as.matrix(Z1) %*% cox1$coef)  rr2 <- exp(as.matrix(Z2) %*% cox2$coef)   d <-  rcrisk(cox1$cum,cox2$cum,rr1,rr2)  dd <- cbind(d,Z1)   scox1 <- phreg(Surv(time,status==1)~tcell+platelet,data=dd)  scox2 <- phreg(Surv(time,status==2)~tcell+platelet,data=dd)  par(mfrow=c(1,2))  plot(cox1); plot(scox1,add=TRUE,col=2)  plot(cox2); plot(scox2,add=TRUE,col=2) cbind(cox1$coef,scox1$coef,cox2$coef,scox2$coef) #>                [,1]       [,2]       [,3]       [,4] #> tcell    -0.4232606 -0.3727007  0.3991068  0.8167564 #> platelet -0.5654438 -0.5834273 -0.2461474 -0.3190683 data(sTRACE)  dtable(sTRACE,~chf+diabetes) #>  #>     diabetes   0   1 #> chf                  #> 0            223  16 #> 1            230  31  coxs <-   phreg(Surv(time,status==9)~strata(diabetes,chf),data=sTRACE)  strata <- sample(0:3,nsim,replace=TRUE)  simb <- sim.phreg(coxs,nsim,data=NULL,strata=strata)   cc <-   phreg(Surv(time,status)~strata(strata),data=simb)  plot(coxs,col=1); plot(cc,add=TRUE,col=2)   simb1 <- sim.phreg(coxs,nsim,data=sTRACE)  cc1 <-   phreg(Surv(time,status)~strata(diabetes,chf),data=simb1)  plot(cc1,add=TRUE,col=3) ## r with phreg   cox0 <- phreg(Surv(time,cause==0)~tcell+platelet,data=bmt)  cox1 <- phreg(Surv(time,cause==1)~tcell+platelet,data=bmt)  cox2 <- phreg(Surv(time,cause==2)~strata(tcell)+platelet,data=bmt)  coxs <- list(cox0,cox1,cox2)  dd <- sim.phregs(coxs,n,data=bmt)   ## checking that  cause specific hazards are as given, make n larger  scox0 <- phreg(Surv(time,status==1)~tcell+platelet,data=dd)  scox1 <- phreg(Surv(time,status==2)~tcell+platelet,data=dd)  scox2 <- phreg(Surv(time,status==3)~strata(tcell)+platelet,data=dd)  cbind(cox0$coef,scox0$coef)  cbind(cox1$coef,scox1$coef)  cbind(cox2$coef,scox2$coef)  par(mfrow=c(1,3))  plot(cox0); plot(scox0,add=TRUE,col=2);   plot(cox1); plot(scox1,add=TRUE,col=2);   plot(cox2); plot(scox2,add=TRUE,col=2);     ########################################  ## second example   ########################################   cox1 <- phreg(Surv(time,cause==1)~strata(tcell)+platelet,data=bmt)  cox2 <- phreg(Surv(time,cause==2)~tcell+strata(platelet),data=bmt)  coxs <- list(cox1,cox2)  dd <- sim.phregs(coxs,n,data=bmt)  scox1 <- phreg(Surv(time,status==1)~strata(tcell)+platelet,data=dd)  scox2 <- phreg(Surv(time,status==2)~tcell+strata(platelet),data=dd)  cbind(cox1$coef,scox1$coef)  cbind(cox2$coef,scox2$coef)  par(mfrow=c(1,2))  plot(cox1); plot(scox1,add=TRUE);   plot(cox2); plot(scox2,add=TRUE); library(mets)  n <- nsim  data(bmt)  bmt$bmi <- rnorm(408)  dcut(bmt) <- gage~age  data <- bmt  cox1 <- phreg(Surv(time,cause==1)~strata(tcell,platelet),data=bmt)  cox2 <- phreg(Surv(time,cause==2)~strata(gage,tcell),data=bmt)  cox3 <- phreg(Surv(time,cause==0)~strata(platelet)+bmi,data=bmt)  coxs <- list(cox1,cox2,cox3)   dd <- sim.phregs(coxs,n,data=bmt,extend=0.002)  dtable(dd,~status) #>  #> status #>   0   1   2   3  #> 227 373 232 168  scox1 <- phreg(Surv(time,status==1)~strata(tcell,platelet),data=dd)  scox2 <- phreg(Surv(time,status==2)~strata(gage,tcell),data=dd)  scox3 <- phreg(Surv(time,status==3)~strata(platelet)+bmi,data=dd)  cbind(coef(cox1),coef(scox1), coef(cox2),coef(scox2), coef(cox3),coef(scox3)) #>           [,1]      [,2] #> bmi 0.07591238 0.1488615  par(mfrow=c(1,3))  plot(scox1,col=2); plot(cox1,add=TRUE,col=1)  plot(scox2,col=2); plot(cox2,add=TRUE,col=1)  plot(scox3,col=2); plot(cox3,add=TRUE,col=1)"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"multistate-models-the-illness-death-model","dir":"Articles","previous_headings":"","what":"Multistate models: The Illness Death model","title":"WIP: Cooking survival data, 5 minute recipes","text":"Using hazard based simulation delayed entry can simulate data example general illness-death model. cumulative hazards need specified. simply give cumulative hazards different transitions function simMultistate simulate data model, subsequently re-estimate parameters based simulated data validate procedure.","code":"data(CPH_HPN_CRBSI)  dr <- CPH_HPN_CRBSI$terminal  base1 <- CPH_HPN_CRBSI$crbsi   base4 <- CPH_HPN_CRBSI$mechanical  dr2 <- scalecumhaz(dr,1.5)  cens <- rbind(c(0,0),c(2000,0.5),c(5110,3))   iddata <- simMultistate(nsim,base1,base1,dr,dr2,cens=cens)  dlist(iddata,.~id|id<3,n=0) #> id: 1 #>   entry     time status rr death from to start     stop #> 1     0 140.6815      3  1     1    1  3     0 140.6815 #> ------------------------------------------------------------  #> id: 2 #>         entry     time status rr death from to    start     stop #> 2      0.0000 335.4145      2  1     0    1  2   0.0000 335.4145 #> 1001 335.4145 394.7477      1  1     0    2  1 335.4145 394.7477 #> 1634 394.7477 395.6884      0  1     0    1  0 394.7477 395.6884     ### estimating rates from simulated data    c0 <- phreg(Surv(start,stop,status==0)~+1,iddata)  c3 <- phreg(Surv(start,stop,status==3)~+strata(from),iddata)  c1 <- phreg(Surv(start,stop,status==1)~+1,subset(iddata,from==2))  c2 <- phreg(Surv(start,stop,status==2)~+1,subset(iddata,from==1))  ###  par(mfrow=c(2,2))  plot(c0)  lines(cens,col=2)   plot(c3,main=\"rates 1-> 3 , 2->3\")  lines(dr,col=1,lwd=2)  lines(dr2,col=2,lwd=2)  ###  plot(c1,main=\"rate 1->2\")  lines(base1,lwd=2)  ###  plot(c2,main=\"rate 2->1\")  lines(base1,lwd=2)"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"cumulative-incidence","dir":"Articles","previous_headings":"","what":"Cumulative incidence","title":"WIP: Cooking survival data, 5 minute recipes","text":"section discuss simulate competing risks data specfied cumulative incidence function. consider simplicity competing risks model two causes denote cumulative incidence curves F1(t,X)=P(T<t,ϵ=1|X)F_1(t,X) = P(T < t, \\epsilon=1|X) F2(t,X)=P(T<t,ϵ=2|X)F_2(t,X) = P(T < t, \\epsilon=2|X). given covariate XX. generate data required cumulative incidence functions simple approach first figure subject dies cause, finally draw survival time according conditional distribution. simplicity consider survival times fixed interval [0,τ][0,\\tau], first flip coin probabilities 1−F1(τ,X)−F2(τ,X)1-F_1(\\tau,X)-F_2(\\tau,X) decide subject survivor dies. subject dies flip coin probabilities F1(τ,X)/(F1(τ,X)+F2(τ,X))F_1(\\tau,X)/(F_1(\\tau,X)+F_2(\\tau,X)) F2(τ,X)/(F1(τ,X)+F2(τ,X))F_2(\\tau,X)/(F_1(\\tau,X)+F_2(\\tau,X)) decide cause !!, ϵ=1\\epsilon=1, cause 2, ϵ=2\\epsilon=2. Finally draw survival time using cumulative incidence distribution. timing cause jj event thus T=(F̃1−1(U,X)T = (\\tilde F_1^{-1}(U,X) F̃1(s,X)=F1(s,X)/F1(τ,X)\\tilde F_1(s,X) = F_1(s,X)/F_1(\\tau,X) UU uniform. indeed P(T≤t,ϵ=j|X)=Fj(t,X)P(T \\leq t, \\epsilon=j|X) = F_j(t,X) j=1,2j=1,2. note use F̃j(s)\\tilde F_j(s) Fj(s)F_j(s) piecewise linear continuous functions inverse easy compute.","code":""},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"cumulative-incidence-i","dir":"Articles","previous_headings":"Cumulative incidence","what":"Cumulative incidence I","title":"WIP: Cooking survival data, 5 minute recipes","text":"simulate two causes death two binary covarites logistic type F1(t,X)=Λ1(t,ρ1)exp(XTβ)1+Λ1(t,ρ1)exp(XTβ)\\begin{align*} F_1(t,X)  &= \\frac{ \\Lambda_1(t,\\rho_1) exp(X^T  \\beta)}{1+\\Lambda_1(t,\\rho_1) exp(X^T  \\beta)} \\end{align*} F2F_2 enforcing sum condition F1+F2≤1F_1+F_2 \\leq 1F2(t,X)=Λ2(t,ρ2)exp(XTβ)1+Λ2(t,ρ2)exp(XTβ)[1−F1(τ,X)]\\begin{align*} F_2(t,X)  & =  \\frac{ \\Lambda_2(t,\\rho_2) exp(X^T  \\beta)}{1+\\Lambda_2(t,\\rho_2) exp(X^T  \\beta)} [ 1- F_1(\\tau,X) ] \\end{align*} F2(t,X)=Λ2(t,ρ2)exp(XTβ)1+Λ2(t,ρ2)exp(XTβ)\\begin{align*} F_2(t,X)  & =  \\frac{ \\Lambda_2(t,\\rho_2) exp(X^T  \\beta)}{1+\\Lambda_2(t,\\rho_2) exp(X^T  \\beta)} \\end{align*} baselines given Λj(t)=ρ1(1−exp(−t/rj))\\Lambda_j(t) = \\rho_1 (1- exp(-t/r_j)) ρj\\rho_j rjr_j postive constants, τ=6\\tau=6. simulate survival time use piecwise linear approximation cumulative incidence functions thus depends grid linear approximation. linear approximation can made arbitrarily close specific smooth cumulative incidence function.  can also use parameters based fitted models","code":"library(mets) nsim <- 100 rho1 <- 0.4; rho2 <- 2 beta <- c(0.3,-0.3,-0.3,0.3)  dats <- simul.cifs(nsim,rho1,rho2,beta,rc=0.5,depcens=0,type=\"logistic\")  par(mfrow=c(1,2)) # Fitting regression model with CIF logistic-link  cif1 <- cifreg(Event(time,status)~Z1+Z2,dats) summary(cif1) #>  #>    n events #>  100     19 #>  #>  100 clusters #> coeffients: #>     Estimate      S.E.   dU^-1/2 P-value #> Z1 -0.097975  0.252782  0.232452  0.6983 #> Z2  0.146136  0.502409  0.464850  0.7711 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  0.90667 0.55244 1.4881 #> Z2  1.15735 0.43233 3.0983 plot(cif1) lines(attr(dats,\"Lam1\"))  dats <- simul.cifs(nsim,rho1,rho2,beta,rc=0.5,depcens=0,type=\"cloglog\") ciff <- cifregFG(Event(time,status)~Z1+Z2,dats) summary(ciff) #>  #>    n events #>  100     20 #>  #>  100 clusters #> coeffients: #>    Estimate    S.E. dU^-1/2 P-value #> Z1  0.24189 0.22915 0.23459  0.2912 #> Z2  0.65704 0.46627 0.46940  0.1588 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  1.27365 0.81283 1.9957 #> Z2  1.92907 0.77349 4.8110 plot(ciff) lines(attr(dats,\"Lam1\")) data(bmt)  ################################################################  #  simulating several causes with specific cumulatives   ################################################################  ## two logistic link models   cif1 <-  cifreg(Event(time,cause)~tcell+age,data=bmt,cause=1)  cif2 <-  cifreg(Event(time,cause)~tcell+age,data=bmt,cause=2)   dd <- sim.cifs(list(cif1,cif2),nsim,data=bmt)   ## still logistic link   scif1 <-  cifreg(Event(time,cause)~tcell+age,data=dd,cause=1)  ## 2nd cause not on logistic form due to restriction  scif2 <-  cifreg(Event(time,cause)~tcell+age,data=dd,cause=2)       cbind(cif1$coef,scif1$coef) #>             [,1]      [,2] #> tcell -0.7966937 0.2319643 #> age    0.4164386 0.7074909  cbind(cif2$coef,scif2$coef) #>              [,1]       [,2] #> tcell  0.66688269 -0.6246131 #> age   -0.03248603 -0.5207753  par(mfrow=c(1,2))     plot(cif1); plot(scif1,add=TRUE,col=2)  plot(cif2); plot(scif2,add=TRUE,col=2)"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"cif-delayed-entry","dir":"Articles","previous_headings":"Cumulative incidence","what":"CIF Delayed entry","title":"WIP: Cooking survival data, 5 minute recipes","text":"Now assume given covariates F1(t;X)=P(T<t,ϵ=1|X)F_1(t;X) = P(T < t, \\epsilon=1|X) F2(t;X)=P(T<t,ϵ=2|X)F_2(t;X) = P(T < t, \\epsilon=2|X) two cumulative incidence functions satistifes needed constraints. wish generate data follows two piecewise linear cumulative indidence functions delayed entry time ss. thus generate data follows cumulative incidence functions F̃1(t,s;X)=F1(t;X)−F1(s;;X)1−F1(s;X)−F2(s;X) \\tilde F_1(t,s;X)=   \\frac{F_1(t;X) - F_1(s;;X)}{ 1 - F_1(s;X) - F_2(s;X)}  F̃2(t,s;X)=F2(t;X)−F2(s;;X)1−F1(s;X)−F2(s;X) \\tilde F_2(t,s;X)=   \\frac{F_2(t;X) - F_2(s;;X)}{ 1 - F_1(s;X) - F_2(s;X)}  can done according recipe previous section. specific (ignoring XX formula) F1−1(F1(s)+U⋅(1−F1(s;X)−F2(s;X)))   F_1^{-1}( F_1(s) + U \\cdot (1 - F_1(s;X) - F_2(s;X)) )  UU uniform, distribution given F̃1(t,s)\\tilde F_1(t,s).","code":""},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"recurrent-events","dir":"Articles","previous_headings":"","what":"Recurrent events","title":"WIP: Cooking survival data, 5 minute recipes","text":"See also recurrent events vignette rate recurrent events among survivors Cox form (phreg) rate recurrent events marginal Ghosh-Lin model (recreg) simulations based approximations piecewise linear models based grid. events can dependent via frailty random effects (Gamma distributed) frailty Gamma model rate events terminal event given based cumulative baselines relative risk covariate effects. Thus ends Cox form given frailty covariates. simRecurrentList can take multiple recurrent events multiple causes death Two-stage models  now Ghosh-Lin Cox marginals  Frailty models","code":"data(hfactioncpx12)  hf <- hfactioncpx12  hf$x <- as.numeric(hf$treatment)   n <- 1000   ##  to fit Cox  models   xr <- phreg(Surv(entry,time,status==1)~treatment+cluster(id),data=hf)  dr <- phreg(Surv(entry,time,status==2)~treatment+cluster(id),data=hf)  estimate(xr) #>            Estimate Std.Err   2.5%    97.5% P-value #> treatment1  -0.1534 0.08145 -0.313 0.006286 0.05973  estimate(dr) #>            Estimate Std.Err    2.5%    97.5% P-value #> treatment1  -0.4301  0.1831 -0.7889 -0.07132  0.0188   simcoxcox <- sim.recurrent(xr,dr,n=n,data=hf)   xrs <- phreg(Surv(start,stop,statusD==1)~treatment+cluster(id),data=simcoxcox)  drs <- phreg(Surv(start,stop,statusD==3)~treatment+cluster(id),data=simcoxcox)  estimate(xrs) #>            Estimate Std.Err    2.5%    97.5%  P-value #> treatment1  -0.2233 0.07217 -0.3648 -0.08185 0.001974  estimate(drs) #>            Estimate Std.Err    2.5%   97.5%  P-value #> treatment1  -0.3901  0.1375 -0.6596 -0.1206 0.004552   par(mfrow=c(1,2))  plot(xrs);   plot(xr,add=TRUE) ###  plot(drs)  plot(dr,add=TRUE) recGL <- recreg(Event(entry,time,status)~treatment+cluster(id),hf,death.code=2)  estimate(recGL) #>            Estimate Std.Err    2.5%   97.5% P-value #> treatment1  -0.1104 0.07866 -0.2646 0.04376  0.1604  estimate(dr) #>            Estimate Std.Err    2.5%    97.5% P-value #> treatment1  -0.4301  0.1831 -0.7889 -0.07132  0.0188   simglcox <- sim.recurrent(recGL,dr,n=n,data=hf)   simcoxcox <- sim.recurrent(xr,dr,n=n,data=hf)  dtable(simcoxcox,~statusD) #>  #> statusD #>    0    1    3  #>  755 2603  245   recGL <- recreg(Event(entry,time,status)~treatment+cluster(id),hf,death.code=2)  simglcox <- sim.recurrent(recGL,dr,n=n,data=hf)   GLs <- recreg(Event(start,stop,statusD)~treatment+cluster(id),data=simglcox,death.code=3)  drs <- phreg(Surv(start,stop,statusD==3)~treatment+cluster(id),data=simglcox)  estimate(GLs) #>            Estimate Std.Err   2.5%  97.5% P-value #> treatment1  -0.1317 0.07259 -0.274 0.0106 0.06968  estimate(drs) #>            Estimate Std.Err    2.5%   97.5%   P-value #> treatment1    -0.46  0.1336 -0.7219 -0.1982 0.0005744   par(mfrow=c(1,2))  plot(GLs);   plot(recGL,add=TRUE) ###  plot(drs)  plot(dr,add=TRUE) data(CPH_HPN_CRBSI)  dr <- CPH_HPN_CRBSI$terminal  base1 <- CPH_HPN_CRBSI$crbsi   base4 <- CPH_HPN_CRBSI$mechanical   n <- 100  rr <- simRecurrent(n,base1,death.cumhaz=dr)  ###  par(mfrow=c(1,3))  showfitsim(causes=1,rr,dr,base1,base1,which=1:2)   rr <- simRecurrentII(n,base1,base4,death.cumhaz=dr)  dtable(rr,~death+status) #>  #>       status   0   1   2 #> death                    #> 0             11 266  25 #> 1             89   0   0  showfitsim(causes=2,rr,dr,base1,base4,which=1:2)   cumhaz <- list(base1,base1,base4)  drl <- list(dr,base4)  rr <- simRecurrentList(n,cumhaz,death.cumhaz=drl)  dtable(rr,~death+status) #>  #>       status   0   1   2   3 #> death                        #> 0              4 179 192  24 #> 1             78   0   0   0 #> 2             18   0   0   0  showfitsimList(rr,cumhaz,drl)"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"parametric-models","dir":"Articles","previous_headings":"","what":"Parametric models","title":"WIP: Cooking survival data, 5 minute recipes","text":"semi‑parametric Cox model provides substantial flexibility simulating survival data, situations fully parametric simulation model convenient preferable. consider Weibull model parametrized cumulative hazard given Λ(t)=λ⋅ts\\Lambda(t) = \\lambda  \\cdot t^s ss shape parameter, λ\\lambda rate parameter. allow regression parameters λ:=exp(β⊤X),s:=exp(γ⊤Z)\\begin{align*} \\lambda :=  \\exp(\\beta^\\top X), \\quad s := \\exp(\\gamma^\\top Z) \\end{align*} XX ZZ covariate vectors. Specifically, opens exploring non‑proportional hazards ss depends covariates. Revisiting TRACE data example can compare predictions Cox Weibull-Cox model stratified chf proportional hazard effect age  simulate data can use rweibullcox() function. Note stats::rweibull() function gives different parametrization cumulative hazard given H(t)=(t/b)sH(t) = (t/b)^s, .e., scale parameter scale parameter bb related rate parameter consider r:=b−sr := b^{-s}. steps wrapped simulate method:","code":"data(sTRACE, package = \"mets\") dat <- sTRACE cox1 <- phreg(Surv(time, status > 0) ~ strata(chf) + I(age - 67), data = sTRACE) coxw <- phreg_weibull(Surv(time, status > 0) ~ chf + age,     shape.formula = ~chf,     data = sTRACE     ) coxw #>  #> - Weibull-Cox model - #>  #> Call: #> phreg_weibull(formula = Surv(time, status > 0) ~ chf + age, shape.formula = ~chf,  #>     data = sTRACE) #>  #> log-Likelihood: -684.750499  #>  #>    n events obs.time #>  500    264 2228.481 #>  #>               Estimate  Std.Err     2.5%   97.5%   P-value #> (Intercept)   -5.59626 0.465886 -6.50938 -4.6831 3.070e-33 #> chf            0.83250 0.197629  0.44516  1.2198 2.526e-05 #> age            0.05331 0.006165  0.04123  0.0654 5.241e-18 #> ─────────────                                              #> s:(Intercept) -0.44096 0.116740 -0.66977 -0.2122 1.585e-04 #> s:chf         -0.11794 0.133078 -0.37877  0.1429 3.755e-01  tt <- seq(0, max(sTRACE$time), length.out = 100) newd <- data.frame(chf = c(1, 0), age=67) pr <- predict(coxw, newdata = newd, times = tt, type=\"chaz\") plot(cox1, col = 1) lines(tt, pr[, 1, 1], lty=2, lwd=2) lines(tt, pr[, 1, 2], lty = 1, lwd = 2) n <- 5000 newd <- mets::dsample(size=n, sTRACE[,c(\"chf\",\"age\")]) # bootstrap covariates lp <- predict(coxw, newdata=newd, type=\"lp\") # linear-predictors head(lp) #>             [,1]       [,2] #> X6549 -0.9896742 -0.5589006 #> X6523 -0.5935585 -0.5589006 #> X3742 -1.3657441 -0.5589006 #> X6258 -0.9611517 -0.5589006 #> X79   -2.8312847 -0.4409608 #> X2952 -2.3217722 -0.4409608  ## simulate event times tt <- rweibullcox(nrow(lp), rate = exp(lp[,1]), shape= exp(lp[,2]))  # censoring model censw <- phreg_weibull(Surv(time, status==0) ~ 1, data=sTRACE) censpar <- exp(coef(censw)) censtime <- pmin(8, rweibullcox(nrow(lp), censpar[1], censpar[2]))  # combined simulated data newd <- transform(newd, time=pmin(tt, censtime), status=(tt<=censtime)) head(newd) #>       chf    age     time status #> X6549   1 70.791 0.374941   TRUE #> X6523   1 78.221 3.973340   TRUE #> X3742   1 63.737 4.330397   TRUE #> X6258   1 71.326 1.414283   TRUE #> X79     0 51.863 5.323526   TRUE #> X2952   0 61.420 6.433051  FALSE  # estimate weibull model on new data phreg_weibull(Surv(time,status) ~ chf + age, ~chf, data=newd) #>  #> - Weibull-Cox model - #>  #> Call: #> phreg_weibull(formula = Surv(time, status) ~ chf + age, shape.formula = ~chf,  #>     data = newd) #>  #> log-Likelihood: -6682.897120  #>  #>     n events obs.time #>  5000   2622 21530.79 #>  #>               Estimate  Std.Err     2.5%    97.5%    P-value #> (Intercept)   -5.57207 0.154019 -5.87394 -5.27020 1.356e-286 #> chf            0.79395 0.057334  0.68158  0.90632  1.311e-43 #> age            0.05367 0.002115  0.04952  0.05782 5.238e-142 #> ─────────────                                                #> s:(Intercept) -0.46327 0.032007 -0.52600 -0.40054  1.767e-47 #> s:chf         -0.11396 0.038514 -0.18944 -0.03847  3.088e-03 # simulate(coxw, n = 5, cens.model = NULL, data=newd, var.names = c(\"time\", \"status\")) simulate(coxw, nsim = 5) #>         no wmi status chf    age sex diabetes       time vf #> X707   707 1.8   TRUE   1 87.175   0        0 0.12940912  0 #> X1157 1157 1.3  FALSE   1 64.074   1        0 7.13699999  0 #> X6628 6628 1.1   TRUE   0 84.825   0        0 0.44463601  0 #> X969   969 0.6   TRUE   1 65.461   1        0 0.05219977  0 #> X4417 4417 1.2  FALSE   0 76.189   1        0 6.01461443  0"},{"path":"http://kkholst.github.io/mets/articles/cooking-survival-data.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"WIP: Cooking survival data, 5 minute recipes","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/cumulative-cost.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"IPCW Cumulative Cost","text":"describe regression modelling cumulative cost $$\\begin{align*} {\\cal U}(t) & = \\int_0^t Z(s) dN(s) \\end{align*}$$ N(s)N(s) counting process registers times cost realized accumulated, Z(t)Z(t) cost (marks) event times. counting process can mix random fixed times. data thus represented counting process format marks/costs going along event times. many additional uses cumulative process, example, considering time-lost recurrent events setting, return . can estimate marginal mean cumulative process $$\\begin{align*} \\nu(t) & = E ( {\\cal U}(t) )   \\end{align*}$$ possibly strata standard errors based derived influence function. provide semi-parametric regression modelling using proportional model $$\\begin{align*} E ( {\\cal U}(t) | X) & = \\Lambda_0(t) \\exp( X^T \\beta). \\end{align*}$$ addition fixed time-point t∈[0,τ]t \\[0,\\tau] can estimate mean given covariates $$\\begin{align*} E ( {\\cal U}(t) | X) & = \\exp( X^T \\beta) \\end{align*}$$ τ\\tau maximum follow-time. similarly Ghosh-Lin model recurrent events terminal event can specified. also estimate probability exceeding thresholds time $$\\begin{align*} P ( {\\cal U}(t)  > k ) & = \\mu_k(t), \\end{align*}$$ situation terminal based derived competing risks data keeps track competing terminal event. Regression modelling quantity also possible using competing risks regression models, using example, cifreg function mets.","code":""},{"path":"http://kkholst.github.io/mets/articles/cumulative-cost.html","id":"hf-action-data","dir":"Articles","previous_headings":"","what":"HF-action data","title":"IPCW Cumulative Cost","text":"Considering HF-action data simulate severity score event.  comparison also compute IPCW estimates without marks time 3, using linear model, note identical. Standard errors however based different formula asymptotically equivalent, note similar. also apply semiparametric proportional cost model IPCW adjustment: treated 14 % lower cumulative severity 11% lower number expected events.","code":"library(mets) data(hfactioncpx12) hf <- hfactioncpx12 hf$severity <- abs((5+rnorm(741)*2))[hf$id]  proc_design <- mets:::proc_design ## marginal mean using formula   outNZ <- recurrentMarginal(Event(entry,time,status)~strata(treatment)+cluster(id)              +marks(severity),hf,cause=1,death.code=2) plot(outNZ,se=TRUE) summary(outNZ,times=3)  #> [[1]] #>     new.time     mean        se  CI-2.5% CI-97.5% strata #> 682        3 10.24832 0.6186346 9.104797 11.53546      0 #>  #> [[2]] #>     new.time     mean        se  CI-2.5% CI-97.5% strata #> 601        3 9.454503 0.6950555 8.185815 10.91982      1  outN <- recurrentMarginal(Event(entry,time,status)~strata(treatment)+cluster(id),data=hf,                cause=1,death.code=2) plot(outN,se=TRUE,add=TRUE) summary(outN,times=3)  #> [[1]] #>     new.time     mean        se  CI-2.5% CI-97.5% strata #> 682        3 2.118496 0.1138572 1.906692 2.353829      0 #>  #> [[2]] #>     new.time     mean        se  CI-2.5% CI-97.5% strata #> 601        3 1.924062 0.1216577 1.699801 2.177912      1 outNZ3 <- recregIPCW(Event(entry,time,status)~-1+treatment+cluster(id)+marks(severity),data=hf,           cause=1,death.code=2,time=3,cens.model=~strata(treatment),model=\"lin\") summary(outNZ3) #>    n events #>  741   1281 #>  #>  741 clusters #> coeffients: #>            Estimate  Std.Err     2.5%    97.5% P-value #> treatment0 10.24832  0.61860  9.03588 11.46075       0 #> treatment1  9.45450  0.69499  8.09234 10.81667       0 head(iid(outNZ3)) #>            [,1]        [,2] #> 1 -0.0051841554  0.00000000 #> 2  0.0102241902  0.00000000 #> 3  0.0000000000 -0.03026973 #> 4 -0.0139508255  0.00000000 #> 5 -0.0004765366  0.00000000 #> 6 -0.0341093932  0.00000000  outN3 <- recregIPCW(Event(entry,time,status)~-1+treatment+cluster(id),data=hf,cause=1,death.code=2,time=3,          cens.model=~strata(treatment),model=\"lin\") summary(outN3) #>    n events #>  741   1281 #>  #>  741 clusters #> coeffients: #>            Estimate Std.Err    2.5%   97.5% P-value #> treatment0  2.11850 0.11385 1.89535 2.34164       0 #> treatment1  1.92406 0.12165 1.68564 2.16248       0 head(iid(outN3)) #>            [,1]        [,2] #> 1  0.0004542472 0.000000000 #> 2  0.0009756994 0.000000000 #> 3  0.0000000000 0.009301496 #> 4 -0.0029668336 0.000000000 #> 5 -0.0001120764 0.000000000 #> 6 -0.0070693971 0.000000000 propNZ <- recreg(Event(entry,time,status)~treatment+marks(severity)+cluster(id),data=hf,cause=1,death.code=2) summary(propNZ)  #>  #>     n events #>  2132   1391 #>  #>  741 clusters #> coeffients: #>             Estimate      S.E.   dU^-1/2 P-value #> treatment1 -0.102856  0.090464  0.024319  0.2555 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.90226 0.75566 1.0773 plot(propNZ,main=\"Baselines\")       GL <- recreg(Event(entry,time,status)~treatment+cluster(id),hf,cause=1,death.code=2) summary(GL) #>  #>     n events #>  2132   1391 #>  #>  741 clusters #> coeffients: #>             Estimate      S.E.   dU^-1/2 P-value #> treatment1 -0.110404  0.078656  0.053776  0.1604 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.89547 0.76754 1.0447 plot(GL,add=TRUE,col=2)"},{"path":"http://kkholst.github.io/mets/articles/cumulative-cost.html","id":"exceed-threshold","dir":"Articles","previous_headings":"","what":"Exceed threshold","title":"IPCW Cumulative Cost","text":"Finally, also estimate probability exceeding cumulative severity 1,5,10","code":"ooNZ <- prob.exceed.recurrent(Event(entry,time,status)~strata(treatment)+cluster(id)+marks(severity),data=hf,                   cause=1,death.code=2,exceed=c(1,5,10,20)) plot(ooNZ,strata=1) plot(ooNZ,strata=2,add=TRUE) summary(ooNZ,times=3) #> $`0` #> $`0`$prob #>            times            #>                3 2.99865085 #> N<1            3 0.05046407 #> exceed>=1      3 0.94953593 #> exceed>=5      3 0.90305672 #> exceed>=10     3 0.78544505 #> exceed>=20     3 0.49877769 #>  #> $`0`$se #>            times            #>                3 2.99865085 #> N<1            3 0.01999300 #> exceed>=1      3 0.01999300 #> exceed>=5      3 0.02693354 #> exceed>=10     3 0.03918605 #> exceed>=20     3 0.05133898 #>  #> $`0`$lower #>      times            #> [1,]     3 2.99865085 #> [2,]     3 0.08885207 #> [3,]     3 0.91114793 #> [4,]     3 0.85178123 #> [5,]     3 0.71227737 #> [6,]     3 0.40765541 #>  #> $`0`$upper #>      times            #> [1,]     3 2.99865085 #> [2,]     3 0.01045872 #> [3,]     3 0.98954128 #> [4,]     3 0.95741889 #> [5,]     3 0.86612878 #> [6,]     3 0.61026834 #>  #>  #> $`1` #> $`1`$prob #>            times            #>                3 2.99865085 #> N<1            3 0.01705929 #> exceed>=1      3 0.98294071 #> exceed>=5      3 0.96772447 #> exceed>=10     3 0.74154673 #> exceed>=20     3 0.51833754 #>  #> $`1`$se #>            times             #>                3 2.998650853 #> N<1            3 0.008885007 #> exceed>=1      3 0.008885007 #> exceed>=5      3 0.012103895 #> exceed>=10     3 0.043163511 #> exceed>=20     3 0.055744489 #>  #> $`1`$lower #>      times            #> [1,]     3 2.99865085 #> [2,]     3 0.03432023 #> [3,]     3 0.96567977 #> [4,]     3 0.94428969 #> [5,]     3 0.66159512 #> [6,]     3 0.41982703 #>  #> $`1`$upper #>      times           #> [1,]     3 2.9986509 #> [2,]     3 0.0000000 #> [3,]     3 1.0000000 #> [4,]     3 0.9917408 #> [5,]     3 0.8311602 #> [6,]     3 0.6399631"},{"path":"http://kkholst.github.io/mets/articles/cumulative-cost.html","id":"cumulative-time-lost-for-recurrent-events","dir":"Articles","previous_headings":"","what":"Cumulative time lost for recurrent events","title":"IPCW Cumulative Cost","text":"cumulative time lost recurrent events defined $$\\begin{align*}   {\\cal M}(t) = E[ \\int_0^\\tau (\\tau-s) dN(s) ] = \\int_0^\\tau \\mu(s) ds \\end{align*}$$ μ(t)=E(N(t))\\mu(t) = E( N(t) ) marginal mean recurrent events time tt.","code":"hf$lost5 <- 5-hf$time  RecLost <- recregIPCW(Event(entry,time,status)~-1+treatment+cluster(id)+marks(lost5),data=hf,            cause=1,death.code=2,time=5,cens.model=~strata(treatment),model=\"lin\") summary(RecLost) #>    n events #>  741   1391 #>  #>  741 clusters #> coeffients: #>            Estimate Std.Err    2.5%   97.5% P-value #> treatment0  8.58300 0.42951 7.74118 9.42482       0 #> treatment1  7.66234 0.46400 6.75292 8.57177       0 head(iid(RecLost)) #>            [,1]       [,2] #> 1  0.0016920221 0.00000000 #> 2  0.0073388996 0.00000000 #> 3  0.0000000000 0.02120478 #> 4 -0.0095548150 0.00000000 #> 5 -0.0005696809 0.00000000 #> 6 -0.0201750011 0.00000000"},{"path":"http://kkholst.github.io/mets/articles/cumulative-cost.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"IPCW Cumulative Cost","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/glm-utility.html","id":"utility-functions-for-glm-objects","dir":"Articles","previous_headings":"","what":"Utility functions for GLM objects","title":"GEE cluster standard errors and predictions for glm objects","text":"Getting confidence intervals using GEE (sandwhich) standard errors Predictions also simple","code":"set.seed(100)  library(mets) data(bmt);  bmt$id <- sample(1:100,408,replace=TRUE)  glm1 <- glm(tcell~platelet+age,bmt,family=binomial) summaryGLM(glm1) #> $coef #>             Estimate Std.Err    2.5%   97.5%   P-value #> (Intercept)  -2.4371  0.2225 -2.8732 -2.0009 6.481e-28 #> platelet      1.1368  0.3076  0.5340  1.7397 2.189e-04 #> age           0.5927  0.1551  0.2888  0.8966 1.319e-04 #>  #> $or #>               Estimate       2.5%     97.5% #> (Intercept) 0.08741654 0.05651794 0.1352076 #> platelet    3.11688928 1.70573194 5.6955015 #> age         1.80895115 1.33489115 2.4513641 #>  #> $fout #> NULL  ## GEE robust standard errors summaryGLM(glm1,id=bmt$id) #> $coef #>             Estimate Std.Err    2.5%   97.5%   P-value #> (Intercept)  -2.4371  0.2157 -2.8599 -2.0142 1.361e-29 #> platelet      1.1368  0.2830  0.5822  1.6914 5.877e-05 #> age           0.5927  0.1434  0.3117  0.8738 3.568e-05 #>  #> $or #>               Estimate       2.5%     97.5% #> (Intercept) 0.08741654 0.05727471 0.1334211 #> platelet    3.11688928 1.79006045 5.4271903 #> age         1.80895115 1.36575550 2.3959664 #>  #> $fout #> NULL age <- seq(-2,2,by=0.1) nd <- data.frame(platelet=0,age=seq(-2,2,by=0.1)) pnd <- predictGLM(glm1,nd) head(pnd$pred) #>      Estimate       2.5%      97.5% #> p1 0.02601899 0.01115243 0.05951051 #> p2 0.02756409 0.01214068 0.06136414 #> p3 0.02919819 0.01321187 0.06328733 #> p4 0.03092608 0.01437206 0.06528441 #> p5 0.03275278 0.01562757 0.06736019 #> p6 0.03468351 0.01698493 0.06952008 plot(age,pnd$pred[,1],type=\"l\",ylab=\"predictions\",xlab=\"age\",ylim=c(0,0.3)) plotConfRegion(age,pnd$pred[,2:3],col=2) ###matlines(age,pnd$pred[,-1],col=2,lty=1)"},{"path":"http://kkholst.github.io/mets/articles/glm-utility.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"GEE cluster standard errors and predictions for glm objects","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/haplo-discrete-ttp.html","id":"haplotype-analysis-for-discrete-ttp","dir":"Articles","previous_headings":"","what":"Haplotype Analysis for discrete TTP","title":"Haplotype Discrete Survival Models","text":"Cycle-specific logistic regression haplo-type effects known haplo-type probabilities. Given observed genotype G unobserved haplotypes H mix possible haplotypes using P(H|G)P(H|G) given input. S(t|x,G)=E(S(t|x,H)|G)=∑h∈GP(h|G)S(t|z,h)\\begin{align*}    S(t|x,G) & = E( S(t|x,H) | G)  = \\sum_{h \\G} P(h|G) S(t|z,h)     \\end{align*} survival can computed mixing possible h given g. Survival based logistic regression discrete hazard function form logit(P(T=t|T>=t,x,h))=αt+x(h)beta\\begin{align*}       \\mbox{logit}(P(T=t| T >= t, x,h)) & = \\alpha_t + x(h) beta          \\end{align*} x(h) regression design x haplotypes h=(h1,h2)h=(h_1,h_2). Simple binomial data can fitted using function. standard errors assume haplotype probabilities known. particularly interested types haplotypes: Among types interest look frequencies choose baseline cycle specific data idid outcome yy list possible haplo-types id likely pp (sum within id 1): first id=1 haplotype fully observed, id=2 two possible haplotypes consistent observed genotype id, probabiblities 7% 93%, respectively. baseline given can specify regression design gives effect “type” present (sm=0), additive effect haplotypes (sm=1): fit model start constructing time-design (named X) takes haplotype distributions id Now can fit model design given designfunction Haplotypes “DCGCGCTCACG” “DTCCGCTGACG” gives increased hazard pregnancy data generated true coefficients design fitted can found output","code":"types <- c(\"DCGCGCTCACG\",\"DTCCGCTGACG\",\"ITCAGTTGACG\",\"ITCCGCTGAGG\")  ## some haplotypes frequencies for simulations  data(haplo) hapfreqs <- haplo$hapfreqs  print(hapfreqs) #>             index   haplotype     freq #> DCGAGCTCACG     1 DCGAGCTCACG 0.010681 #> DCGCGCTCACG     2 DCGCGCTCACG 0.138387 #> DTGAGCTCACG     3 DTGAGCTCACG 0.000310 #> DTGAGCTCACA     4 DTGAGCTCACA 0.006800 #> DTGAGCTCGCG     5 DTGAGCTCGCG 0.034517 #> DTGACCTCACG     6 DTGACCTCACG 0.001336 #> DTGCGCTCACG     7 DTGCGCTCACG 0.009969 #> DTGCGCTCACA     8 DTGCGCTCACA 0.011833 #> DTGCGCTCGCG     9 DTGCGCTCGCG 0.302389 #> DTGCGCCCGCG    10 DTGCGCCCGCG 0.001604 #> DTGCCCTCACG    11 DTGCCCTCACG 0.003912 #> DTCAGCTGACG    12 DTCAGCTGACG 0.001855 #> DTCCGCTGACG    13 DTCCGCTGACG 0.103394 #> DTCCCCTGACG    14 DTCCCCTGACG 0.000310 #> ITCAGTTGACG    15 ITCAGTTGACG 0.048124 #> ITCCGCTGAGG    16 ITCCGCTGAGG 0.291273 #> ITCCGTTGACG    17 ITCCGTTGACG 0.031089 #> ITCCGTCGACG    18 ITCCGTCGACG 0.001502 #> ITCCCCTGAGG    19 ITCCCCTGAGG 0.000653 www <-which(hapfreqs$haplotype %in% types) hapfreqs$freq[www] #> [1] 0.138387 0.103394 0.048124 0.291273  baseline=hapfreqs$haplotype[9] baseline #> [1] \"DTGCGCTCGCG\" haploX  <- haplo$haploX dlist(haploX,.~id|id %in% c(1,4,7)) #> id: 1 #>   y X1 X2 X3 X4 times lbnr__id Count1 #> 1 0 0  0  0  0  1     1        0      #> 2 0 0  0  0  0  2     2        0      #> 3 0 0  0  0  0  3     3        0      #> 4 0 0  0  0  0  4     4        0      #> 5 0 0  0  0  0  5     5        0      #> 6 0 0  0  0  0  6     6        0      #> ------------------------------------------------------------  #> id: 4 #>    y X1 X2 X3 X4 times lbnr__id Count1 #> 19 1 0  0  0  0  1     1        0      #> ------------------------------------------------------------  #> id: 7 #>    y X1 X2 X3 X4 times lbnr__id Count1 #> 37 0 1  0  0  0  1     1        0      #> 38 0 1  0  0  0  2     2        0      #> 39 1 1  0  0  0  3     3        0 ghaplos <- haplo$ghaplos head(ghaplos) #>    id      haplo1      haplo2          p #> 1   1 DTGCGCTCGCG DTGAGCTCGCG 1.00000000 #> 19  2 ITCCGTTGACG DTGAGCTCGCG 0.06867716 #> 21  2 ITCAGTTGACG DTGCGCTCGCG 0.93132284 #> 51  3 ITCCGTTGACG DTGAGCTCGCG 0.06867716 #> 53  3 ITCAGTTGACG DTGCGCTCGCG 0.93132284 #> 66  4 DTGCGCTCGCG DTGCGCTCGCG 1.00000000 designftypes <- function(x,sm=0) { hap1=x[1] hap2=x[2] if (sm==0) y <- 1*( (hap1==types) | (hap2==types)) if (sm==1) y <- 1*(hap1==types) + 1*(hap2==types) return(y) } haploX$time <- haploX$times Xdes <- model.matrix(~factor(time),haploX) colnames(Xdes) <- paste(\"X\",1:ncol(Xdes),sep=\"\") X <- dkeep(haploX,~id+y+time) X <- cbind(X,Xdes) Haplos <- dkeep(ghaplos,~id+\"haplo*\"+p) desnames=paste(\"X\",1:6,sep=\"\")   # six X's related to 6 cycles  head(X) #>   id y time X1 X2 X3 X4 X5 X6 #> 1  1 0    1  1  0  0  0  0  0 #> 2  1 0    2  1  1  0  0  0  0 #> 3  1 0    3  1  0  1  0  0  0 #> 4  1 0    4  1  0  0  1  0  0 #> 5  1 0    5  1  0  0  0  1  0 #> 6  1 0    6  1  0  0  0  0  1 out <- haplo.surv.discrete(X=X,y=\"y\",time.name=\"time\",       Haplos=Haplos,desnames=desnames,designfunc=designftypes)  names(out$coef) <- c(desnames,types) out$coef #>          X1          X2          X3          X4          X5          X6  #> -1.82153345 -0.61608261 -0.17143057 -1.27152045 -0.28635976 -0.19349091  #> DCGCGCTCACG DTCCGCTGACG ITCAGTTGACG ITCCGCTGAGG  #>  0.79753613  0.65747412  0.06119231  0.31666905 summary(out) #>             Estimate Std.Err     2.5%   97.5%   P-value #> X1          -1.82153  0.1619 -2.13892 -1.5041 2.355e-29 #> X2          -0.61608  0.1895 -0.98748 -0.2447 1.149e-03 #> X3          -0.17143  0.1799 -0.52398  0.1811 3.406e-01 #> X4          -1.27152  0.2631 -1.78719 -0.7559 1.346e-06 #> X5          -0.28636  0.2030 -0.68425  0.1115 1.584e-01 #> X6          -0.19349  0.2134 -0.61184  0.2249 3.647e-01 #> DCGCGCTCACG  0.79754  0.1494  0.50465  1.0904 9.445e-08 #> DTCCGCTGACG  0.65747  0.1621  0.33971  0.9752 5.007e-05 #> ITCAGTTGACG  0.06119  0.2145 -0.35931  0.4817 7.755e-01 #> ITCCGCTGAGG  0.31667  0.1361  0.04989  0.5834 1.999e-02 tcoef=c(-1.93110204,-0.47531630,-0.04118204,-1.57872602,-0.22176426,-0.13836416, 0.88830288,0.60756224,0.39802821,0.32706859)  cbind(out$coef,tcoef) #>                               tcoef #> X1          -1.82153345 -1.93110204 #> X2          -0.61608261 -0.47531630 #> X3          -0.17143057 -0.04118204 #> X4          -1.27152045 -1.57872602 #> X5          -0.28635976 -0.22176426 #> X6          -0.19349091 -0.13836416 #> DCGCGCTCACG  0.79753613  0.88830288 #> DTCCGCTGACG  0.65747412  0.60756224 #> ITCAGTTGACG  0.06119231  0.39802821 #> ITCCGCTGAGG  0.31666905  0.32706859 head(out$X,10) #>    X1 X2 X3 X4 X5 X6 haplo1 haplo2 haplo3 haplo4 #> 1   1  0  0  0  0  0      0      0      0      0 #> 2   1  1  0  0  0  0      0      0      0      0 #> 3   1  0  1  0  0  0      0      0      0      0 #> 4   1  0  0  1  0  0      0      0      0      0 #> 5   1  0  0  0  1  0      0      0      0      0 #> 6   1  0  0  0  0  1      0      0      0      0 #> 8   1  0  0  0  0  0      0      0      1      0 #> 10  1  1  0  0  0  0      0      0      1      0 #> 12  1  0  1  0  0  0      0      0      1      0 #> 14  1  0  0  1  0  0      0      0      1      0"},{"path":"http://kkholst.github.io/mets/articles/haplo-discrete-ttp.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Haplotype Discrete Survival Models","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/interval-discrete-survival.html","id":"discrete-inteval-censored-survival-times","dir":"Articles","previous_headings":"","what":"Discrete Inteval Censored survival times","title":"Discrete Interval Censored Survival Models","text":"consider cumulative odds model probability dying time t: logit(P(T≤t|x))=log(G(t))+xTβP(T≤t|x)=G(t)exp(xTβ)1+G(t)exp(xTβ)P(T>t|x)=11+G(t)exp(xTβ)\\begin{align*}    \\mbox{logit}(P(T \\leq t | x)) & = \\log(G(t)) + x^T \\beta              \\\\                 P(T \\leq t | x)  & =  \\frac{G(t) exp( x^T \\beta)}{1 + G(t) exp( x^T \\beta) }          \\\\                 P(T >t | x)  & =  \\frac{1}{1 + G(t) exp( x^T \\beta) }           \\end{align*} Input intervals given ]tl,tr]]t_l,t_r] t_r can infinity right-censored intervals. data discrete, contrast grouping continuous data, ]0,1]]0,1] intervals ]j,j+1]]j,j+1] equvilant observation j+1 (see example). Likelihood maximized: ∏iP(Ti>til|x)−P(Ti>tir|x).\\begin{align*}     \\prod_i  P(T_i >t_{il} | x) - P(T_i> t_{ir}| x).             \\end{align*} model also called cumulative odds model P(T≤t|x)=G(t)exp(xTβ)1+G(t)exp(xTβ).\\begin{align*}      P(T \\leq t | x)  & =  \\frac{ G(t) exp( x^T \\beta) }{1 + G(t) exp( x^T \\beta) }. \\end{align*} β\\beta says something probability tt. baseline parametrized G(t)=∑j≤texp(αj)\\begin{align*}        G(t)  & =  \\sum_{j \\leq t} \\exp( \\alpha_j )  \\end{align*} important consequence model cut-points tt parameters early later tt.","code":""},{"path":"http://kkholst.github.io/mets/articles/interval-discrete-survival.html","id":"discrete-ttp","dir":"Articles","previous_headings":"","what":"Discrete TTP","title":"Discrete Interval Censored Survival Models","text":"First look time pregnancy data (simulated discrete survival data) right-censored, set fit cumulative odds model constructing intervals appropriately: note probability dying increased considerably covariates. Now using discrete survival model simulate data model  Finally, look data compare icenReg package can also fit proportional odds model continous discrete data. make data fully interval censored/discrete letting also exact obsevations observed interval. consider interval censored survival times time onset diabetes diabetic nephronpathy, modify observe event times certain intervals. note gender effect equivalent two approaches. Also agrees cumulative link regression ordinal package, although baseline parametrized differently. additon clm describing probability surviving rather probabibility dying.","code":"library(mets)  data(ttpd)  dtable(ttpd,~entry+time2) #>  #>       time2   1   2   3   4   5   6 Inf #> entry                                   #> 0           316   0   0   0   0   0   0 #> 1             0 133   0   0   0   0   0 #> 2             0   0 150   0   0   0   0 #> 3             0   0   0  23   0   0   0 #> 4             0   0   0   0  90   0   0 #> 5             0   0   0   0   0  68   0 #> 6             0   0   0   0   0   0 220 out <- interval.logitsurv.discrete(Interval(entry,time2)~X1+X2+X3+X4,ttpd) summary(out) #> $baseline #>       Estimate Std.Err   2.5%   97.5%   P-value #> time1  -2.0064  0.1523 -2.305 -1.7079 1.273e-39 #> time2  -2.1749  0.1599 -2.488 -1.8614 4.118e-42 #> time3  -1.4581  0.1544 -1.761 -1.1554 3.636e-21 #> time4  -2.9260  0.2453 -3.407 -2.4453 8.379e-33 #> time5  -1.2051  0.1706 -1.539 -0.8706 1.633e-12 #> time6  -0.9102  0.1860 -1.275 -0.5457 9.843e-07 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1   0.9913  0.1179 0.76024 1.2223 4.100e-17 #> X2   0.6962  0.1162 0.46847 0.9238 2.064e-09 #> X3   0.3466  0.1159 0.11941 0.5738 2.788e-03 #> X4   0.3223  0.1151 0.09668 0.5478 5.111e-03 #>  #> $or #>    Estimate     2.5%    97.5% #> X1 2.694610 2.138791 3.394874 #> X2 2.006032 1.597554 2.518953 #> X3 1.414239 1.126834 1.774950 #> X4 1.380231 1.101503 1.729490  dfactor(ttpd) <- entry.f~entry out <- cumoddsreg(entry.f~X1+X2+X3+X4,ttpd) summary(out) #> $baseline #>       Estimate Std.Err   2.5%   97.5%   P-value #> time1  -2.0064  0.1523 -2.305 -1.7079 1.273e-39 #> time2  -2.1749  0.1599 -2.488 -1.8614 4.118e-42 #> time3  -1.4581  0.1544 -1.761 -1.1554 3.636e-21 #> time4  -2.9260  0.2453 -3.407 -2.4453 8.379e-33 #> time5  -1.2051  0.1706 -1.539 -0.8706 1.633e-12 #> time6  -0.9102  0.1860 -1.275 -0.5457 9.843e-07 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1   0.9913  0.1179 0.76024 1.2223 4.100e-17 #> X2   0.6962  0.1162 0.46847 0.9238 2.064e-09 #> X3   0.3466  0.1159 0.11941 0.5738 2.788e-03 #> X4   0.3223  0.1151 0.09668 0.5478 5.111e-03 #>  #> $or #>    Estimate     2.5%    97.5% #> X1 2.694610 2.138791 3.394874 #> X2 2.006032 1.597554 2.518953 #> X3 1.414239 1.126834 1.774950 #> X4 1.380231 1.101503 1.729490 set.seed(1000) # to control output in simulatins for p-values below. n <- 200 Z <- matrix(rbinom(n*4,1,0.5),n,4) outsim <- simlogitSurvd(out$coef,Z) outsim <- transform(outsim,left=time,right=time+1) outsim <- dtransform(outsim,right=Inf,status==0) outss <- interval.logitsurv.discrete(Interval(left,right)~+X1+X2+X3+X4,outsim) summary(outss) #> $baseline #>       Estimate Std.Err    2.5%   97.5%   P-value #> time1  -2.0154  0.3698 -2.7402 -1.2906 5.036e-08 #> time2  -1.5474  0.3473 -2.2281 -0.8666 8.385e-06 #> time3  -0.8119  0.3411 -1.4804 -0.1434 1.729e-02 #> time4  -2.0085  0.5102 -3.0084 -1.0086 8.248e-05 #> time5  -0.2185  0.3858 -0.9746  0.5376 5.711e-01 #> time6   0.2637  0.4618 -0.6415  1.1689 5.681e-01 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1  1.27893  0.2804  0.7293 1.8286 5.106e-06 #> X2  0.39293  0.2635 -0.1235 0.9094 1.359e-01 #> X3 -0.09008  0.2524 -0.5847 0.4045 7.211e-01 #> X4  0.20766  0.2627 -0.3072 0.7225 4.292e-01 #>  #> $or #>    Estimate      2.5%    97.5% #> X1 3.592796 2.0735647 6.225116 #> X2 1.481310 0.8838237 2.482711 #> X3 0.913858 0.5572845 1.498582 #> X4 1.230798 0.7355301 2.059553  pred <- predictlogitSurvd(out,se=TRUE) plotSurvd(pred,se=TRUE) test <- 0  if (test==1) {  require(icenReg) data(IR_diabetes) IRdia <- IR_diabetes ## removing fully observed data in continuous version, here making it a discrete observation  IRdia <- dtransform(IRdia,left=left-1,left==right) dtable(IRdia,~left+right,level=1)  ints <- with(IRdia,dInterval(left,right,cuts=c(0,5,10,20,30,40,Inf),show=TRUE) ) } if (test==1) { ints$Ileft <- ints$left ints$Iright <- ints$right IRdia <- cbind(IRdia,data.frame(Ileft=ints$Ileft,Iright=ints$Iright)) dtable(IRdia,~Ileft+Iright) #  #       Iright   1   2   3   4   5 Inf # Ileft                                # 0             10   1  34  25   4   0 # 1              0  55  19  17   1   1 # 2              0   0 393  16   4   0 # 3              0   0   0 127   1   0 # 4              0   0   0   0  21   0 # 5              0   0   0   0   0   2  outss <- interval.logitsurv.discrete(Interval(Ileft,Iright)~+gender,IRdia) #            Estimate Std.Err    2.5%    97.5%   P-value # time1        -3.934  0.3316 -4.5842 -3.28418 1.846e-32 # time2        -2.042  0.1693 -2.3742 -1.71038 1.710e-33 # time3         1.443  0.1481  1.1530  1.73340 1.911e-22 # time4         3.545  0.2629  3.0295  4.06008 1.976e-41 # time5         6.067  0.7757  4.5470  7.58784 5.217e-15 # gendermale   -0.385  0.1691 -0.7165 -0.05351 2.283e-02 summary(outss) outss$ploglik # [1] -646.1946  fit <- ic_sp(cbind(Ileft, Iright) ~ gender, data = IRdia, model = \"po\") #  # Model:  Proportional Odds # Dependency structure assumed: Independence # Baseline:  semi-parametric  # Call: ic_sp(formula = cbind(Ileft, Iright) ~ gender, data = IRdia,  #     model = \"po\") #  #            Estimate Exp(Est) # gendermale    0.385     1.47 #  # final llk =  -646.1946  # Iterations =  6  # Bootstrap Samples =  0  # WARNING: only  0  bootstrap samples used for standard errors.  # Suggest using more bootstrap samples for inference summary(fit)  ## sometimes NR-algorithm needs modifications of stepsize to run  ## outss <- interval.logitsurv.discrete(Interval(Ileft,Iright)~+gender,IRdia,control=list(trace=TRUE,stepsize=1.0)) } data(ttpd)  dtable(ttpd,~entry+time2) #>  #>       time2   1   2   3   4   5   6 Inf #> entry                                   #> 0           316   0   0   0   0   0   0 #> 1             0 133   0   0   0   0   0 #> 2             0   0 150   0   0   0   0 #> 3             0   0   0  23   0   0   0 #> 4             0   0   0   0  90   0   0 #> 5             0   0   0   0   0  68   0 #> 6             0   0   0   0   0   0 220 ttpd <- dfactor(ttpd,fentry~entry) out <- cumoddsreg(fentry~X1+X2+X3+X4,ttpd) summary(out) #> $baseline #>       Estimate Std.Err   2.5%   97.5%   P-value #> time1  -2.0064  0.1523 -2.305 -1.7079 1.273e-39 #> time2  -2.1749  0.1599 -2.488 -1.8614 4.118e-42 #> time3  -1.4581  0.1544 -1.761 -1.1554 3.636e-21 #> time4  -2.9260  0.2453 -3.407 -2.4453 8.379e-33 #> time5  -1.2051  0.1706 -1.539 -0.8706 1.633e-12 #> time6  -0.9102  0.1860 -1.275 -0.5457 9.843e-07 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1   0.9913  0.1179 0.76024 1.2223 4.100e-17 #> X2   0.6962  0.1162 0.46847 0.9238 2.064e-09 #> X3   0.3466  0.1159 0.11941 0.5738 2.788e-03 #> X4   0.3223  0.1151 0.09668 0.5478 5.111e-03 #>  #> $or #>    Estimate     2.5%    97.5% #> X1 2.694610 2.138791 3.394874 #> X2 2.006032 1.597554 2.518953 #> X3 1.414239 1.126834 1.774950 #> X4 1.380231 1.101503 1.729490  out$ploglik #> [1] -1676.456  if (test==1) { ### library(ordinal) ### out1 <- clm(fentry~X1+X2+X3+X4,data=ttpd) ### summary(out1)  # formula: fentry ~ X1 + X2 + X3 + X4 # data:    ttpd #  #  link  threshold nobs logLik   AIC     niter max.grad cond.H  #  logit flexible  1000 -1676.46 3372.91 6(2)  1.17e-12 5.3e+02 #  # Coefficients: #    Estimate Std. Error z value Pr(>|z|)     # X1  -0.9913     0.1171  -8.465  < 2e-16 *** # X2  -0.6962     0.1156  -6.021 1.74e-09 *** # X3  -0.3466     0.1150  -3.013  0.00259 **  # X4  -0.3223     0.1147  -2.810  0.00495 **  # --- # Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #  # Threshold coefficients: #     Estimate Std. Error z value # 0|1  -2.0064     0.1461 -13.733 # 1|2  -1.3940     0.1396  -9.984 # 2|3  -0.7324     0.1347  -5.435 # 3|4  -0.6266     0.1343  -4.667 # 4|5  -0.1814     0.1333  -1.361 # 5|6   0.2123     0.1342   1.582 }"},{"path":"http://kkholst.github.io/mets/articles/interval-discrete-survival.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Discrete Interval Censored Survival Models","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/marginal-cox.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Marginal modelling of clustered survival data","text":"basic component modelling multivariate survival data many models build around marginals Cox form. marginal Cox model can fitted efficiently mets package, particular handling strata robust standard errors optimized. basic models assumes subject marginal Cox-form λg(k,)(t)exp(XkiTβ).  \\lambda_{g(k,)}(t) \\exp( X_{ki}^T \\beta).  g(k,)g(k,) gives strata subject. discuss show get robust standard errors regression parameters baseline goodness fit test using cumulative residuals score test First generate data Clayton-Oakes model, 55 members cluster variance parameter 22 data one subject per row. time : time event status : 1 event 0 censoring x : x binary covariate cluster : cluster Now fit model produce robust standard errors regression parameters baseline. First, recall baseline strata gg asymptotically equivalent ̂g(t)−Ag(t)=∑k∈g(∑:ki∈g∫0t1S0,gdMkig−Pg(t)βk)\\begin{align} \\hat A_g(t) - A_g(t)  & = \\sum_{k \\g} \\left( \\sum_{: ki \\g} \\int_0^t \\frac{1}{S_{0,g}} dM_{ki}^g  - P^g(t) \\beta_k \\right)  \\end{align} Pg(t)=∫0tEg(s)dΛ̂g(s)P^g(t) = \\int_0^t E_g(s) d \\hat \\Lambda_g(s) derivative ∫0t1/S0,g(s)dN⋅g\\int_0^t 1/S_{0,g}(s) dN_{\\cdot g} wrt β\\beta, β̂−β=(τ)−1∑k(∑∫0τ(Zki−Eg)dMkig)=∑kβk\\begin{align} \\hat \\beta  - \\beta  & = (\\tau)^{-1} \\sum_k ( \\sum_i \\int_0^\\tau (Z_{ki} - E_{g}) dM_{ki}^g ) =  \\sum_k \\beta_{k}  \\end{align} Mkig(t)=Nki(t)−∫0tYki(s)exp(Zkiβ)dΛg(k,)(t),βk=(τ)−1∑∫0τ(Zki−Eg)dMkig\\begin{align}  M_{ki}^g(t) &  = N_{ki}(t) - \\int_0^t Y_{ki}(s) \\exp( Z_{ki} \\beta) d \\Lambda_{g(k,)}(t), \\\\  \\beta_{k} & =  (\\tau)^{-1} \\sum_i \\int_0^\\tau (Z_{ki} - E_{g}) dM_{ki}^g  \\end{align} basic 0-mean processes, martingales iid setting, (t)(t) derivative total score, Û(t,β))\\hat U(t,\\beta)), respect β\\beta evaluated time tt. variance baseline strata g estimated ∑k∈g(∑:ki∈g∫0t1S0,g(k,)dM̂kig−Pg(t)βk)2\\begin{align} \\sum_{k \\g} ( \\sum_{: ki \\g} \\int_0^t \\frac{1}{S_{0,g(k,)}} d\\hat M_{ki}^g - P^g(t) \\beta_k )^2 \\end{align} can computed using particular structure dM̂ikg(t)=dNik(t)−1S0,g(,k)exp(Zikβ)dNg.(t)\\begin{align} d \\hat M_{ik}^g(t) & =  dN_{ik}(t) -  \\frac{1}{S_{0,g(,k)}} \\exp(Z_{ik} \\beta) dN_{g.}(t)  \\end{align} robust variance baseline iid decomposition β\\beta computed mets : can get iid decomposition β̂−β\\hat \\beta - \\beta now look plot robust standard errors  can also make survival prediction robust standard errors using phreg.  Finally, just check can recover model also estimate dependence parameter","code":"library(mets)  options(warn=-1)  set.seed(1000) # to control output in simulatins for p-values below.  n <- 1000  k <- 5  theta <- 2  data <- simClaytonOakes(n,k,theta,0.3,3) out <- phreg(Surv(time,status)~x+cluster(cluster),data=data)    summary(out) #>  #>     n events #>  5000   4854 #> coeffients: #>   Estimate     S.E.  dU^-1/2 P-value #> x 0.287859 0.028177 0.028897       0 #>  #> exp(coeffients): #>   Estimate   2.5%  97.5% #> x   1.3336 1.2619 1.4093    # robust standard errors attached to output    rob <- robust.phreg(out) # making iid decomposition of regression parameters    betaiid <- IC(out)    head(betaiid) #>             x #> 1 -0.34616008 #> 2 -1.44918926 #> 3 -0.03898156 #> 4  0.42156050 #> 5  0.34253904 #> 6 -0.07706668    # robust standard errors    crossprod(betaiid/NROW(betaiid))^.5 #>            x #> x 0.02817714    # same as plot(rob,se=TRUE,robust=TRUE,col=3) pp <-  predict(out,data[1:20,],se=TRUE,robust=TRUE)   plot(pp,se=TRUE,whichx=1:10) tt <- twostageMLE(out,data=data) summary(tt) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.         SE        z P-val Kendall tau        SE #> dependence1 0.5316753 0.03497789 15.20032     0   0.2100093 0.0109146 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/marginal-cox.html","id":"goodness-of-fit","dir":"Articles","previous_headings":"","what":"Goodness of fit","title":"Marginal modelling of clustered survival data","text":"observed score process given U(t,β̂)=∑k∑∫0t(Zki−Êg)dM̂kig\\begin{align} U(t,\\hat \\beta) & = \\sum_k \\sum_i \\int_0^t (Z_{ki} - \\hat E_g ) d \\hat M_{ki}^g  \\end{align} gg strata g(k,)g(k,). observed score iid decomposition Û(t)=∑k∑∫0t(Zki−Eg)dMkig−(t)∑kβk\\begin{align} \\hat U(t) = \\sum_k \\sum_i  \\int_0^t (Z_{ki} - E_g) dM_{ki}^g  -      (t)  \\sum_k \\beta_k  \\end{align} βk\\beta_k iid decomposition score process true β\\betaβk=(τ)−1∑∫0τ(Zki−Eg)dMkig\\begin{align} \\beta_k  & =  (\\tau)^{-1} \\sum_i \\int_0^\\tau (Z_{ki} - E_g ) d  M_{ki}^g  \\end{align} (t)(t) derivative total score, Û(t,β))\\hat U(t,\\beta)), respect β\\beta evaluated time tt. observed score can resampled given iid form terms clusters. Now using cumulative score process checking proportional hazards p-value reflects wheter observed score process consistent model.","code":"gout <- gof(out) gout #> Cumulative score process test for Proportionality: #>   Sup|U(t)|  pval #> x  30.24353 0.401 plot(gout)"},{"path":"http://kkholst.github.io/mets/articles/marginal-cox.html","id":"computational-aspects","dir":"Articles","previous_headings":"Goodness of fit","what":"Computational aspects","title":"Marginal modelling of clustered survival data","text":"score processes can resampled Lin, Wei, Ying (1993) using martingale structure, observed score process resampled ∑k∑∫0tgki(Zki−Eg)dNki−(t)−1(τ)gki∫0τ(Zki−Eg)dNki.\\begin{align}   \\sum_k \\sum_i \\int_0^t g_{ki} (Z_{ki} - E_g) dN_{ki}  - (t)  ^{-1}(\\tau) g_{ki} \\int_0^{\\tau} (Z_{ki} - E_g) dN_{ki} . \\end{align} gkig_{ki} ..d. standard normals. Based zero mean processes generally clusters can resample score process. resampling score process need U(t,β)=∑k∑igk∫0t(Zki−Eg)dMkig\\begin{align} U(t,\\beta) & = \\sum_k \\sum_i g_k \\int_0^t (Z_{ki} - E_g ) dM_{ki}^g  \\end{align} gg strata. write gkg_k gkig_{ki} thus repeating gkg_k within cluster. Computations done using ∫0t(Zki−Eg)dMkig=∫0t(Zki−Eg)dNkig−∫0t(Zki−Eg)Yki(u)dΛg(u)\\begin{align*} \\int_0^t (Z_{ki} - E_{g}) dM_{ki}^g & = \\int_0^t (Z_{ki} - E_{g}) dN_{ki}^g - \\int_0^{t} (Z_{ki} - E_{g})  Y_{ki}(u) d\\Lambda^g(u)   \\end{align*} therefore summing compensator part gkig_{ki} multipliers gives strata gg∫0tS1gw(u)S0g(u)dNg.(v)−∫0tEg(u)S0gw(u)S0g(u)dNg.(v)\\begin{align*}    &  \\int_0^t \\frac{S_{1g}^w(u)}{S_{0g}(u)} dN_{g.}(v) -  \\int_0^t E_{g}(u) \\frac{S_{0g}^w(u)}{S_{0g}(u)} dN_{g.}(v) \\end{align*} withSjgw(t)=∑ki∈gexp(Zkiβ)ZkijYki(t)gkiSjg(t)=∑ki∈gexp(Zkiβ)ZkijYki(t).\\begin{align*} S_{jg}^w(t) & =  \\sum_{ki \\g} \\exp(Z_{ki} \\beta) Z_{ki}^j Y_{ki}(t) g_{ki}  \\\\ S_{jg}(t) & =  \\sum_{ki \\g} \\exp(Z_{ki} \\beta) Z_{ki}^j Y_{ki}(t). \\end{align*}","code":""},{"path":"http://kkholst.github.io/mets/articles/marginal-cox.html","id":"cluster-stratified-cox-models","dir":"Articles","previous_headings":"","what":"Cluster stratified Cox models","title":"Marginal modelling of clustered survival data","text":"clustered data possible estimate regression coefficient within clusters using Cox’s partial likelihood stratified clusters. Note, data generated different subject specific structure, recover β\\beta 0.3 model proportional Cox model, also expect reject “proportionality” gof-test. model can thought λg(k,)(t)exp(XkiTβ)  \\lambda_{g(k,)} (t) \\exp( X_{ki}^T \\beta)  λg(t)\\lambda_g(t) cluster specific baseline. regression coefficient β\\beta can estimated using partial likelihood clusters.","code":"out <- phreg(Surv(time,status)~x+strata(cluster),data=data)  summary(out) #>  #>     n events #>  5000   4854 #> coeffients: #>   Estimate     S.E.  dU^-1/2 P-value #> x 0.406307 0.032925 0.039226       0 #>  #> exp(coeffients): #>   Estimate   2.5%  97.5% #> x   1.5013 1.4074 1.6013"},{"path":"http://kkholst.github.io/mets/articles/marginal-cox.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Marginal modelling of clustered survival data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Mediation Analysis for survival data","text":"Fit binomial-regression IPCW, binreg additive Lin-Ying model, aalenMets cox model phreg standard logistic regression via binreg context mediation analysis using mediation weights medFlex package. thus fit natural effects models, example binary scale might state logit(P(Y(x,M(x*))=1|Z)=β0+β1x+β2x*+β3TZ,\\begin{align*} \\mbox{logit}(P(Y(x,M(x^*))=1| Z)  = \\beta_0+ \\beta_1 x + \\beta_2 x^* + \\beta_3^T Z, \\end{align*} case Natural Direct Effect (NDE) fixed covariates ZZ OR1,0|ZNDE=odds(Y(1,M(x))|Z)odds(Y(0,M(x))|Z)=exp(β1),\\begin{align*}  \\mbox{}_{1,0|Z}^{\\mbox{NDE}} = \\frac{\\mbox{odds}(Y(1,M(x))|Z)}{\\mbox{odds}(Y(0,M(x))|Z)}  = \\exp(\\beta_1), \\end{align*} Natural Inderect Effect (NIE) fixed covariates ZZ OR1,0|ZNIE=odds(Y(x,M(1))|Z)odds(Y(x,M(0))|Z)=exp(β2).\\begin{align*}  \\mbox{}_{1,0|Z}^{\\mbox{NIE}} = \\frac{\\mbox{odds}(Y(x,M(1))|Z)}{\\mbox{odds}(Y(x,M(0))|Z)} = \\exp(\\beta_2). \\end{align*} See medFlex package additional discussion parametrization. mediator can binomial using glm-binomial. multnomial via mlogit function mets mediator exposure must coded factors. example mediator: gp.f exposure : dnr.f outcome model concerned risk/hazard cause=2. key standard errors computed using ..d influence functions Taylor expansion deal uncertainty mediation weights.","code":""},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"simulated-data","dir":"Articles","previous_headings":"","what":"Simulated Data","title":"Mediation Analysis for survival data","text":"First simulate data mimics Kumar et al 2012. data multiple myeloma patients treated allogeneic stem cell transplantation Center International Blood Marrow Transplant Research (CIBMTR) Kumar et al (2012), “Trends allogeneic stem cell transplantation multiple myeloma: CIBMTR analysis”. data used paper consist patients transplanted 1995 2005, compared outcomes transplant periods: 2001-2005 (N=488) versus 1995-2000 (N=375). two competing events relapse (cause 2) treatment-related mortality (TRM, cause 1)) defined death without relapse. considered following risk covariates: transplant time period (gp (main interest study): 1 transplanted 2001-2005 versus 0 transplanted 1995-2000), donor type (dnr: 1 Unrelated related donor (N=280) versus 0 HLA-identical sibling (N=584)), prior autologous transplant (preauto: 1 Auto+Allo transplant (N=399) versus 0 allogeneic transplant alone (N=465)) time transplant (ttt24: 1 24 months (N=289) versus 0 less equal 24 months (N=575))). interest effect period (gp) possible mediation via amount unrealted related donors (dnr). somewhat artificial example ! adjusted important counfounders.","code":"library(mets)  runb <- 0  options(warn=-1)  set.seed(1000) # to control output in simulatins for p-values below.  n <- 200; k.boot <- 10;   dat <- kumarsimRCT(n,rho1=0.5,rho2=0.5,rct=2,censpar=c(0,0,0,0),           beta = c(-0.67, 0.59, 0.55, 0.25, 0.98, 0.18, 0.45, 0.31),     treatmodel = c(-0.18, 0.56, 0.56, 0.54),restrict=1) dfactor(dat) <- dnr.f~dnr dfactor(dat) <- gp.f~gp drename(dat) <- ttt24~\"ttt24*\" dat$id <- 1:n dat$ftime <- 1"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"mediation-weights","dir":"Articles","previous_headings":"","what":"Mediation Weights","title":"Mediation Analysis for survival data","text":"compute mediation weights based mediation model","code":"weightmodel <- fit <- glm(gp.f~dnr.f+preauto+ttt24,data=dat,family=binomial) wdata <- medweight(fit,data=dat)"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"binomial-regression","dir":"Articles","previous_headings":"","what":"Binomial Regression","title":"Mediation Analysis for survival data","text":"simple multvariate regression probaibility relapse 50 months exposure mediator (given covariates)","code":"aaMss2 <- binreg(Event(time,status)~gp+dnr+preauto+ttt24+cluster(id),data=dat,time=50,cause=2) summary(aaMss2) #>    n events #>  200     97 #>  #>  200 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept) -1.01508  0.31869 -1.63971 -0.39046  0.0014 #> gp           1.08533  0.34216  0.41471  1.75594  0.0015 #> dnr          0.51969  0.35757 -0.18113  1.22051  0.1461 #> preauto      0.39417  0.35936 -0.31017  1.09851  0.2727 #> ttt24        0.50469  0.38681 -0.25344  1.26283  0.1920 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.36237 0.19404 0.6767 #> gp           2.96041 1.51394 5.7889 #> dnr          1.68151 0.83433 3.3889 #> preauto      1.48316 0.73332 2.9997 #> ttt24        1.65648 0.77612 3.5354"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"binomial-regression-ipcw-mediation-analysis","dir":"Articles","previous_headings":"","what":"Binomial regression IPCW Mediation Analysis","title":"Mediation Analysis for survival data","text":"first look probability relapse 50 months NDE 1.40(0.72,2.76)1.40 (0.72,2.76) NIE 1.32(1.05,1.66)1.32 (1.05,1.66).","code":"### binomial regression ########################################################### aaMss <- binreg(Event(time,status)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,         time=50,weights=wdata$weights,cause=2) summary(aaMss) #>    n events #>  400    194 #>  #>  200 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.535534  0.256218 -1.037712 -0.033356  0.0366 #> dnr.f01      0.375817  0.348618 -0.307462  1.059095  0.2810 #> dnr.f11      0.275383  0.071199  0.135836  0.414931  0.0001 #> preauto      0.588221  0.350437 -0.098624  1.275066  0.0932 #> ttt24        0.266179  0.363603 -0.446469  0.978827  0.4641 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.58536 0.35426 0.9672 #> dnr.f01      1.45618 0.73531 2.8838 #> dnr.f11      1.31704 1.14549 1.5143 #> preauto      1.80078 0.90608 3.5789 #> ttt24        1.30497 0.63988 2.6613  ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    194 #>  #>  200 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.535534  0.254832 -1.034995 -0.036073  0.0356 #> dnr.f01      0.375817  0.317732 -0.246927  0.998560  0.2369 #> dnr.f11      0.275383  0.117175  0.045726  0.505041  0.0188 #> preauto      0.588221  0.346523 -0.090951  1.267394  0.0896 #> ttt24        0.266179  0.366361 -0.451875  0.984233  0.4675 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.58536 0.35523 0.9646 #> dnr.f01      1.45618 0.78120 2.7144 #> dnr.f11      1.31704 1.04679 1.6571 #> preauto      1.80078 0.91306 3.5516 #> ttt24        1.30497 0.63643 2.6758 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"mediation-analysis","dir":"Articles","previous_headings":"","what":"Mediation Analysis","title":"Mediation Analysis for survival data","text":"also illustrate use models mentioned .","code":"### lin-ying model ################################################################ aaMss <- aalenMets(Surv(time/100,status==2)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,            weights=wdata$weights) ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    196 #> coeffients: #>          Estimate   Std.Err      2.5%     97.5% P-value #> dnr.f01  1.169592  0.739323 -0.279454  2.618637  0.1137 #> dnr.f11  0.206757  0.131289 -0.050565  0.464078  0.1153 #> preauto  0.617537  0.504302 -0.370877  1.605950  0.2207 #> ttt24    0.457736  0.517822 -0.557175  1.472648  0.3767 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}  ### cox model ############################################################################### aaMss <- phreg(Surv(time,status==2)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,            weights=wdata$weights) summary(aaMss) #>  #>    n events #>  400    196 #> coeffients: #>         Estimate     S.E.  dU^-1/2 P-value #> dnr.f01 0.414565 0.213724 0.157231  0.0524 #> dnr.f11 0.100656 0.039308 0.144971  0.0104 #> preauto 0.284460 0.232166 0.162375  0.2205 #> ttt24   0.185561 0.226044 0.160886  0.4117 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.51371 0.99568 2.3013 #> dnr.f11  1.10590 1.02389 1.1945 #> preauto  1.32904 0.84318 2.0949 #> ttt24    1.20389 0.77300 1.8750 ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    196 #> coeffients: #>            Estimate     Std.Err        2.5%       97.5% P-value #> dnr.f01  0.41456472  0.20869639  0.00552731  0.82360212  0.0470 #> dnr.f11  0.10065575  0.05121458  0.00027702  0.20103448  0.0494 #> preauto  0.28445952  0.23037280 -0.16706288  0.73598192  0.2169 #> ttt24    0.18556110  0.22549763 -0.25640614  0.62752835  0.4106 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.51371 1.00554 2.2787 #> dnr.f11  1.10590 1.00028 1.2227 #> preauto  1.32904 0.84615 2.0875 #> ttt24    1.20389 0.77383 1.8730 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}  ### Fine-Gray #############################################################3 aaMss <- cifreg(Event(time,status)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,         weights=wdata$weights,propodds=NULL,cause=2) summary(aaMss) #>  #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>         Estimate    S.E. dU^-1/2 P-value #> dnr.f01  0.18943 0.21986 0.15855  0.3889 #> dnr.f11  0.18730 0.04083 0.14503  0.0000 #> preauto  0.41452 0.22783 0.16098  0.0688 #> ttt24    0.17304 0.22892 0.16308  0.4497 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.20856 0.78545 1.8596 #> dnr.f11  1.20599 1.11324 1.3065 #> preauto  1.51364 0.96849 2.3656 #> ttt24    1.18892 0.75910 1.8621 ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>          Estimate   Std.Err      2.5%     97.5% P-value #> dnr.f01  0.189426  0.233939 -0.269087  0.647939  0.4181 #> dnr.f11  0.187298  0.047733  0.093744  0.280853  0.0001 #> preauto  0.414517  0.230676 -0.037600  0.866634  0.0723 #> ttt24    0.173042  0.230810 -0.279338  0.625422  0.4534 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.20856 0.76408 1.9116 #> dnr.f11  1.20599 1.09828 1.3243 #> preauto  1.51364 0.96310 2.3789 #> ttt24    1.18892 0.75628 1.8690 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}  ### logit model  #############################################################3 aaMss <- cifreg(Event(time,status)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,         weights=wdata$weights,cause=2) summary(aaMss) #>  #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>         Estimate     S.E.  dU^-1/2 P-value #> dnr.f01 0.357168 0.339848 0.158937  0.2933 #> dnr.f11 0.272392 0.064166 0.145076  0.0000 #> preauto 0.657010 0.326082 0.160361  0.0439 #> ttt24   0.191333 0.353606 0.167443  0.5884 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.42928 0.73424 2.7822 #> dnr.f11  1.31310 1.15792 1.4891 #> preauto  1.92902 1.01806 3.6551 #> ttt24    1.21086 0.60549 2.4215 ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>          Estimate   Std.Err      2.5%     97.5% P-value #> dnr.f01  0.357168  0.351089 -0.330953  1.045289  0.3090 #> dnr.f11  0.272392  0.068131  0.138857  0.405927  0.0001 #> preauto  0.657010  0.328207  0.013736  1.300284  0.0453 #> ttt24    0.191333  0.356086 -0.506583  0.889250  0.5910 #>  #> exp(coeffients): #>         Estimate    2.5%  97.5% #> dnr.f01  1.42928 0.71824 2.8442 #> dnr.f11  1.31310 1.14896 1.5007 #> preauto  1.92902 1.01383 3.6703 #> ttt24    1.21086 0.60255 2.4333 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}  ### binomial outcome  ############################ aaMss <- binreg(Event(ftime,status)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),data=wdata,         time=50,weights=wdata$weights,cens.weights=1,cause=2) summary(aaMss) #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.674433  0.235285 -1.135583 -0.213284  0.0042 #> dnr.f01      0.221834  0.318264 -0.401952  0.845620  0.4858 #> dnr.f11      0.262722  0.060281  0.144572  0.380871  0.0000 #> preauto      0.578077  0.319091 -0.047331  1.203484  0.0700 #> ttt24        0.214442  0.328183 -0.428784  0.857669  0.5135 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.50944 0.32123 0.8079 #> dnr.f01      1.24836 0.66901 2.3294 #> dnr.f11      1.30046 1.15555 1.4636 #> preauto      1.78261 0.95377 3.3317 #> ttt24        1.23917 0.65130 2.3577 ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  400    196 #>  #>  200 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.674433  0.235022 -1.135069 -0.213798  0.0041 #> dnr.f01      0.221834  0.286717 -0.340122  0.783789  0.4391 #> dnr.f11      0.262722  0.107508  0.052011  0.473432  0.0145 #> preauto      0.578077  0.315260 -0.039822  1.195975  0.0667 #> ttt24        0.214442  0.329107 -0.430596  0.859480  0.5147 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.50944 0.32140 0.8075 #> dnr.f01      1.24836 0.71168 2.1898 #> dnr.f11      1.30046 1.05339 1.6055 #> preauto      1.78261 0.96096 3.3068 #> ttt24        1.23917 0.65012 2.3619 if (runb>0) { bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=k.boot); summary(bll)}"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"multinomial-regression","dir":"Articles","previous_headings":"","what":"Multinomial regression","title":"Mediation Analysis for survival data","text":"Also works mediator two levels meditor: wmi 4 categories exposure: age 4 categories","code":"library(mets) data(tTRACE) dcut(tTRACE) <- ~.   weightmodel <- fit <- mlogit(wmicat.4 ~agecat.4+vf+chf,data=tTRACE,family=binomial) wdata <- medweight(fit,data=tTRACE)  aaMss <- binreg(Event(time,status)~agecat.40+ agecat.41+ vf+chf+cluster(id),data=wdata,         time=7,weights=wdata$weights,cause=9) summary(aaMss) MultMed <- mediatorSurv(aaMss,fit,data=tTRACE,wdata=wdata) summary(MultMed) summary(MultMed) #>     n events #>  4000   2016 #>  #>  1000 clusters #> coeffients: #>                       Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)          -1.839306  0.174541 -2.181401 -1.497211  0.0000 #> agecat.40(60.1,68.6]  0.916646  0.223488  0.478618  1.354674  0.0000 #> agecat.40(68.6,75.6]  1.363830  0.222418  0.927898  1.799762  0.0000 #> agecat.40(75.6,96.3]  2.277415  0.249815  1.787786  2.767044  0.0000 #> agecat.41(60.1,68.6]  0.121100  0.053334  0.016567  0.225633  0.0232 #> agecat.41(68.6,75.6]  0.119374  0.053193  0.015118  0.223631  0.0248 #> agecat.41(75.6,96.3]  0.095356  0.053874 -0.010234  0.200947  0.0767 #> vf                    0.712461  0.293627  0.136962  1.287960  0.0152 #> chf                   1.166578  0.154721  0.863331  1.469825  0.0000 #>  #> exp(coeffients): #>                      Estimate    2.5%   97.5% #> (Intercept)           0.15893 0.11288  0.2238 #> agecat.40(60.1,68.6]  2.50089 1.61384  3.8755 #> agecat.40(68.6,75.6]  3.91114 2.52919  6.0482 #> agecat.40(75.6,96.3]  9.75144 5.97621 15.9115 #> agecat.41(60.1,68.6]  1.12874 1.01671  1.2531 #> agecat.41(68.6,75.6]  1.12679 1.01523  1.2506 #> agecat.41(75.6,96.3]  1.10005 0.98982  1.2226 #> vf                    2.03900 1.14678  3.6254 #> chf                   3.21099 2.37105  4.3485"},{"path":"http://kkholst.github.io/mets/articles/mediation-survival.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Mediation Analysis for survival data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"two-stage-randomization-for-counting-process-outcomes","dir":"Articles","previous_headings":"","what":"Two-Stage Randomization for counting process outcomes","title":"Randomization for Cox Type rate models ","text":"Specify rate models N1(t)N_1(t). survival data competing risks, cause specific hazards. recurrent events data simple randomization can estimate rate Cox model λ0(t)exp(A0β0)\\lambda_0(t) \\exp(A_0 \\beta_0) two-stage randomization can estimate rate Cox model λ0(t)exp(A0β0+A1(t)β1)\\lambda_0(t) \\exp(A_0 \\beta_0 + A_1(t) \\beta_1 ) Starting point Cox’s partial likelihood score can used estimating parameters U(β)=∫((t)−e(t))dN1(t)\\begin{align*}    U(\\beta) & = \\int ((t) - e(t)) dN_1(t)  \\end{align*} (t)(t) combined treatments time. solution converge β*\\beta^* Struthers-Kalbfleisch solution score robust standard errors Lin-Wei. estimator can agumented different ways using additional covariates time randomization censoring augmentation. solved estimating eqution ∑iUi−AUG0−AUG1+AUGC=0\\begin{align*}  \\sum_i   U_i - AUG_0 - AUG_1 + AUG_C   = 0   \\end{align*} using covariates augmentR0 augment AUG0=(A0−π0(X0))X0γ0\\begin{align*}  AUG_0 = ( A_0 - \\pi_0(X_0) ) X_0 \\gamma_0  \\end{align*} possibly P(A0=1|X0)=π0(X0)P(A_0=1|X_0)=\\pi_0(X_0) depend covariates randomization, furhter using covariates augmentR1, augment R indiciating randomization takes place , AUG1=R(A1−π1(X1))X1γ1\\begin{align*}  AUG_1 = R ( A_1 - \\pi_1(X_1))  X_1 \\gamma_1  \\end{align*} dynamic censoring augmentingAUGC=∫0tγc(s)T(e(s)−e‾(s))1Gc(s)dMc(s)\\begin{align*}   AUG_C =  \\int_0^t \\gamma_c(s)^T (e(s) - \\bar e(s))  \\frac{1}{G_c(s) } dM_c(s)   \\end{align*} γc(s)\\gamma_c(s) chosen minimize variance given dynamic covariates specified augmentC. propensity score models always estimated unless requested use fixed number π0=1/2\\pi_0=1/2 example, always better adaptive estimate π0\\pi_0. Also γ0\\gamma_0 γ1\\gamma_1 estimated reduce variance UiU_i. treatment’s must given factors. Treatment probabilities estimated default uncertainty adjusted . treat.model must typically allow interaction treatment number covariates typesR=c(“R0”,“R1”,“R01”) default model stratify randomization R0 cens.model can specified typesC=c(“C”,“dynC”), C fixed coefficients dynC dynamic done strata censoring model Standard errors estimated using influence function estimators tests differences can therefore computed subsequently. variance adjustment censoring augmentation computed subtracting variance gain influence functions given case R0 times randomization specified default assume time-points corresponds treatment, survival case without time-dependent covariates recurrent events situation must specified first record subject, see example. Data must given start,stop,status survival format one code status indicating events interest one code censorings phreg_rct can used counting process style data, thus covers situations recurrent events survival data cause-specific hazards (competing risks) cases compute augmentations dynC R0, R1 R01","code":""},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"simple-randomization-lu-tsiatis-marginal-cox-model","dir":"Articles","previous_headings":"","what":"Simple Randomization: Lu-Tsiatis marginal Cox model","title":"Randomization for Cox Type rate models ","text":"Results consitent speff library(speff2trial) study actually block-randomized according (?) standard computed adjustment equivalent augmenting block factor","code":"library(mets)  set.seed(100)  ## Lu, Tsiatis simulation data <- mets:::simLT(0.7,100) dfactor(data) <- Z.f~Z   out <- phreg_rct(Surv(time,status)~Z.f,data=data,augmentR0=~X,augmentC=~factor(Z):X) summary(out) #>                 Estimate   Std.Err       2.5%     97.5%   P-value #> Marginal-Z.f1 0.29263400 0.2739159 -0.2442313 0.8294993 0.2853693 #> R0_C:Z.f1     0.07166242 0.2234066 -0.3662065 0.5095313 0.7483838 #> R0_dynC:Z.f1  0.08321604 0.2221710 -0.3522312 0.5186633 0.7079889 #> attr(,\"class\") #> [1] \"summary.phreg_rct\" ###out <- phreg_rct(Surv(time,status)~Z.f,data=data,augmentR0=~X,augmentC=~X) ###out <- phreg_rct(Surv(time,status)~Z.f,data=data,augmentR0=~X,augmentC=~factor(Z):X,cens.model=~+1) ###library(speff2trial)  library(mets) data(ACTG175) ### data <- ACTG175[ACTG175$arms==0 | ACTG175$arms==1, ] data <- na.omit(data[,c(\"days\",\"cens\",\"arms\",\"strat\",\"cd40\",\"cd80\",\"age\")]) data$days <- data$days+runif(nrow(data))*0.01 dfactor(data) <- arms.f~arms notrun <- 1  if (notrun==0) {  fit1 <- speffSurv(Surv(days,cens)~cd40+cd80+age,data=data,trt.id=\"arms\",fixed=TRUE) summary(fit1) } #  # Treatment effect #             Log HR       SE   LowerCI   UpperCI           p # Prop Haz  -0.70375  0.12352  -0.94584  -0.46165  1.2162e-08 # Speff     -0.72430  0.12051  -0.96050  -0.48810  1.8533e-09  out <- phreg_rct(Surv(days,cens)~arms.f,data=data,augmentR0=~cd40+cd80+age,augmentC=~cd40+cd80+age) summary(out) #>                    Estimate   Std.Err       2.5%      97.5%      P-value #> Marginal-arms.f1 -0.7036460 0.1224406 -0.9436251 -0.4636669 9.092786e-09 #> R0_C:arms.f1     -0.7265342 0.1197607 -0.9612610 -0.4918075 1.306891e-09 #> R0_dynC:arms.f1  -0.7204699 0.1196158 -0.9549125 -0.4860272 1.710025e-09 #> attr(,\"class\") #> [1] \"summary.phreg_rct\" dtable(data,~strat+arms) #>  #>       arms   0   1 #> strat              #> 1          223 213 #> 2           96 106 #> 3          213 203 dfactor(data) <- strat.f~strat out <- phreg_rct(Surv(days,cens)~arms.f,data=data,augmentR0=~strat.f) summary(out) #>                    Estimate   Std.Err       2.5%      97.5%      P-value #> Marginal-arms.f1 -0.7036460 0.1224406 -0.9436251 -0.4636669 9.092786e-09 #> R0_none:arms.f1  -0.7009844 0.1217138 -0.9395390 -0.4624298 8.447051e-09 #> attr(,\"class\") #> [1] \"summary.phreg_rct\""},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"two-stage-randomization-calgb-9823-for-survival-outcomes","dir":"Articles","previous_headings":"","what":"Two-Stage Randomization CALGB-9823 for survival outcomes","title":"Randomization for Cox Type rate models ","text":"illustrate analysis one SMART conducted Cancer Leukemia Group B Protocol 8923 (CALGB 8923), Stone others (2001). 388 patients randomized initial treatment GM-CSF (A1 ) standard chemotherapy (A2 ). Patients complete remission informed consent second stage re-randomized cytarabine (B1 ) cytarabine plus mitoxantrone (B2 ). first compute weighted risk-set estimator based estimated weights ΛA1,B1(t)=∑∫0twi(s)Yw(s)dNi(s)\\begin{align*} \\Lambda_{A1,B1}(t) & = \\sum_i \\int_0^t \\frac{w_i(s)}{Y^w(s)} dN_i(s) \\end{align*} wi(s)=(A0i=A1)+(t>TR)(A1i=B1)/π1(Xi)w_i(s) = (A0_i=A1) + (t>T_R) (A1_i=B1)/\\pi_1(X_i), 1 start treatment A1A1 changes B1B1 time TRT_R scaled proportion . equivalent IPTW (inverse probability treatment weighted estimator). estimate treatment regimes A1,B1A1, B1 A2,B1A2, B1 letting A10A10 indicate consistent ending B1B1. A10A10 starts 11 becomes 00 subject treated B2B2, stays 11 subject treated B1B1. can look two strata A0=0,A10=1A0=0,A10=1 A0=1,A10=1A0=1,A10=1. Similary, end consistent B2B2. Thus defining A11A11 start 11, stays 11 B2B2 taken, becomes 00 second randomization B1B1. treatment models time-points, unless weight.var variable given (1 treatments, 0 otherwise) accomodate general start,stop format treatment model may also depend response value standard errors based influence functions also computed baseline use propensity score model P(A1=B1|A0)P(A1=B1|A0) uses observed frequencies arm B1B1 among starting either A1A1 A2A2.  propensity score mode can extended use covariates get increased efficiency. Note also propensity scores A0A0 cancel different strata. now illustrate fit Cox model form λA0(t)exp(B1(t)β1+B2(t)β2)\\begin{align*}   & \\lambda_{A0}(t) \\exp( B1(t) \\beta_1 + B2(t) \\beta_2) \\end{align*} β0\\beta_0 effect treatment A2A2 effect B1B1 Now comparing starting A1/A2 compare effect B1 versus B2 structured model A0 A1, seem reasonable based ,","code":"data(calgb8923) calgt <- calgb8923  ## tm <- At.f~factor(Count2)+age+sex+wbc ## tm <- At.f~factor(Count2) tm <-  At.f~factor(Count2)*A0.f  head(calgt) #>   id V X Z   TR R     U delta  stop age   wbc sex race      time status start #> 1  1 0 0 0 0.00 0 13.33     1 13.33  64 128.0   1    1 13.338219      1  0.00 #> 2  2 1 1 0 0.00 0 17.80     1 17.80  71   4.3   2    1 17.802995      1  0.00 #> 3  3 1 0 0 0.00 0  1.27     1  1.27  71  43.6   2    1  1.271527      1  0.00 #> 4  4 1 0 1 0.00 0 24.77     1 24.77  63  72.3   2    1  0.730000      2  0.00 #> 5  4 1 0 1 0.73 1 24.77     1 24.77  63  72.3   2    1 24.772515      1  0.73 #> 6  5 0 1 0 0.00 0 10.37     1 10.37  65   1.4   1    1 10.374479      1  0.00 #>   A0.f A0 A1 A11 A12 A1.f A10 At.f lbnr__id Count1 Count2 consent trt2 trt1 #> 1    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 2    1  1  0   1   0    0   0    1        1      0      0      -1   -1    2 #> 3    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 4    0  0  0   1   0    0   0    0        1      0      0      -1   -1    1 #> 5    0  0  1   1   1    1   1    1        2      0      1       1    1    1 #> 6    1  1  0   1   0    0   0    1        1      0      0      -1   -1    2 ll0 <- phreg_IPTW(Event(start,time,status==1)~strata(A0,A10)+cluster(id),calgt,treat.model=tm) pll0 <- predict(ll0,expand.grid(A0=0:1,A10=0,id=1)) ll1 <- phreg_IPTW(Event(start,time,status==1)~strata(A0,A11)+cluster(id),calgt,treat.model=tm) pll1 <- predict(ll1,expand.grid(A0=0:1,A11=1,id=1)) plot(pll0,se=1,lwd=2,col=1:2,lty=1,xlab=\"time (months)\",xlim=c(0,30)) plot(pll1,add=TRUE,col=3:4,se=1,lwd=2,lty=1,xlim=c(0,30)) abline(h=0.25) legend(\"topright\",c(\"A1B1\",\"A2B1\",\"A1B2\",\"A2B2\"),col=c(1,2,3,4),lty=1) summary(pll1,times=1:10) #> $pred #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.8022569 0.7119656 0.6675967 0.6471848 0.6369788 0.6164021 0.5770105 #> [2,] 0.8568499 0.7871414 0.7456444 0.7133504 0.6878999 0.6623719 0.6400970 #>           [,8]      [,9]     [,10] #> [1,] 0.5427705 0.5154506 0.5103024 #> [2,] 0.6109335 0.5646244 0.5543596 #>  #> $se.pred #>            [,1]       [,2]       [,3]       [,4]       [,5]       [,6] #> [1,] 0.02861524 0.03101141 0.03283701 0.03265187 0.03255301 0.03229413 #> [2,] 0.02491113 0.02819870 0.02918381 0.03134551 0.03175905 0.03205534 #>            [,7]       [,8]       [,9]      [,10] #> [1,] 0.03387985 0.03486639 0.03785952 0.03806234 #> [2,] 0.03345603 0.03601502 0.03946837 0.03959668 #>  #> $lower #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.7480876 0.6537066 0.6062423 0.5862506 0.5762674 0.5562481 0.5142857 #> [2,] 0.8093900 0.7337687 0.6905840 0.6544855 0.6283866 0.6024322 0.5777713 #>           [,8]      [,9]     [,10] #> [1,] 0.4785606 0.4463411 0.4408982 #> [2,] 0.5442707 0.4923330 0.4819390 #>  #> $upper #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.8603486 0.7754168 0.7351604 0.7144523 0.7040864 0.6830613 0.6473856 #> [2,] 0.9070927 0.8443964 0.8050947 0.7775096 0.7530497 0.7282753 0.7091460 #>           [,8]      [,9]     [,10] #> [1,] 0.6155957 0.5952608 0.5906319 #> [2,] 0.6857613 0.6475306 0.6376627 #>  #> $times #>  [1]  1  2  3  4  5  6  7  8  9 10 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\" summary(pll0,times=1:10) #> $pred #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.8017327 0.7008265 0.6523029 0.6158134 0.5923768 0.5659830 0.5329907 #> [2,] 0.8560743 0.7740713 0.7153512 0.6690102 0.6272139 0.5642496 0.5412531 #>           [,8]      [,9]     [,10] #> [1,] 0.4856035 0.4751084 0.4580125 #> [2,] 0.5244200 0.5014231 0.4784263 #>  #> $se.pred #>            [,1]       [,2]       [,3]       [,4]       [,5]       [,6] #> [1,] 0.02874374 0.03363964 0.03593932 0.03745772 0.03849765 0.03905568 #> [2,] 0.02508408 0.03053222 0.03382459 0.03662354 0.03831324 0.04083119 #>            [,7]       [,8]       [,9]      [,10] #> [1,] 0.03953451 0.04080406 0.04110910 0.04157512 #> [2,] 0.04145722 0.04203299 0.04238077 0.04269490 #>  #> $lower #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.7473298 0.6379004 0.5855331 0.5466050 0.5215305 0.4943860 0.4608737 #> [2,] 0.8082955 0.7164839 0.6520354 0.6009461 0.5564424 0.4896381 0.4658035 #>           [,8]      [,9]     [,10] #> [1,] 0.4118674 0.4009976 0.3833640 #> [2,] 0.4481818 0.4248738 0.4016554 #>  #> $upper #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7] #> [1,] 0.8600959 0.7699599 0.7266867 0.6937847 0.6728470 0.6479486 0.6163926 #> [2,] 0.9066774 0.8362873 0.7848152 0.7447834 0.7069865 0.6502305 0.6289239 #>           [,8]      [,9]     [,10] #> [1,] 0.5725404 0.5629159 0.5471966 #> [2,] 0.6136267 0.5917643 0.5698710 #>  #> $times #>  [1]  1  2  3  4  5  6  7  8  9 10 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\" library(mets) data(calgb8923) calgt <- calgb8923 calgt$treatvar <- 1  ## making time-dependent indicators of going to B1/B2 calgt$A10t <- calgt$A11t <- 0 calgt <- dtransform(calgt,A10t=1,A1==0 & Count2==1) calgt <- dtransform(calgt,A11t=1,A1==1 & Count2==1) calgt0 <- subset(calgt,A0==0)  ss0 <- phreg_rct(Event(start,time,status)~A10t+A11t+cluster(id),data=subset(calgt,A0==0),      typesR=c(\"non\",\"R1\"),typesC=c(\"non\",\"dynC\"),      treat.var=\"treatvar\",treat.model=At.f~factor(Count2),      augmentR1=~age+wbc+sex+TR,augmentC=~age+wbc+sex+TR+Count2) summary(ss0) #>                Estimate   Std.Err      2.5%      97.5%      P-value #> Marginal-A10t -1.570250 0.2433389 -2.047185 -1.0933143 1.097054e-10 #> Marginal-A11t -1.407287 0.2193924 -1.837289 -0.9772861 1.413090e-10 #> non_dynC:A10t -1.583146 0.2418997 -2.057260 -1.1090311 5.963963e-11 #> non_dynC:A11t -1.406682 0.2190539 -1.836020 -0.9773442 1.348264e-10 #> R1_non:A10t   -1.544312 0.2396152 -2.013949 -1.0746751 1.156250e-10 #> R1_non:A11t   -1.423064 0.2087465 -1.832199 -1.0139282 9.284039e-12 #> R1_dynC:A10t  -1.557021 0.2381534 -2.023793 -1.0902486 6.239330e-11 #> R1_dynC:A11t  -1.422360 0.2083906 -1.830798 -1.0139218 8.764919e-12 #> attr(,\"class\") #> [1] \"summary.phreg_rct\"  ss1 <- phreg_rct(Event(start,time,status)~A10t+A11t+cluster(id),data=subset(calgt,A0==1),      typesR=c(\"non\",\"R1\"),typesC=c(\"non\",\"dynC\"),      treat.var=\"treatvar\",treat.model=At.f~factor(Count2),      augmentR1=~age+wbc+sex+TR,augmentC=~age+wbc+sex+TR+Count2) summary(ss1) #>                 Estimate   Std.Err      2.5%      97.5%      P-value #> Marginal-A10t -0.8968608 0.2312067 -1.350018 -0.4437039 1.048683e-04 #> Marginal-A11t -0.9754528 0.2215523 -1.409687 -0.5412181 1.068580e-05 #> non_dynC:A10t -0.8312901 0.2263294 -1.274888 -0.3876925 2.397942e-04 #> non_dynC:A11t -1.0165973 0.2211108 -1.449967 -0.5832280 4.272177e-06 #> R1_non:A10t   -0.9310307 0.2299136 -1.381653 -0.4804083 5.133147e-05 #> R1_non:A11t   -0.9361199 0.2204289 -1.368153 -0.5040872 2.168342e-05 #> R1_dynC:A10t  -0.8634407 0.2250083 -1.304449 -0.4224326 1.243576e-04 #> R1_dynC:A11t  -0.9753885 0.2199851 -1.406551 -0.5442256 9.255029e-06 #> attr(,\"class\") #> [1] \"summary.phreg_rct\" ssf <- phreg_rct(Event(start,time,status)~A0.f+A10t+A11t+cluster(id),      data=calgt,      typesR=c(\"non\",\"R0\",\"R1\",\"R01\"),typesC=c(\"non\",\"C\",\"dynC\"),      treat.var=\"treatvar\",treat.model=At.f~factor(Count2),      augmentR0=~age+wbc+sex,augmentR1=~age+wbc+sex+TR,augmentC=~age+wbc+sex+TR+Count2)"},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"recurrent-events-simple-randomization","dir":"Articles","previous_headings":"","what":"Recurrent events: Simple Randomization","title":"Randomization for Cox Type rate models ","text":"Recurrents events simulation death censoring. Now fit model Censoring model stratified Z.f treatment probabilities estimated using data","code":"n <- 1000 beta <- 0.15;   data(CPH_HPN_CRBSI)  dr <- CPH_HPN_CRBSI$terminal  base1 <- CPH_HPN_CRBSI$crbsi   base4 <- scalecumhaz(CPH_HPN_CRBSI$mechanical,0.5)  cens <- rbind(c(0,0),c(2000,0.5),c(5110,3)) ce <- 3; betao1 <- 0  varz <- 1; dep=4; X <- z <- rgamma(n,1/varz)*varz Z0 <- NULL px <- 0.5 if (betao1!=0) px <- lava::expit(betao1*X)       A0 <- rbinom(n,1,px) r1 <- exp(A0*beta[1]) rd <- exp( A0 * 0.15) rc <- exp( A0 * 0 ) ### rr <-    mets:::simLUCox(n,base1,death.cumhaz=dr,r1=r1,Z0=X,dependence=dep,var.z=varz,cens=ce/5000) rr$A0 <- A0[rr$id] rr$z1 <- attr(rr,\"z\")[rr$id] rr$lz1 <- log(rr$z1) rr$X <- rr$lz1  rr$lX <- rr$z1 rr$statusD <- rr$status rr <- dtransform(rr,statusD=2,death==1) rr <- count.history(rr) rr$Z <- rr$A0 data <- rr data$Z.f <- as.factor(data$Z) data$treattime <- 0 data <- dtransform(data,treattime=1,lbnr__id==1) dlist(data,start+stop+statusD+A0+z1+treattime+Count1~id|id %in% c(4,5)) #> id: 4 #>      start   stop    statusD A0 z1    treattime Count1 #> 4      0.000   9.565 1       0  0.471 1         0      #> 1003   9.565 372.057 1       0  0.471 0         1      #> 1468 372.057 389.831 0       0  0.471 0         2      #> ------------------------------------------------------------  #> id: 5 #>   start stop  statusD A0 z1    treattime Count1 #> 5 0     213.9 2       1  2.338 1         0 fit2 <- phreg_rct(Event(start,stop,statusD)~Z.f+cluster(id),data=data,      treat.var=\"treattime\",typesR=c(\"non\",\"R0\"),typesC=c(\"non\",\"C\",\"dynC\"),      augmentR0=~z1,augmentC=~z1+Count1) summary(fit2) #>                Estimate    Std.Err       2.5%     97.5%      P-value #> Marginal-Z.f1 0.2870649 0.09632565 0.09827011 0.4758597 0.0028810700 #> non_C:Z.f1    0.2826049 0.09631924 0.09382262 0.4713871 0.0033457707 #> non_dynC:Z.f1 0.1926888 0.08864883 0.01894025 0.3664373 0.0297337758 #> R0_non:Z.f1   0.3110880 0.08049844 0.15331399 0.4688621 0.0001113067 #> R0_C:Z.f1     0.3066141 0.08049078 0.14885504 0.4643731 0.0001393570 #> R0_dynC:Z.f1  0.2164684 0.07113356 0.07704922 0.3558877 0.0023413376 #> attr(,\"class\") #> [1] \"summary.phreg_rct\""},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"twostage-randomization-recurrent-events","dir":"Articles","previous_headings":"","what":"Twostage Randomization: Recurrent events","title":"Randomization for Cox Type rate models ","text":"Now fitting model computing different augmentations (true values 0.3 0.3) treat.model A0 A1 coded .f allow model depends randomization adaptive possible. observational case one can also adjust covarites. Censoring model stratified A0.f","code":"n <- 500 beta=c(0.3,0.3);betatr=0.3;betac=0;betao=0;betao1=0;ce=3;fixed=1;sim=1;dep=4;varz=1;ztr=0; ce <- 3 ## take possible frailty  Z0 <- rgamma(n,1/varz)*varz px0 <- 0.5; if (betao!=0) px0 <- expit(betao*Z0) A0 <- rbinom(n,1,px0) r1 <- exp(A0*beta[1]) # px1 <- 0.5; if (betao1!=0) px1 <- expit(betao1*Z0) A1 <- rbinom(n,1,px1) r2 <- exp(A1*beta[2]) rtr <- exp(A0*betatr[1]) rr <-  mets:::simLUCox(n,base1,death.cumhaz=dr,cumhaz2=base1,rtr=rtr,betatr=0.3,A0=A0,Z0=Z0,         r1=r1,r2=r2,dependence=dep,var.z=varz,cens=ce/5000,ztr=ztr) rr$z1 <- attr(rr,\"z\")[rr$id] rr$A1 <- A1[rr$id] rr$A0 <- A0[rr$id] rr$lz1 <- log(rr$z1) rr <- count.history(rr,types=1:2) rr$A1t <- 0 rr <- dtransform(rr,A1t=A1,Count2==1)  rr$At.f <- rr$A0 rr$A0.f <- factor(rr$A0) rr$A1.f <- factor(rr$A1) rr <- dtransform(rr, At.f = A1, Count2 == 1) rr$At.f <- factor(rr$At.f) dfactor(rr)  <-  A0.f~A0 rr$treattime <- 0 rr <- dtransform(rr,treattime=1,lbnr__id==1) rr$lagCount2 <- dlag(rr$Count2) rr <- dtransform(rr,treattime=1,Count2==1 & (Count2!=lagCount2)) dlist(rr,start+stop+statusD+A0+A1+A1t+At.f+Count2+z1+treattime+Count1~id|id %in% c(5,10)) #> id: 5 #>   start stop  statusD A0 A1 A1t At.f Count2 z1     treattime Count1 #> 5 0     132.3 3       1  1  0   1    0      0.2316 1         0      #> ------------------------------------------------------------  #> id: 10 #>     start stop    statusD A0 A1 A1t At.f Count2 z1      treattime Count1 #> 10   0.00   33.12 2       1  0  0   1    0      0.06891 1         0      #> 509 33.12 1363.53 0       1  0  0   0    1      0.06891 1         0 sse <- phreg_rct(Event(start,time,statusD)~A0.f+A1t+cluster(id),data=rr,      typesR=c(\"non\",\"R0\",\"R1\",\"R01\"),typesC=c(\"non\",\"C\",\"dynC\"),treat.var=\"treattime\",      treat.model=At.f~factor(Count2),      augmentR0=~z1,augmentR1=~z1,augmentC=~z1+Count1+A1t) summary(sse) #>                 Estimate   Std.Err        2.5%     97.5%      P-value #> Marginal-A0.f1 0.3179631 0.1418023  0.04003574 0.5958904 0.0249420566 #> Marginal-A1t   0.3290147 0.1472363  0.04043683 0.6175925 0.0254434247 #> non_C:A0.f1    0.3002782 0.1391490  0.02755115 0.5730053 0.0309308283 #> non_C:A1t      0.4151190 0.1405664  0.13961382 0.6906241 0.0031451130 #> non_dynC:A0.f1 0.3104992 0.1314476  0.05286660 0.5681318 0.0181692114 #> non_dynC:A1t   0.4374223 0.1300991  0.18243265 0.6924119 0.0007731772 #> R0_non:A0.f1   0.4142867 0.1176773  0.18364338 0.6449300 0.0004306830 #> R0_non:A1t     0.3382185 0.1470378  0.05002979 0.6264072 0.0214360247 #> R0_C:A0.f1     0.3962505 0.1144662  0.17190089 0.6206002 0.0005367253 #> R0_C:A1t       0.4242165 0.1403584  0.14911904 0.6993140 0.0025079567 #> R0_dynC:A0.f1  0.4066624 0.1049692  0.20092652 0.6123983 0.0001070147 #> R0_dynC:A1t    0.4464941 0.1298744  0.19194497 0.7010432 0.0005862621 #> R1_non:A0.f1   0.3269008 0.1416813  0.04921043 0.6045911 0.0210383383 #> R1_non:A1t     0.2104421 0.1254571 -0.03544923 0.4563335 0.0934636201 #> R1_C:A0.f1     0.3092275 0.1390258  0.03674199 0.5817130 0.0261319083 #> R1_C:A1t       0.2976811 0.1175580  0.06727171 0.5280905 0.0113347106 #> R1_dynC:A0.f1  0.3194771 0.1313171  0.06210024 0.5768540 0.0149798139 #> R1_dynC:A1t    0.3202318 0.1048176  0.11479297 0.5256706 0.0022496121 #> R01_non:A0.f1  0.4092668 0.1176428  0.17869115 0.6398424 0.0005034879 #> R01_non:A1t    0.2280764 0.1243165 -0.01557936 0.4717322 0.0665584775 #> R01_C:A0.f1    0.3912859 0.1144307  0.16700576 0.6155659 0.0006275647 #> R01_C:A1t      0.3150865 0.1163399  0.08706445 0.5431086 0.0067623432 #> R01_dynC:A0.f1 0.4016977 0.1049305  0.19603760 0.6073577 0.0001290708 #> R01_dynC:A1t   0.3375831 0.1034497  0.13482542 0.5403408 0.0011013908 #> attr(,\"class\") #> [1] \"summary.phreg_rct\""},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"causal-assumptions-for-twostage-randomization-recurrent-events","dir":"Articles","previous_headings":"","what":"Causal assumptions for Twostage Randomization: Recurrent events","title":"Randomization for Cox Type rate models ","text":"take interest N1N_1 also death NdN_d. Now need given X0X_0 A0⊥N10(),N11(),Nd0(),Nd1()|X0A_0 \\perp  N_1^0(), N_1^1(), N_d^0(), N_d^1() | X_0 positivity 1>π0(X0)>01 > \\pi_0(X_0) > 0 given X‾1\\bar X_1 history accumulated time TRT_R 2nd randomization A1⊥N10(),N11(),Nd0(),Nd1()|X‾1A_1 \\perp  N_1^0(), N_1^1(), N_d^0(), N_d^1() | \\bar X_1 positivity 1>π1(X1)>01 > \\pi_1(X_1) > 0 consistency link counterfactual quantities observed data. must use IPTW weighted Cox score augment addition need censoring independent given example A0A_0 Independent censoring use phreg_rct situation RCT=FALSE propensity score models must specified model used propensity scores augmentation models augmentation models needed due adaptive nature fitting propensity score models. twostage randomization","code":"fit2 <- phreg_rct(Event(start,stop,statusD)~Z.f+cluster(id),data=data,      typesR=c(\"non\",\"R0\"),typesC=c(\"non\",\"C\",\"dynC\"),          RCT=FALSE,treat.model=Z.f~z1,augmentR0=~z1,augmentC=~z1+Count1,      treat.var=\"treattime\") summary(fit2) #>                Estimate    Std.Err       2.5%     97.5%      P-value #> Marginal-Z.f1 0.3111195 0.08058494 0.15317593 0.4690631 0.0001130326 #> non_C:Z.f1    0.3067348 0.08057748 0.14880581 0.4646637 0.0001408301 #> non_dynC:Z.f1 0.2169383 0.07119837 0.07739205 0.3564845 0.0023117164 #> R0_non:Z.f1   0.3111195 0.08058494 0.15317593 0.4690631 0.0001130326 #> R0_C:Z.f1     0.3067348 0.08057748 0.14880581 0.4646637 0.0001408301 #> R0_dynC:Z.f1  0.2169383 0.07119837 0.07739205 0.3564845 0.0023117164 #> attr(,\"class\") #> [1] \"summary.phreg_rct\" sse <- phreg_rct(Event(start,time,statusD)~A0.f+A1t+cluster(id),data=rr,      typesR=c(\"non\",\"R0\",\"R1\",\"R01\"),typesC=c(\"non\",\"C\",\"dynC\"),          treat.var=\"treattime\",      RCT=FALSE,treat.model=At.f~z1*factor(Count2),          augmentR0=~z1,augmentR1=~z1,augmentC=~z1+Count1+A1t) summary(sse) #>                 Estimate    Std.Err        2.5%     97.5%      P-value #> Marginal-A0.f1 0.3817476 0.13188068  0.12326622 0.6402290 0.0037958891 #> Marginal-A1t   0.2259319 0.12344157 -0.01600910 0.4678730 0.0672089296 #> non_C:A0.f1    0.3765255 0.12594711  0.12967369 0.6233773 0.0027938653 #> non_C:A1t      0.3187675 0.11256529  0.09814361 0.5393914 0.0046280182 #> non_dynC:A0.f1 0.3797091 0.11214290  0.15991306 0.5995051 0.0007093493 #> non_dynC:A1t   0.3514300 0.09297578  0.16920079 0.5336591 0.0001569535 #> R0_non:A0.f1   0.3817476 0.13188068  0.12326622 0.6402290 0.0037958891 #> R0_non:A1t     0.2259319 0.12344157 -0.01600910 0.4678730 0.0672089296 #> R0_C:A0.f1     0.3765255 0.12594711  0.12967369 0.6233773 0.0027938653 #> R0_C:A1t       0.3187675 0.11256529  0.09814361 0.5393914 0.0046280182 #> R0_dynC:A0.f1  0.3797091 0.11214290  0.15991306 0.5995051 0.0007093493 #> R0_dynC:A1t    0.3514300 0.09297578  0.16920079 0.5336591 0.0001569535 #> R1_non:A0.f1   0.3817476 0.13188068  0.12326622 0.6402290 0.0037958891 #> R1_non:A1t     0.2259319 0.12344157 -0.01600910 0.4678730 0.0672089296 #> R1_C:A0.f1     0.3765255 0.12594711  0.12967369 0.6233773 0.0027938653 #> R1_C:A1t       0.3187675 0.11256529  0.09814361 0.5393914 0.0046280182 #> R1_dynC:A0.f1  0.3797091 0.11214290  0.15991306 0.5995051 0.0007093493 #> R1_dynC:A1t    0.3514300 0.09297578  0.16920080 0.5336591 0.0001569535 #> R01_non:A0.f1  0.3817476 0.13185641  0.12331378 0.6401814 0.0037894525 #> R01_non:A1t    0.2259319 0.12307282 -0.01528637 0.4671502 0.0663934395 #> R01_C:A0.f1    0.3765255 0.12592170  0.12972349 0.6233275 0.0027883528 #> R01_C:A1t      0.3187675 0.11216079  0.09893641 0.5385986 0.0044823275 #> R01_dynC:A0.f1 0.3797091 0.11211436  0.15996900 0.5994492 0.0007071245 #> R01_dynC:A1t   0.3514300 0.09248564  0.17016144 0.5326985 0.0001447938 #> attr(,\"class\") #> [1] \"summary.phreg_rct\""},{"path":"http://kkholst.github.io/mets/articles/phreg_rct.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Randomization for Cox Type rate models ","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/quantitative-twin.html","id":"twin-analysis-continuous-traits","dir":"Articles","previous_headings":"","what":"Twin analysis, continuous traits","title":"Twin models","text":"following examine heritability Body Mass Indexkorkeila_bmi_1991hjelmborg_bmi_2008, based data self-reported BMI-values random sample 11,411 -sex twins. First, load data data long format one subject per row. tvparnr: twin id bmi: Body Mass Index (kg/m2\\mathrm{kg}/{\\mathrm{m}^2}) age: Age (years) gender: Gender factor (male,female) zyg: zygosity (MZ, DZ) transpose data allowing us pairwise analyses Next plot association within zygosity group show log-transformed data slightly symmetric appropiate twin analysis (see Figure @ref(fig:scatter1) @ref(fig:scatter2)) Scatter plot logarithmic BMI measurements MZ twins Scatter plot logarithmic BMI measurements DZ twins plots raw association measures shows considerable stronger dependence MZ twins, thus indicating genetic influence trait Ńext examine marginal distribution (GEE model working independence) Marginal association BMI Age males females.","code":"library(mets) data(\"twinbmi\") head(twinbmi) #>   tvparnr      bmi      age gender zyg id num #> 1       1 26.33289 57.51212   male  DZ  1   1 #> 2       1 25.46939 57.51212   male  DZ  1   2 #> 3       2 28.65014 56.62696   male  MZ  2   1 #> 5       3 28.40909 57.73097   male  DZ  3   1 #> 7       4 27.25089 53.68683   male  DZ  4   1 #> 8       4 28.07504 53.68683   male  DZ  4   2 twinwide <- fast.reshape(twinbmi, id=\"tvparnr\",varying=c(\"bmi\")) head(twinwide) #>    tvparnr     bmi1      age gender zyg id num     bmi2 #> 1        1 26.33289 57.51212   male  DZ  1   1 25.46939 #> 3        2 28.65014 56.62696   male  MZ  2   1       NA #> 5        3 28.40909 57.73097   male  DZ  3   1       NA #> 7        4 27.25089 53.68683   male  DZ  4   1 28.07504 #> 9        5 27.77778 52.55838   male  DZ  5   1       NA #> 11       6 28.04282 52.52231   male  DZ  6   1 22.30936 mz <- log(subset(twinwide, zyg==\"MZ\")[,c(\"bmi1\",\"bmi2\")]) plot_twin(mz) dz <- log(subset(twinwide, zyg==\"DZ\")[,c(\"bmi1\",\"bmi2\")]) plot_twin(dz) cor.test(mz[,1],mz[,2], method=\"spearman\") #>  #>  Spearman's rank correlation rho #>  #> data:  mz[, 1] and mz[, 2] #> S = 165457624, p-value < 2.2e-16 #> alternative hypothesis: true rho is not equal to 0 #> sample estimates: #>       rho  #> 0.6956209 cor.test(dz[,1],dz[,2], method=\"spearman\") #>  #>  Spearman's rank correlation rho #>  #> data:  dz[, 1] and dz[, 2] #> S = 2162514570, p-value < 2.2e-16 #> alternative hypothesis: true rho is not equal to 0 #> sample estimates: #>       rho  #> 0.4012686 l0 <- lm(bmi ~ gender + I(age-40), data=twinbmi) estimate(l0, id=twinbmi$tvparnr) #>             Estimate  Std.Err    2.5%   97.5%    P-value #> (Intercept)  23.3687 0.054534 23.2618 23.4756  0.000e+00 #> gendermale    1.4077 0.073216  1.2642  1.5512  2.230e-82 #> I(age - 40)   0.1177 0.004787  0.1083  0.1271 1.499e-133 library(\"splines\") l1 <- lm(bmi ~ gender*ns(age,3), data=twinbmi) marg1 <- estimate(l1, id=twinbmi$tvparnr) dm <- lava::Expand(twinbmi,         bmi=0,         gender=c(\"male\"),         age=seq(33,61,length.out=50)) df <- lava::Expand(twinbmi,         bmi=0,         gender=c(\"female\"),         age=seq(33,61,length.out=50))  plot(marg1, function(p) model.matrix(l1,data=dm)%*%p,      data=dm[\"age\"], ylab=\"BMI\", xlab=\"Age\",      ylim=c(22,26.5)) plot(marg1, function(p) model.matrix(l1,data=df)%*%p,      data=df[\"age\"], col=\"red\", add=TRUE) legend(\"bottomright\", c(\"Male\",\"Female\"),        col=c(\"black\",\"red\"), lty=1, bty=\"n\")"},{"path":"http://kkholst.github.io/mets/articles/quantitative-twin.html","id":"polygenic-model","dir":"Articles","previous_headings":"Twin analysis, continuous traits","what":"Polygenic model","title":"Twin models","text":"can decompose trait following variance components Yi=Ai+Di+C+Ei,=1,2\\begin{align*} Y_i = A_i + D_i + C + E_i, \\quad =1,2  \\end{align*} AA: Additive genetic effects alleles DD: Dominante genetic effects alleles CC: Shared environmental effects EE: Unique environmental genetic effects Dissimilarity MZ twins arises unshared environmental effects , $\\cor(E_1,E_2)=0$ $$\\begin{align*} \\cor(A_1^{MZ},A_2^{MZ}) = 1, \\quad \\cor(D_1^{MZ},D_2^{MZ}) = 1, \\end{align*}$$ $$\\begin{align*} \\cor(A_1^{DZ},A_2^{DZ}) = 0.5, \\quad \\cor(D_1^{DZ},D_2^{DZ}) = 0.25, \\end{align*}$$ Yi=Ai+Ci+Di+Ei\\begin{align*} Y_i = A_i + C_i + D_i + E_i \\end{align*} Ai∼𝒩(0,σA2),Ci∼𝒩(0,σC2),Di∼𝒩(0,σD2),Ei∼𝒩(0,σE2)\\begin{align*} A_i \\sim\\mathcal{N}(0,\\sigma_A^2), C_i \\sim\\mathcal{N}(0,\\sigma_C^2), D_i \\sim\\mathcal{N}(0,\\sigma_D^2), E_i \\sim\\mathcal{N}(0,\\sigma_E^2) \\end{align*} $$\\begin{gather*}     \\cov(Y_{1},Y_{2}) = \\\\     \\begin{pmatrix}       \\sigma_A^2 & 2\\Phi\\sigma_A^2 \\\\       2\\Phi\\sigma_A^2 & \\sigma_A^2     \\end{pmatrix} +     \\begin{pmatrix}       \\sigma_C^2 & \\sigma_C^2 \\\\       \\sigma_C^2 & \\sigma_C^2   \\end{pmatrix} +     \\begin{pmatrix}       \\sigma_D^2 & \\Delta_{7}\\sigma_D^2 \\\\       \\Delta_{7}\\sigma_D^2 & \\sigma_D^2   \\end{pmatrix} +   \\begin{pmatrix}     \\sigma_E^2 & 0 \\\\     0 & \\sigma_E^2   \\end{pmatrix} \\end{gather*}$$ Saturated model (different marginals MZ DZ twins different marginals twin 1 twin 2): Different marginals MZ DZ twins (marginals within pair) marginals free correlation MZ, DZ formal test genetic effects can obtained comparing MZ DZ correlation: also consider ACE model","code":"dd <- na.omit(twinbmi) l0 <- twinlm(bmi ~ age+gender, data=dd, DZ=\"DZ\", zyg=\"zyg\", id=\"tvparnr\", type=\"sat\") lf <- twinlm(bmi ~ age+gender, data=dd,DZ=\"DZ\", zyg=\"zyg\", id=\"tvparnr\", type=\"flex\") lu <- twinlm(bmi ~ age+gender, data=dd, DZ=\"DZ\", zyg=\"zyg\", id=\"tvparnr\", type=\"eqmarg\") estimate(lu) #>                      Estimate  Std.Err    2.5%   97.5%    P-value #> bmi.1@1               18.6037 0.251036 18.1116 19.0957  0.000e+00 #> bmi.1~age.1@1          0.1189 0.005635  0.1078  0.1299  9.177e-99 #> bmi.1~gendermale.1@1   1.3848 0.086573  1.2151  1.5544  1.376e-57 #> log(var)@1             2.4424 0.022095  2.3991  2.4857  0.000e+00 #> atanh(rhoMZ)@1         0.7803 0.036249  0.7092  0.8513 9.008e-103 #> atanh(rhoDZ)@2         0.2987 0.020953  0.2576  0.3397  4.288e-46 estimate(lu,lava::contr(5:6,6)) #>                           Estimate Std.Err   2.5%  97.5%   P-value #> [atanh(rhoMZ)@1] - [a....   0.4816 0.04177 0.3997 0.5635 9.431e-31 #>  #>  Null Hypothesis:  #>   [atanh(rhoMZ)@1] - [atanh(rhoDZ)@2] = 0 ace0 <- twinlm(bmi ~ age+gender, data=dd, DZ=\"DZ\", zyg=\"zyg\", id=\"tvparnr\", type=\"ace\") summary(ace0) #>                  Estimate Std. Error Z value Pr(>|z|) #> bmi            1.8599e+01 2.5576e-01  72.720   <2e-16 #> sd(A)          2.7270e+00 4.2658e-02  63.927   <2e-16 #> sd(C)          1.7129e-06 3.1064e-01   0.000        1 #> sd(E)          2.0276e+00 3.4787e-02  58.286   <2e-16 #> bmi~age        1.1892e-01 5.6246e-03  21.142   <2e-16 #> bmi~gendermale 1.3846e+00 8.8748e-02  15.601   <2e-16 #>  #> MZ-pairs DZ-pairs  #>     1483     2788  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.64399  0.61793 0.67005 #> C 0.00000  0.00000 0.00000 #> E 0.35601  0.32995 0.38207 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.64399  0.61793 0.67005 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.64399  0.61718 0.66931 #> Correlation within DZ: 0.32200  0.30890 0.33497 #>  #> 'log Lik.' -22019.66 (df=6) #> AIC: 44051.32  #> BIC: 44089.47"},{"path":"http://kkholst.github.io/mets/articles/quantitative-twin.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Twin models","text":"[korkeila_bmi_1991] Korkeila, Kaprio, Rissanen & Koskenvuo, Effects gender age heritability body mass index, Int J Obes, 15(10), 647-654 (1991). ↩︎ [hjelmborg_bmi_2008] Hjelmborg, Fagnani, Silventoinen, McGue, Korkeila, Christensen, Rissanen & Kaprio, Genetic influences growth traits BMI: longitudinal study adult twins, Obesity (Silver Spring), 16(4), 847-852 (2008). ↩︎","code":""},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Recurrent events","text":"recurrent events data often interest compute basic descriptive quantities get basic understanding phenonmenon studied. demonstrate one can compute: efficient marginal mean estimation fast computation standard errors efficient regression augmentation Ghosh-Lin model clusters can specfied allows stratified baseline variance recurrent events process probability exceeding k events two-stage recurrent events random effects model also show improve efficiency recurrents events marginal mean. addition several tools can used simulating recurrent events bivariate recurrent events data, also possible terminating event: event types Cox type rates Cox type rates frailty extenstions frailty extenstions general illness death model Cox models hazards.","code":""},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"simulation-of-recurrents-events","dir":"Articles","previous_headings":"","what":"Simulation of recurrents events","title":"Recurrent events","text":"start simulating recurrent events data two type events cumulative hazards Λ1(t)\\Lambda_1(t) (rate among survivors) Λ2(t)\\Lambda_2(t) (rate among survivors) ΛD(t)\\Lambda_D(t) consider types 1 2 rate terminal event given ΛD(t)\\Lambda_D(t). let events independent, also specify random effects structure generate dependence. simulating data can impose various random-effects structures generate dependence Dependence=0: intensities can independent. Dependence=1: One gamma distributed random effects ZZ. intensities Zλ1(t)Z \\lambda_1(t) Zλ2(t)Z \\lambda_2(t) ZλD(t)Z \\lambda_D(t) Dependence=4: One gamma distributed random effects ZZ. intensities Zλ1(t)Z \\lambda_1(t) Zλ2(t)Z \\lambda_2(t) λD(t)\\lambda_D(t) Dependence=2: can draw normally distributed random effects Z1,Z2,ZdZ_1,Z_2,Z_d variance (var.z) correlation can specified (cor.mat). intensities exp(Z1)λ1(t)\\exp(Z_1) \\lambda_1(t) exp(Z2)λ2(t)\\exp(Z_2) \\lambda_2(t) exp(Z3)λD(t)\\exp(Z_3) \\lambda_D(t) Dependence=3: can draw gamma distributed random effects Z1,Z2,ZdZ_1,Z_2,Z_d sum-structure can speicifed via matrix cor.mat. compute Z̃j=∑kZkcor.mat(j,k)\\tilde Z_j = \\sum_k  Z_k^{cor.mat(j,k)} j=1,2,3j=1,2,3. intensities Z̃1λ1(t)\\tilde Z_1 \\lambda_1(t) Z̃2λ2(t)\\tilde Z_2 \\lambda_2(t) Z̃3λD(t)\\tilde Z_3 \\lambda_D(t) return run different set-ups later start simulating independent processes. key functions simple simulation one event type death extended version possibly multiple types recurrent events (rates can 0) Allows Cox types rates subject specific rates lists allowed multiple events cause death (competing risks) Allows Cox types rates subject specific rates sim.recurrent simulate Cox-Cox (marginals) Ghosh-Lin-Cox addition can simulate data Ghosh-Lin model marginals rates among survivors Cox form can simulate data Ghosh-Lin model (also simRecurrentCox) survival model terminal event Cox form frailties see examples specific models.","code":""},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"utility-functions","dir":"Articles","previous_headings":"","what":"Utility functions","title":"Recurrent events","text":"mention two utility functions tie.breaker breaking ties among jump-times expected functions . count.history counts number jumps previous subject N1(t−)N_1(t-) N2(t−)N_2(t-).","code":""},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"marginal-mean","dir":"Articles","previous_headings":"","what":"Marginal Mean","title":"Recurrent events","text":"start estimating marginal mean E(N1(t∧D))E(N_1(t \\wedge D)) DD timing terminal event. marginal mean average number events seen time tt. based two rate models type 1 events ∼E(dN1(t)|D>t)\\sim E(dN_1(t) | D > t) terminal event ∼E(dNd(t)|D>t)\\sim E(dN_d(t) | D > t) defined μ1(t)=E(N1(t))\\mu_1(t)=E(N_1(t))∫0tS(u)dR1(u)\\begin{align}    \\int_0^t S(u) d R_1(u)    \\end{align} S(t)=P(D≥t)S(t)=P(D \\geq t) dR1(t)=E(dN1(t)|D>t)dR_1(t) = E(dN_1(t) | D > t) can therefore estimated Kaplan-Meier estimator, Ŝ(u)\\hat S(u) Nelson-Aalen estimator R1(t)R_1(t) R̂1(t)=∑∫0t1Y•(s)dN1i(s)\\begin{align}   \\hat R_1(t) & =   \\sum_i \\int_0^t  \\frac{1}{Y_\\bullet (s)}  dN_{1i}(s) \\end{align} Y•(t)=∑iYi(t)Y_{\\bullet}(t)= \\sum_i Y_i(t) estimator μ̂1(t)=∫0tŜ(u)dR̂1(u),\\begin{align}   \\hat \\mu_1(t) & =    \\int_0^t \\hat S(u) d\\hat R_1(u), \\end{align} see Cook & Lawless (1997) Gosh & Lin (2000). variance can estimated based asymptotic expansion μ̂1(t)−μ1(t)\\hat \\mu_1(t) - \\mu_1(t)∑∫0tS(s)π(s)dMi1−μ1(t)∫0t1π(s)dMid+∫0tμ1(s)π(s)dMid,\\begin{align*}   & \\sum_i \\int_0^t \\frac{S(s)}{\\pi(s)} dM_{i1}  - \\mu_1(t) \\int_0^t  \\frac{1}{\\pi(s)} dM_i^d +  \\int_0^t \\frac{\\mu_1(s) }{\\pi(s)} dM_i^d, \\end{align*} mean-zero processes Mid(t)=NiD(t)−∫0tYi(s)dΛD(s)M_i^d(t) = N_i^D(t)- \\int_0^t Y_i(s) d \\Lambda^D(s), Mi1(t)=Ni1(t)−∫0tYi(s)dR1(s)M_{i1}(t) = N_{i1}(t) - \\int_0^t Y_{}(s) dR_1(s). described Gosh & Lin (2000)","code":""},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"generating-data","dir":"Articles","previous_headings":"","what":"Generating data","title":"Recurrent events","text":"start generating data illustrate computation marginal mean status variable keeps track recurrent evnts type, death timing death. compute marginal mean simly estimate two rates functions number events interest death using phreg function (start without covariates). estimates combined standard error computation recurrentMarginal function  can also extract estimate different time-points marginal mean can also estimated stratified case:  , adjust covariates two rates can still predictions marginal mean, can plotted baseline marginal mean, covariates equal 0 models. Predictions specific covariates can also obtained recmarg (recurren marginal mean used solely predictions without standard error computation).  simulate multiple recurrent events processes two causes death causes exponential censoring rate 3/50003/5000, processes assumed independent (dependence=0)","code":"library(mets) set.seed(1000) # to control output in simulatins for p-values below. data(CPH_HPN_CRBSI)  dr <- CPH_HPN_CRBSI$terminal  base1 <- CPH_HPN_CRBSI$crbsi   base4 <- CPH_HPN_CRBSI$mechanical  rr <- simRecurrent(200,base1,death.cumhaz=dr) rr$x <- rnorm(nrow(rr))  rr$strata <- floor((rr$id-0.01)/100) dlist(rr,.~id| id %in% c(1,7,9)) #> id: 1 #>   entry time  status dtime fdeath death start stop  x     strata #> 1 0     132.6 0      132.6 1      1     0     132.6 1.386 0      #> ------------------------------------------------------------  #> id: 7 #>   entry time  status dtime fdeath death start stop  x      strata #> 7 0     141.3 0      141.3 1      1     0     141.3 0.6628 0      #> ------------------------------------------------------------  #> id: 9 #>     entry time  status dtime fdeath death start stop  x       strata #> 9     0.0 433.5 1      558.7 1      0       0.0 433.5 -0.7528 0      #> 204 433.5 558.7 0      558.7 1      1     433.5 558.7 -0.6565 0 #  to fit non-parametric models with just a baseline  xr <- phreg(Surv(entry,time,status)~cluster(id),data=rr) xdr <- phreg(Surv(entry,time,death)~cluster(id),data=rr) par(mfrow=c(1,3)) plot(xdr,se=TRUE) title(main=\"death\") plot(xr,se=TRUE) # robust standard errors  rxr <-   robust.phreg(xr,fixbeta=1) plot(rxr,se=TRUE,robust=TRUE,add=TRUE,col=4)  # marginal mean of expected number of recurrent events  out <- recurrentMarginal(Event(entry,time,status)~cluster(id),data=rr,cause=1,death.code=2) plot(out,se=TRUE,ylab=\"marginal mean\",col=2) summary(out,times=c(1000,2000)) #> [[1]] #>     new.time     mean        se  CI-2.5% CI-97.5% strata #> 245     1000 1.929941 0.1179801 1.712020 2.175601      0 #> 384     2000 4.070540 0.2243678 3.653708 4.534926      0 xr <- phreg(Surv(entry,time,status)~strata(strata)+cluster(id),data=rr) xdr <- phreg(Surv(entry,time,death)~strata(strata)+cluster(id),data=rr) par(mfrow=c(1,3)) plot(xdr,se=TRUE) title(main=\"death\") plot(xr,se=TRUE) rxr <-   robust.phreg(xr,fixbeta=1) plot(rxr,se=TRUE,robust=TRUE,add=TRUE,col=1:2)  out <- recurrentMarginal(Event(entry,time,status)~strata(strata)+cluster(id),              data=rr,cause=1,death.code=2) plot(out,se=TRUE,ylab=\"marginal mean\",col=1:2) # cox case xr <- phreg(Surv(entry,time,status)~x+cluster(id),data=rr) xdr <- phreg(Surv(entry,time,death)~x+cluster(id),data=rr) par(mfrow=c(1,3)) plot(xdr,se=TRUE) title(main=\"death\") plot(xr,se=TRUE) rxr <- robust.phreg(xr) plot(rxr,se=TRUE,robust=TRUE,add=TRUE,col=1:2)  out <- recurrentMarginalPhreg(xr,xdr) plot(out,se=TRUE,ylab=\"marginal mean\",col=1:2) #### predictions witout se's  ###outX <- recmarg(xr,dr,Xr=1,Xd=1) ###plot(outX,add=TRUE,col=3) rr <- simRecurrentList(100,list(base1,base1,base4),death.cumhaz=list(dr,base4),cens=3/5000,dependence=0) dtable(rr,~status+death,level=2) #>  #>      status #> death   0   1   2   3 #>     0  31 120 109  10 #>     1  56   0   0   0 #>     2  13   0   0   0 mets:::showfitsimList(rr,list(base1,base1,base4),list(dr,base4))"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"improving-efficiency","dir":"Articles","previous_headings":"","what":"Improving efficiency","title":"Recurrent events","text":"illustrate efficiency can improved using heterogenity data, now simulate data strong heterogenity. dynmamic augmentation regression history subject consisting specified terms terms: Nt, Nt2 (Nt squared), expNt (exp(-Nt)), NtexpNt (Nt*exp(-Nt)) simply specifying directly. developed Cortese Scheike (2022).  case covariates might important still interested marginal mean can also augment wrt covariates","code":"rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,cens=3/5000,dependence=4,var.z=1) rr <-  count.history(rr)  rr <- transform(rr,statusD=status) rr <- dtransform(rr,statusD=3,death==1) dtable(rr,~statusD+status+death,level=2,response=1) #>  #>       statusD #> status   0   1   2   3 #>      0  79   0   0 121 #>      1   0 275   0   0 #>      2   0   0  35   0 #>  #>      statusD #> death   0   1   2   3 #>     0  79 275  35   0 #>     1   0   0   0 121  ##xr <- phreg(Surv(start,stop,status==1)~cluster(id),data=rr) ##dr <- phreg(Surv(start,stop,death)~cluster(id),data=rr) # marginal mean of expected number of recurrent events  out <- recurrentMarginal(Event(start,stop,statusD)~cluster(id),data=rr,cause=1,death.code=3)  times <- 500*(1:10) recEFF1 <- recurrentMarginalAIPCW(Event(start,stop,statusD)~cluster(id),data=rr,times=times,cens.code=0,                    death.code=3,cause=1,augment.model=~Nt) with( recEFF1, cbind(times,muP,semuP,muPAt,semuPAt,semuPAt/semuP)) #>       times       muP      semuP     muPAt    semuPAt           #>  [1,]   500 0.6613768 0.08161137 0.6613573 0.08140749 0.9975018 #>  [2,]  1000 1.0744212 0.12757420 1.0663041 0.12611114 0.9885317 #>  [3,]  1500 1.4043440 0.18482988 1.4063277 0.18029148 0.9754455 #>  [4,]  2000 1.6558938 0.23411074 1.6459306 0.22412588 0.9573498 #>  [5,]  2500 1.9655087 0.31221814 1.9646959 0.29588974 0.9477020 #>  [6,]  3000 2.1616770 0.37981206 2.1950498 0.35547431 0.9359216 #>  [7,]  3500 2.4425027 0.51943450 2.5067063 0.46175008 0.8889476 #>  [8,]  4000 2.7545313 0.66543528 2.8446029 0.56805536 0.8536598 #>  [9,]  4500 2.8247378 0.66133467 2.9122268 0.57249107 0.8656602 #> [10,]  5000 2.8247378 0.66133467 2.9122268 0.57249107 0.8656602  times <- 500*(1:10) ###recEFF14 <- recurrentMarginalAIPCW(Event(start,stop,statusD)~cluster(id),data=rr,times=times,cens.code=0, ###death.code=3,cause=1,augment.model=~Nt+Nt2+expNt+NtexpNt) ###with(recEFF14,cbind(times,muP,semuP,muPAt,semuPAt,semuPAt/semuP))  recEFF14 <- recurrentMarginalAIPCW(Event(start,stop,statusD)~cluster(id),data=rr,times=times,cens.code=0, death.code=3,cause=1,augment.model=~Nt+I(Nt^2)+I(exp(-Nt))+ I( Nt*exp(-Nt))) with(recEFF14,cbind(times,muP,semuP,muPAt,semuPAt,semuPAt/semuP)) #>       times       muP      semuP     muPAt    semuPAt           #>  [1,]   500 0.6613768 0.08161137 0.6656565 0.08123893 0.9954365 #>  [2,]  1000 1.0744212 0.12757420 1.0643130 0.12557776 0.9843508 #>  [3,]  1500 1.4043440 0.18482988 1.3959278 0.17882547 0.9675138 #>  [4,]  2000 1.6558938 0.23411074 1.6237759 0.21969809 0.9384366 #>  [5,]  2500 1.9655087 0.31221814 1.8801575 0.28908695 0.9259134 #>  [6,]  3000 2.1616770 0.37981206 2.0838062 0.34493993 0.9081858 #>  [7,]  3500 2.4425027 0.51943450 2.1734464 0.42984065 0.8275166 #>  [8,]  4000 2.7545313 0.66543528 2.3319579 0.51255611 0.7702569 #>  [9,]  4500 2.8247378 0.66133467 2.3579323 0.51482135 0.7784581 #> [10,]  5000 2.8247378 0.66133467 2.3579323 0.51482135 0.7784581  plot(out,se=TRUE,ylab=\"marginal mean\",col=2) k <- 1 for (t in times) {     ci1 <- c(recEFF1$muPAt[k]-1.96*recEFF1$semuPAt[k],              recEFF1$muPAt[k]+1.96*recEFF1$semuPAt[k])     ci2 <- c(recEFF1$muP[k]-1.96*recEFF1$semuP[k],              recEFF1$muP[k]+1.96*recEFF1$semuP[k])     lines(rep(t,2)-2,ci2,col=2,lty=1,lwd=2)     lines(rep(t,2)+2,ci1,col=1,lty=1,lwd=2)     k <- k+1 } legend(\"bottomright\",c(\"Eff-pred\"),lty=1,col=c(1,3)) n <- 200 X <- matrix(rbinom(n*2,1,0.5),n,2) colnames(X) <- paste(\"X\",1:2,sep=\"\") ### r1 <- exp( X %*% c(0.3,-0.3)) rd <- exp( X %*% c(0.3,-0.3)) rc <- exp( X %*% c(0,0)) fz <- NULL rr <- mets:::simGLcox(n,base1,dr,var.z=0,r1=r1,rd=rd,rc=rc,fz,model=\"twostage\",cens=3/5000)  rr <- cbind(rr,X[rr$id+1,])  dtable(rr,~statusD+status+death,level=2,response=1) #>  #>       statusD #> status   0   1   3 #>      0  92   0 108 #>      1   0 556   0 #>  #>      statusD #> death   0   1   3 #>     0  92 363   0 #>     1   0 193 108  times <- seq(500,5000,by=500) recEFF1x <- recurrentMarginalAIPCW(Event(start,stop,statusD)~cluster(id),data=rr,times=times,                    cens.code=0,death.code=3,cause=1,augment.model=~X1+X2) with(recEFF1x, cbind(muP,muPA,muPAt,semuP,semuPA,semuPAt,semuPAt/semuP)) #>            muP     muPA    muPAt      semuP     semuPA    semuPAt           #>  [1,] 1.145071 1.144032 1.143158 0.08446448 0.08438871 0.08433747 0.9984963 #>  [2,] 1.992395 1.987834 1.967759 0.16163097 0.16021919 0.15987853 0.9891577 #>  [3,] 2.987070 3.041386 2.975609 0.29841261 0.29352422 0.29276652 0.9810796 #>  [4,] 3.750338 3.781183 3.742977 0.38089153 0.37347550 0.37254937 0.9780983 #>  [5,] 4.401481 4.363984 4.379360 0.45643233 0.44836832 0.44476899 0.9744467 #>  [6,] 5.058667 4.943227 4.948666 0.58230638 0.56486481 0.55682137 0.9562344 #>  [7,] 5.797576 5.777539 5.609930 0.75658794 0.72607368 0.71043739 0.9390017 #>  [8,] 6.500899 6.740989 6.188770 0.89938615 0.86946649 0.84664142 0.9413547 #>  [9,] 7.160265 7.277316 6.575000 1.02814316 1.02039663 0.99768282 0.9703734 #> [10,] 7.709737 7.297200 6.832030 1.13548187 1.11392250 1.09078010 0.9606319  out <- recurrentMarginal(Event(start,stop,statusD)~cluster(id),data=rr,cause=1,death.code=3) summary(out,times=times) #> [[1]] #>     new.time     mean         se   CI-2.5%  CI-97.5% strata #> 201      500 1.145071 0.08446448 0.9909346  1.323184      0 #> 310     1000 1.992395 0.16163097 1.6995055  2.335760      0 #> 397     1500 2.987070 0.29841261 2.4558915  3.633136      0 #> 448     2000 3.750338 0.38089153 3.0734121  4.576359      0 #> 483     2500 4.401481 0.45643233 3.5919438  5.393469      0 #> 509     3000 5.058667 0.58230638 4.0369534  6.338966      0 #> 529     3500 5.797576 0.75658794 4.4891486  7.487362      0 #> 543     4000 6.500899 0.89938615 4.9569146  8.525806      0 #> 552     4500 7.160265 1.02814316 5.4038714  9.487531      0 #> 557     5000 7.709737 1.13548187 5.7766426 10.289721      0"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"regression-models-for-the-marginal-mean","dir":"Articles","previous_headings":"","what":"Regression models for the marginal mean","title":"Recurrent events","text":"One can also regression modelling , using model E(N1(t)|X)=Λ0(t)exp(XTβ)\\begin{align*} E(N_1(t) | X) &  = \\Lambda_0(t)  \\exp(X^T \\beta) \\end{align*} Ghost-Lin suggested IPCW score equations implemented recreg function mets. First generate data Ghosh-Lin model regression coefficients β=(−0.3,0.3)\\beta=(-0.3,0.3) baseline given base1, done assumption death rate given covariates Cox form baseline dr:  note extended censoring model gain little efficiency estimates close true values. Also possible IPCW regression fixed time-point can also Mao-Lin type composite outcome count recurrent events (cause 1) deaths (cause 3) example E(N1(t)+(D<t,ϵ=3)|X)=Λ0(t)exp(XTβ)\\begin{align*} E(N_1(t) + (D<t,\\epsilon=3) | X) &  = \\Lambda_0(t)  \\exp(X^T \\beta) \\end{align*} can also done competing risks death E(w1N1(t)+w2I(D<t,ϵ=3)|X)=Λ0(t)exp(XTβ)\\begin{align*} E(w_1 N_1(t) + w_2 (D<t,\\epsilon=3) | X) &  = \\Lambda_0(t)  \\exp(X^T \\beta) \\end{align*} weights w1,w2w_1,w_2 follow causes, 1 3. modify data changing cause 3 deaths cause 4  Predictions standard errors can computed via iid decompositions baseline regression coefficients. illustrate standard Ghosh-Lin model Ghosh-Lin model can made efficient regression augmentation method. First computing augmentation second step augmented estimator (Cortese Scheike (2023)): note simple augmentation improves standard errors expected. data generated assuming independence previous number events suffice augment covariates.","code":"n <- 100 X <- matrix(rbinom(n*2,1,0.5),n,2) colnames(X) <- paste(\"X\",1:2,sep=\"\") ### r1 <- exp( X %*% c(0.3,-0.3)) rd <- exp( X %*% c(0.3,-0.3)) rc <- exp( X %*% c(0,0)) fz <- NULL rr <- mets:::simGLcox(n,base1,dr,var.z=1,r1=r1,rd=rd,rc=rc,fz,cens=1/5000,type=2)  rr <- cbind(rr,X[rr$id+1,])  out  <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,cens.code=0) outs <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,cens.code=0,         cens.model=~strata(X1,X2)) summary(out)$coef #>      Estimate      S.E.    dU^-1/2   P-value #> X1  0.5194368 0.3797930 0.07990766 0.1714110 #> X2 -0.3358640 0.3803112 0.07976036 0.3771663 summary(outs)$coef #>      Estimate      S.E.    dU^-1/2   P-value #> X1  0.5762836 0.3571995 0.08035274 0.1066710 #> X2 -0.2775462 0.3520266 0.08007337 0.4304488  ## checking baseline par(mfrow=c(1,1)) plot(out) plot(outs,add=TRUE,col=2) lines(scalecumhaz(base1,1),col=3,lwd=2) outipcw  <- recregIPCW(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,         cens.code=0,times=2000) outipcws <- recregIPCW(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,         cens.code=0,times=2000,cens.model=~strata(X1,X2)) summary(outipcw)$coef #>               Estimate   Std.Err       2.5%     97.5%      P-value #> (Intercept) 1.19494027 0.2038193  0.7954619 1.5944187 4.552780e-09 #> X1          0.18509341 0.2825398 -0.3686743 0.7388612 5.123997e-01 #> X2          0.09805584 0.2805028 -0.4517195 0.6478312 7.266601e-01 summary(outipcws)$coef #>               Estimate   Std.Err       2.5%     97.5%      P-value #> (Intercept) 1.18811953 0.2024682  0.7912891 1.5849499 4.406099e-09 #> X1          0.20055180 0.2761747 -0.3407407 0.7418443 4.677301e-01 #> X2          0.09514999 0.2729203 -0.4397640 0.6300640 7.273622e-01 out  <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=c(1,3),         death.code=3,cens.code=0) summary(out)$coef #>      Estimate      S.E.    dU^-1/2   P-value #> X1  0.4896610 0.3460183 0.07609363 0.1570303 #> X2 -0.3056174 0.3459166 0.07595286 0.3769660 rr$binf <- rbinom(nrow(rr),1,0.5)  rr$statusDC <- rr$statusD rr <- dtransform(rr,statusDC=4, statusD==3 & binf==0) rr$weight <- 1 rr <- dtransform(rr,weight=2,statusDC==3)  outC  <- recreg(Event(start,stop,statusDC)~X1+X2+cluster(id),data=rr,cause=c(1,3),          death.code=c(3,4),cens.code=0) summary(outC)$coef #>      Estimate      S.E.    dU^-1/2   P-value #> X1  0.5069982 0.3651533 0.07814845 0.1649991 #> X2 -0.3255327 0.3653288 0.07801576 0.3728929  outCW  <- recreg(Event(start,stop,statusDC)~X1+X2+cluster(id),data=rr,cause=c(1,3),           death.code=c(3,4),cens.code=0,wcomp=c(1,2)) summary(outCW)$coef #>      Estimate      S.E.    dU^-1/2   P-value #> X1  0.4955643 0.3521275 0.07650295 0.1593256 #> X2 -0.3160290 0.3519907 0.07638286 0.3692744  plot(out,ylab=\"Mean composite\") plot(outC,col=2,add=TRUE) plot(outCW,col=3,add=TRUE) out  <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,cens.code=0) summary(out) #>  #>    n events #>  740    640 #>  #>  100 clusters #> coeffients: #>     Estimate      S.E.   dU^-1/2 P-value #> X1  0.519437  0.379793  0.079908  0.1714 #> X2 -0.335864  0.380311  0.079760  0.3772 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> X1  1.68108 0.79856 3.5389 #> X2  0.71472 0.33917 1.5061 baseiid <- iidBaseline(out,time=3000) GLprediid(baseiid,rr[1:5,]) #>          pred    se-log    lower     upper #> [1,] 9.117785 0.3716211 4.401137 18.889209 #> [2,] 3.876474 0.3854773 1.821034  8.251935 #> [3,] 3.876474 0.3854773 1.821034  8.251935 #> [4,] 3.876474 0.3854773 1.821034  8.251935 #> [5,] 6.516666 0.2216876 4.220119 10.062971 outA  <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,cause=1,death.code=3,          cens.code=0,augment.model=~Nt+X1+X2) summary(outA)$coef #>      Estimate      S.E.    dU^-1/2    P-value #> X1  0.5716809 0.3461524 0.08022919 0.09863056 #> X2 -0.3447143 0.3480247 0.07981423 0.32193577"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"two-stage-modelling","dir":"Articles","previous_headings":"","what":"Two-stage modelling","title":"Recurrent events","text":"simulate data terminal event Cox form recurrent events satisfying Ghosh-Lin model rate Cox form. type=3 Ghosh-Lin model recurrent events Cox terminal event. type=2 Cox model recurrent events among survivors Cox terminal event. simulations based time-grid make linear approximations cumulative hazards Now fit two-stage model (recreg must called twostage=TRUE) Standard errors computed assuming parameters outs known, therefore propobly bit small. bootstrap get reliable standard errors.","code":"set.seed(100) n <- 200 X <- matrix(rbinom(n*2,1,0.5),n,2) colnames(X) <- paste(\"X\",1:2,sep=\"\") ### r1 <- exp( X %*% c(0.3,-0.3)) rd <- exp( X %*% c(0.3,-0.3)) rc <- exp( X %*% c(0,0)) fz <- NULL ## type=3 is cox-cox and type=2 is Ghosh-Lin/Cox model  rr <- mets:::simGLcox(n,base1,dr,var.z=1,r1=r1,rd=rd,rc=rc,fz,cens=1/5000,type=3)  rr <- cbind(rr,X[rr$id+1,]) ### out  <- phreg(Event(start,stop,statusD==1)~X1+X2+cluster(id),data=rr) outs <- phreg(Event(start,stop,statusD==3)~X1+X2+cluster(id),data=rr) ## cox/cox tsout <- twostageREC(outs,out,data=rr) summary(tsout) #> Cox(recurrent)-Cox(terminal) intensity model #>  #>  200 clusters #> coeffients: #>             Estimate Std.Err    2.5%   97.5% P-value #> dependence1  1.14651 0.14126 0.86964 1.42339       0 #>  #> var,shared: #>             Estimate Std.Err    2.5%   97.5% P-value #> dependence1  1.14651 0.14126 0.86964 1.42339       0 ### rr <- mets:::simGLcox(n,base1,dr,var.z=1,r1=r1,rd=rd,rc=rc,fz,cens=1/5000,type=3,share=0.5)  rr <- cbind(rr,X[rr$id+1,]) ### out  <- phreg(Event(start,stop,statusD==1)~X1+X2+cluster(id),data=rr) outs <- phreg(Event(start,stop,statusD==3)~X1+X2+cluster(id),data=rr) # tsout <- twostageREC(outs,out,data=rr,model=\"shared\") summary(tsout) #> Cox(recurrent)-Cox(terminal) intensity model #>  #>  200 clusters #> coeffients: #>             Estimate Std.Err    2.5%   97.5% P-value #> dependence1  1.07333 0.14096 0.79707 1.34960       0 #> share1       0.67344 0.12832 0.42193 0.92495       0 #>  #> var,shared: #>             Estimate    2.5%  97.5% #> dependence1  1.07333 0.79707 1.3496 #> share1       0.67344 0.42193 0.9250 ### rr <- mets:::simGLcox(n,base1,dr,var.z=1,r1=r1,rd=rd,rc=rc,fz,cens=1/5000,type=2)  rr <- cbind(rr,X[rr$id+1,]) outs  <- phreg(Event(start,stop,statusD==3)~X1+X2+cluster(id),data=rr) outgl  <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),data=rr,twostage=TRUE,death.code=3) ## ## ghosh-lin/cox glout <- twostageREC(outs,outgl,data=rr,theta=1) summary(glout) #> Ghosh-Lin(recurrent)-Cox(terminal) mean model #>  #>  200 clusters #> coeffients: #>             Estimate Std.Err    2.5%   97.5% P-value #> dependence1  1.04401 0.08988 0.86785 1.22017       0 #>  #> var,shared: #>             Estimate Std.Err    2.5%   97.5% P-value #> dependence1  1.04401 0.08988 0.86785 1.22017       0 ### glout <- twostageREC(outs,outgl,data=rr,model=\"shared\",theta=1,nu=0.9) #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced #> Warning in log(N): NaNs produced summary(glout) #> Ghosh-Lin(recurrent)-Cox(terminal) mean model #>  #>  200 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> dependence1 1.192629 0.094129 1.008140 1.377117       0 #> share1      1.643844 0.176676 1.297566 1.990121       0 #>  #> var,shared: #>             Estimate   2.5%  97.5% #> dependence1   1.1926 1.0081 1.3771 #> share1        1.6438 1.2976 1.9901 glout$gradient #> [1] -1.823718e-08  2.506173e-08"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"simulations-with-specific-structure","dir":"Articles","previous_headings":"","what":"Simulations with specific structure","title":"Recurrent events","text":"function simGLcox can simulate data recurrent process mean Ghosh-Lin form. key E(N1(t)|X)=Λ0(t)exp(XTβ)=∫0tS(t|X,Z)dR(t|X,Z)\\begin{align*} E(N_1(t) | X) &  = \\Lambda_0(t)  \\exp(X^T \\beta) = \\int_0^t S(t|X,Z) dR(t|X,Z) \\end{align*} ZZ possible frailty. Therefore R(t|X,Z)=ZΛ0(t)exp(XTβ)S(t|X,Z)\\begin{align*}  R(t|X,Z) & = \\frac{Z \\Lambda_0(t)  \\exp(X^T \\beta) }{S(t|X,Z)} \\end{align*} leads Ghosh-Lin model. can choose survival model Cox form among survivors option model=“twostage”, otherwise model=“frailty” uses survival model rate Zλd(t)rdZ \\lambda_d(t) rd. ZZ gamma distributed variance can specified. simulations based piecwise-linear approximation hazard functions S(t|X,Z)S(t|X,Z) R(t|X,Z)R(t|X,Z). can also simulate models terminal event Cox form rate among survivors Cox form. E(dN1|D>t,X)=λ1(t)r1E(dN_1 | D>t, X) = \\lambda_1(t) r_1 E(dNd|D>t,X)=λd(t)rdE(dN_d | D>t, X) = \\lambda_d(t) r_d underlying models shared frailty model  can simulate data underlying dependence fromm two-stage model (simGLcox) using simRecurrent random effects models, Cox-Cox Ghosh-Lin-Cox models. marginals - Cox- Cox form - Ghosh-Lin - Cox form Draws covariates data simulates data marginals given.","code":"n <- 100 X <- matrix(rbinom(n*2,1,0.5),n,2) colnames(X) <- paste(\"X\",1:2,sep=\"\") ### r1 <- exp( X %*% c(0.3,-0.3)) rd <- exp( X %*% c(0.3,-0.3)) rc <- exp( X %*% c(0,0)) rr <- mets:::simGLcox(n,base1,dr,var.z=0,r1=r1,rd=rd,rc=rc,model=\"twostage\",cens=3/5000)  rr <- cbind(rr,X[rr$id+1,]) rr <- mets:::simGLcox(100,base1,dr,var.z=1,r1=r1,rd=rd,rc=rc,type=3,cens=3/5000)  rr <- cbind(rr,X[rr$id+1,]) margsurv <- phreg(Surv(start,stop,statusD==3)~X1+X2+cluster(id),rr) recurrent <- phreg(Surv(start,stop,statusD==1)~X1+X2+cluster(id),rr) estimate(margsurv) #>    Estimate Std.Err    2.5%  97.5% P-value #> X1   0.2535  0.2553 -0.2469 0.7538  0.3208 #> X2  -0.2595  0.2637 -0.7764 0.2574  0.3252 estimate(recurrent) #>    Estimate Std.Err     2.5%   97.5% P-value #> X1   0.6675  0.3019  0.07583 1.25921 0.02702 #> X2  -0.4356  0.2637 -0.95247 0.08135 0.09864 par(mfrow=c(1,2));  plot(margsurv); lines(dr,col=3);  plot(recurrent); lines(base1,col=3) simcoxcox <- sim.recurrent(recurrent,margsurv,n=10,data=rr)  recurrentGL <- recreg(Event(start,stop,statusD)~X1+X2+cluster(id),rr,death.code=3) simglcox <- sim.recurrent(recurrentGL,margsurv,n=10,data=rr)"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"other-marginal-properties","dir":"Articles","previous_headings":"","what":"Other marginal properties","title":"Recurrent events","text":"mean useful summary measure easy useful look simple summary measures probability exceeding kk events cumulative incidence Tk=inf{t:N1*(t)=k}T_{k} = \\inf \\{ t: N_1^*(t)=k \\} competing DD. thus equivalent certain cumulative incidence TkT_k occurring DD. denote cumulative incidence F̂k(t)\\hat F_k(t). note also N1*(t)2N_1^*(t)^2 can written ∑k=0K∫0tI(D>s)(N1*(s−)=k)f(k)dN1*(s)\\begin{align*}    \\sum_{k=0}^K  \\int_0^t (D > s) (N_1^*(s-)=k) f(k) dN_1^*(s) \\end{align*} f(k)=(k+1)2−k2f(k)=(k+1)^2 - k^2, mean can written ∑k=0K∫0tS(s)f(k)P(N1*(s−)=k|D≥s)E(dN1*(s)|N1*(s−)=k,D>s)\\begin{align*}     \\sum_{k=0}^K \\int_0^t S(s) f(k) P(N_1^*(s-)= k  | D  \\geq s) E( dN_1^*(s)  | N_1^*(s-)=k, D> s)  \\end{align*} estimated μ̃1,2(t)=∑k=0K∫0tŜ(s)f(k)Y1•k(s)Y•(s)1Y1•k(s)dN1•k(s)=∑=1n∫0tŜ(s)f(Ni1(s−))1Y•(s)dNi1(s),\\begin{align*} \\tilde \\mu_{1,2}(t) & =      \\sum_{k=0}^K \\int_0^t \\hat S(s) f(k)      \\frac{Y_{1\\bullet}^k(s)}{Y_\\bullet (s)} \\frac{1}{Y_{1\\bullet}^k(s)} d N_{1\\bullet}^k(s)= \\sum_{=1}^n \\int_0^t \\hat S(s) f(N_{i1}(s-)) \\frac{1}{Y_\\bullet (s)} d N_{i1}(s), \\end{align*} similar “product-limit” estimator E((N1*(t))2)E( (N_1^*(t))^2 )μ̂1,2(t)=∑k=0Kk2(F̂k(t)−F̂k+1(t)).\\begin{align}   \\hat \\mu_{1,2}(t) & =    \\sum_{k=0}^K k^2 ( \\hat F_{k}(t) - \\hat F_{k+1}(t) ). \\end{align} use esimator probabilty exceeding “k” events based fact (N1*(t)≥k)(N_1^*(t) \\geq k) equivalent ∫0tI(D>s)(N1*(s−)=k−1)dN1*(s),\\begin{align*}     \\int_0^t (D > s) (N_1^*(s-)=k-1) dN_1^*(s), \\end{align*} suggesting mean can computed ∫0tS(s)P(N1*(s−)=k−1|D≥s)E(dN1*(s)|N1*(s−)=k−1,D>s)\\begin{align*} \\int_0^t S(s) P(N_1^*(s-)= k-1  | D  \\geq s) E( dN_1^*(s)  | N_1^*(s-)=k-1, D> s)  \\end{align*} estimated F̃k(t)=∫0tŜ(s)Y1•k−1(s)Y•(s)1Y1•k−1(s)dN1•k−1(s).\\begin{align*} \\tilde F_k(t) = \\int_0^t \\hat S(s)  \\frac{Y_{1\\bullet}^{k-1}(s)}{Y_\\bullet (s)}              \\frac{1}{Y_{1\\bullet}^{k-1}(s)} d N_{1\\bullet}^{k-1}(s). \\end{align*} compute estimators use prob.exceed.recurrent function  can also look mean variance based estimators just described","code":"rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,cens=3/5000,dependence=4,var.z=1) rr <- transform(rr,statusD=status) rr <- dtransform(rr,statusD=3,death==1) rr <-  count.history(rr) dtable(rr,~statusD) #>  #> statusD #>   0   1   2   3  #>  93 287  32 107  oo <- prob.exceed.recurrent(Event(entry,time,statusD)~cluster(id),rr,cause=1,death.code=3) plot(oo,types=1:5) par(mfrow=c(1,2)) with(oo,plot(time,meanN,col=2,type=\"l\")) with(oo,plot(time,varN,type=\"l\"))"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"multiple-events","dir":"Articles","previous_headings":"","what":"Multiple events","title":"Recurrent events","text":"now generate recurrent events two types events. start generating data events independent. Based can estimate also joint distribution function, probability (N1(t)≥k1,N2(t)≥k2)(N_1(t) \\geq k_1, N_2(t) \\geq k_2)","code":"rr <- simRecurrentII(200,base1,cumhaz2=base4,death.cumhaz=dr) rr <-  count.history(rr) dtable(rr,~death+status) #>  #>       status   0   1   2 #> death                    #> 0             32 651  84 #> 1            168   0   0 # Bivariate probability of exceeding  ## oo <- prob.exceedBiRecurrent(rr,1,2,exceed1=c(1,5),exceed2=c(1,2)) ## with(oo, matplot(time,pe1e2,type=\"s\")) ## nc <- ncol(oo$pe1e2) ## legend(\"topleft\",legend=colnames(oo$pe1e2),lty=1:nc,col=1:nc)"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"looking-at-other-simulations-with-dependence","dir":"Articles","previous_headings":"","what":"Looking at other simulations with dependence","title":"Recurrent events","text":"Using normally distributed random effects plot 4 different settings. variance 0.50.5 random effects change correlation. let correlation random effect associated N1N_1 N2N_2 denoted ρ12\\rho_{12} correlation random effects associated NjN_j DD terminal event denoted ρj3\\rho_{j3}, organize correlation vector ρ=(ρ12,ρ13,ρ23)\\rho=(\\rho_{12},\\rho_{13},\\rho_{23}). Scenario ρ=(0,0.0,0.0)\\rho=(0,0.0,0.0) Independence among efects. Scenario II ρ=(0,0.5,0.5)\\rho=(0,0.5,0.5) Independence among survivors dependence terminal event Scenario III ρ=(0.5,0.5,0.5)\\rho=(0.5,0.5,0.5) Positive dependence among survivors dependence terminal event Scenario IV ρ=(−0.4,0.5,0.5)\\rho=(-0.4,0.5,0.5) Negative dependence among survivors positive dependence terminal event","code":"data(CPH_HPN_CRBSI)  dr <- CPH_HPN_CRBSI$terminal  base1 <- CPH_HPN_CRBSI$crbsi   base4 <- CPH_HPN_CRBSI$mechanical    par(mfrow=c(1,3))   var.z <- c(0.5,0.5,0.5)   # death related to  both causes in same way    cor.mat <- corM <- rbind(c(1.0, 0.0, 0.0), c(0.0, 1.0, 0.0), c(0.0, 0.0, 1.0))   rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,var.z=var.z,cor.mat=cor.mat,dependence=2)   rr <- count.history(rr,types=1:2) ###  cor(attr(rr,\"z\")) ###  coo <- covarianceRecurrent(rr,1,2,status=\"status\",start=\"entry\",stop=\"time\") ###  plot(coo,main =\"Scenario I\") var.z <- c(0.5,0.5,0.5)   # death related to  both causes in same way    cor.mat <- corM <- rbind(c(1.0, 0.0, 0.5), c(0.0, 1.0, 0.5), c(0.5, 0.5, 1.0))   rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,var.z=var.z,cor.mat=cor.mat,dependence=2)   rr <- count.history(rr,types=1:2) ###  coo <- covarianceRecurrent(rr,1,2,status=\"status\",start=\"entry\",stop=\"time\") ###  par(mfrow=c(1,3)) ###  plot(coo,main =\"Scenario II\") var.z <- c(0.5,0.5,0.5)   # positive dependence for N1 and N2 all related in same way   cor.mat <- corM <- rbind(c(1.0, 0.5, 0.5), c(0.5, 1.0, 0.5), c(0.5, 0.5, 1.0))   rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,var.z=var.z,cor.mat=cor.mat,dependence=2)   rr <- count.history(rr,types=1:2) ###  coo <- covarianceRecurrent(rr,1,2,status=\"status\",start=\"entry\",stop=\"time\") ###  par(mfrow=c(1,3)) ###  plot(coo,main=\"Scenario III\") var.z <- c(0.5,0.5,0.5)   # negative dependence for N1 and N2 all related in same way   cor.mat <- corM <- rbind(c(1.0, -0.4, 0.5), c(-0.4, 1.0, 0.5), c(0.5, 0.5, 1.0))   rr <- simRecurrentII(200,base1,base4,death.cumhaz=dr,var.z=var.z,cor.mat=cor.mat,dependence=2)   rr <- count.history(rr,types=1:2) ###  coo <- covarianceRecurrent(rr,1,2,status=\"status\",start=\"entry\",stop=\"time\") ###  par(mfrow=c(1,3)) ###  plot(coo,main=\"Scenario IV\")"},{"path":"http://kkholst.github.io/mets/articles/recurrent-events.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Recurrent events","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/rmst-ate.html","id":"rmst","dir":"Articles","previous_headings":"","what":"RMST","title":"Average treatment effect (ATE) for Restricted mean survival and years lost of Competing risks","text":"Regression rmst outcome E(T∧t|X)=exp(XTβ)E(T \\wedge t | X) = exp(X^T \\beta) based IPCW adjustment censoring, thus solving estimating equation XT[(T∧t)(C>T∧t)GC(T∧t,X)−exp(XTβ)]=0.\\begin{align*}  & X^T [ (T \\wedge t) \\frac{(C > T \\wedge t)}{G_C(T \\wedge t,X)} - exp(X^T \\beta) ] = 0 . \\end{align*} done resmeanIPCW function. fully saturated model full censoring model equal integrals Kaplan-Meier estimators illustrated . can also compute integral Kaplan-Meier Cox based survival estimator get RMST (resmean.phreg function) ∫0tS(s|X)ds \\int_0^t S(s|X) ds . competing risks years lost can computed via cumulative incidence functions (cif.yearslost) IPCW estimator sinceE((ϵ=1)(t−T∧t)|X)=∫0tF1(s)ds. E( (\\epsilon=1) ( t - T \\wedge t ) | X) = \\int_0^t F_1(s) ds.  fully saturated model full censoring model estimators equivalent illustrated .  Based output IPCW estimators can derive measure interest","code":"set.seed(101)       data(bmt); bmt$time <- bmt$time+runif(nrow(bmt))*0.001       # E( min(T;t) | X ) = exp( a+b X) with IPCW estimation       out <- resmeanIPCW(Event(time,cause!=0)~tcell+platelet+age,bmt,                      time=50,cens.model=~strata(platelet),model=\"exp\")      summary(out) #>    n events #>  408    245 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  3.014321  0.065114  2.886700  3.141941  0.0000 #> tcell        0.106526  0.138268 -0.164473  0.377525  0.4410 #> platelet     0.247103  0.097337  0.056325  0.437880  0.0111 #> age         -0.185936  0.043566 -0.271324 -0.100548  0.0000 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 20.37524 17.93403 23.1488 #> tcell        1.11241  0.84834  1.4587 #> platelet     1.28031  1.05794  1.5494 #> age          0.83033  0.76237  0.9043             ### same as Kaplan-Meier for full censoring model       bmt$int <- with(bmt,strata(tcell,platelet))      out <- resmeanIPCW(Event(time,cause!=0)~-1+int,bmt,time=30,                                   cens.model=~strata(platelet,tcell),model=\"lin\")      estimate(out) #>                        Estimate Std.Err  2.5% 97.5%   P-value #> inttcell=0, platelet=0    13.60  0.8316 11.97 15.23 3.826e-60 #> inttcell=0, platelet=1    18.90  1.2696 16.41 21.39 3.997e-50 #> inttcell=1, platelet=0    16.19  2.4061 11.48 20.91 1.705e-11 #> inttcell=1, platelet=1    17.77  2.4536 12.96 22.58 4.463e-13      out1 <- phreg(Surv(time,cause!=0)~strata(tcell,platelet),data=bmt)      rm1 <- resmean.phreg(out1,times=30)      summary(rm1) #>                     strata times    rmean  se.rmean    lower    upper #> tcell=0, platelet=0      0    30 13.60295 0.8315418 12.06700 15.33439 #> tcell=0, platelet=1      1    30 18.90127 1.2693263 16.57021 21.56026 #> tcell=1, platelet=0      2    30 16.19121 2.4006185 12.10806 21.65131 #> tcell=1, platelet=1      3    30 17.76610 2.4421987 13.57008 23.25956 #>                     years.lost #> tcell=0, platelet=0   16.39705 #> tcell=0, platelet=1   11.09873 #> tcell=1, platelet=0   13.80879 #> tcell=1, platelet=1   12.23390            ## competing risks years-lost for cause 1        out <- resmeanIPCW(Event(time,cause)~-1+int,bmt,time=30,cause=1,                                  cens.model=~strata(platelet,tcell),model=\"lin\")      estimate(out) #>                        Estimate Std.Err   2.5%  97.5%   P-value #> inttcell=0, platelet=0   12.105  0.8508 10.438 13.773 6.162e-46 #> inttcell=0, platelet=1    6.884  1.1741  4.583  9.185 4.536e-09 #> inttcell=1, platelet=0    7.261  2.3533  2.648 11.873 2.033e-03 #> inttcell=1, platelet=1    5.780  2.0925  1.679  9.882 5.737e-03      ## same as integrated cumulative incidence       rmc1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=30,cause=1)      summary(rmc1) #> $estimate #>                     strata times    intF_1   intF_2 se.intF_1 se.intF_2 #> tcell=0, platelet=0      0    30 12.105125 4.291930 0.8508102 0.6161439 #> tcell=0, platelet=1      1    30  6.884171 4.214556 1.1740988 0.9057028 #> tcell=1, platelet=0      2    30  7.260755 6.548034 2.3532867 1.9703340 #> tcell=1, platelet=1      3    30  5.780369 6.453535 2.0924946 2.0815225 #>                     total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell=0, platelet=0         16.39705    10.547328    13.893001     3.239330 #> tcell=0, platelet=1         11.09873     4.928092     9.616665     2.765857 #> tcell=1, platelet=0         13.80879     3.846790    13.704561     3.630616 #> tcell=1, platelet=1         12.23390     2.843285    11.751429     3.429661 #>                     upper_intF_2 #> tcell=0, platelet=0     5.686565 #> tcell=0, platelet=1     6.422055 #> tcell=1, platelet=0    11.809770 #> tcell=1, platelet=1    12.143507       ## plotting the years lost for different horizon's and the two causes       par(mfrow=c(1,3))      plot(rm1,years.lost=TRUE,se=1)      ## cause refers to column of cumhaz for the different causes      plot(rmc1,cause=1,se=1)      plot(rmc1,cause=2,se=1) estimate(out) #>                        Estimate Std.Err   2.5%  97.5%   P-value #> inttcell=0, platelet=0   12.105  0.8508 10.438 13.773 6.162e-46 #> inttcell=0, platelet=1    6.884  1.1741  4.583  9.185 4.536e-09 #> inttcell=1, platelet=0    7.261  2.3533  2.648 11.873 2.033e-03 #> inttcell=1, platelet=1    5.780  2.0925  1.679  9.882 5.737e-03  measures <- function(p) {  ratio1 <- p[1]/p[2]; ratio2 <- p[2]/p[1]; dif1 <- p[4]-p[1]; dif2 <- p[3]-p[1]  m <- c(dif1,dif2,ratio1,ratio2)  return(m) }   labs <- c(\"dif4-1\",\"dif3-1\",\"ratio 1/2\",\"ratio 2/1\")  estimate(out,f=measures,labels=labs) #>           Estimate Std.Err     2.5%    97.5%   P-value #> dif4-1     -6.3248  2.2589 -10.7520 -1.89748 5.111e-03 #> dif3-1     -4.8444  2.5024  -9.7489  0.06018 5.288e-02 #> ratio 1/2   1.7584  0.3244   1.1227  2.39414 5.924e-08 #> ratio 2/1   0.5687  0.1049   0.3631  0.77431 5.924e-08"},{"path":"http://kkholst.github.io/mets/articles/rmst-ate.html","id":"average-treatment-effect","dir":"Articles","previous_headings":"","what":"Average treatment effect","title":"Average treatment effect (ATE) for Restricted mean survival and years lost of Competing risks","text":"Computes average treatment effect restricted mean survival years lost competing risks situation","code":"dfactor(bmt) <- tcell~tcell  bmt$event <- (bmt$cause!=0)*1  out <- resmeanATE(Event(time,event)~tcell+platelet,data=bmt,time=40,treat.model=tcell~platelet)  summary(out) #>    n events #>  408    241 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  2.852670  0.062493  2.730187  2.975153  0.0000 #> tcell1       0.021401  0.122907 -0.219493  0.262295  0.8618 #> platelet     0.303489  0.090758  0.125606  0.481372  0.0008 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 17.33400 15.33575 19.5926 #> tcell1       1.02163  0.80293  1.2999 #> platelet     1.35458  1.13383  1.6183 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.26223  0.95926 17.38212 21.14234  0.0000 #> treat1    19.67890  2.22777 15.31255 24.04526  0.0000 #> treat:1-0  0.41667  2.41067 -4.30815  5.14150  0.8628 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.28131  0.95807 17.40352 21.15910  0.0000 #> treat1    20.34466  2.54101 15.36438 25.32494  0.0000 #> treat:1-0  1.06335  2.70979 -4.24773  6.37443  0.6948    out1 <- resmeanATE(Event(time,cause)~tcell+platelet,data=bmt,cause=1,time=40,             treat.model=tcell~platelet)  summary(out1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  2.806295  0.069619  2.669845  2.942745  0.0000 #> tcell1      -0.374065  0.247669 -0.859488  0.111358  0.1310 #> platelet    -0.491736  0.164946 -0.815025 -0.168447  0.0029 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 16.54849 14.43773 18.9678 #> tcell1       0.68793  0.42338  1.1178 #> platelet     0.61156  0.44263  0.8450 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    14.53185  0.95707 12.65604 16.40767  0.0000 #> treat1     9.99693  2.37814  5.33587 14.65800  0.0000 #> treat:1-0 -4.53492  2.57516 -9.58213  0.51229  0.0782 #>  #> Average Treatment effects (double robust) : #>             Estimate    Std.Err       2.5%      97.5% P-value #> treat0     14.513742   0.958025  12.636048  16.391436  0.0000 #> treat1      9.365012   2.417032   4.627717  14.102307  0.0001 #> treat:1-0  -5.148730   2.597947 -10.240612  -0.056848  0.0475   out2 <- resmeanATE(Event(time,cause)~tcell+platelet,data=bmt,cause=2,time=40,             treat.model=tcell~platelet)  summary(out2) #>    n events #>  408     84 #>  #>  408 clusters #> coeffients: #>               Estimate    Std.Err       2.5%      97.5% P-value #> (Intercept)  1.8266090  0.1312181  1.5694263  2.0837918  0.0000 #> tcell1       0.4751558  0.2403839  0.0040121  0.9462996  0.0481 #> platelet    -0.0090724  0.2168469 -0.4340845  0.4159397  0.9666 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  6.21278 4.80389 8.0349 #> tcell1       1.60826 1.00402 2.5762 #> platelet     0.99097 0.64786 1.5158 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0     6.19518  0.71372  4.79631  7.59405  0.0000 #> treat1     9.96349  2.09256  5.86216 14.06482  0.0000 #> treat:1-0  3.76831  2.21654 -0.57602  8.11264  0.0891 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0     6.20484  0.71392  4.80559  7.60410  0.0000 #> treat1    10.30065  2.21700  5.95542 14.64588  0.0000 #> treat:1-0  4.09581  2.32897 -0.46889  8.66050  0.0786"},{"path":"http://kkholst.github.io/mets/articles/rmst-ate.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Average treatment effect (ATE) for Restricted mean survival and years lost of Competing risks","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/survival-ate.html","id":"g-computation-for-the-cox-and-fine-gray-models","dir":"Articles","previous_headings":"","what":"G-computation for the Cox and Fine-Gray models","title":"G-Computation or standardization for the Cox, Fine-Gray and binomial regression models for survival data","text":"Computing standardized estimate (G-estimation) based Cox Fine-Gray model : Ŝ(t,=)=n−1∑(t,=,Xi) \\hat S(t,=) = n^{-1} \\sum_i S(t,=,X_i)   estimator influence function S(t,=,Xi)−S(t,=)+E(DA0(t),βS(t,=,Xi))ϵi(t) S(t,=,X_i) -  S(t,=)  + E( D_{A_0(t), \\beta} S(t,=,X_i)  ) \\epsilon_i(t)  ϵi(t)\\epsilon_i(t) iid decomposition (̂(t)−(t),β̂−β)(\\hat (t) - (t), \\hat \\beta- \\beta). estimates causal interpration assumption -unmeasured confounders, even without causal assumptions standardization can still useful summary measure. First looking cumulative incidence via Fine-Gray model two causes making plot standardized cumulative incidence cause 1.  Now looking survival probability","code":"library(mets) set.seed(100)  data(bmt); bmt$time <- bmt$time+runif(nrow(bmt))*0.001 dfactor(bmt) <- tcell~tcell bmt$event <- (bmt$cause!=0)*1  fg1 <- cifregFG(Event(time,cause)~tcell+platelet+age,bmt,cause=1) summary(survivalG(fg1,bmt,time=50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.4331 0.02749 0.3793 0.4870 6.321e-56 #> risk1   0.2727 0.05863 0.1577 0.3876 3.313e-06 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err   2.5%    97.5% P-value #> ps0  -0.1605 0.06353 -0.285 -0.03597 0.01153 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%       97.5%    P-value #> [ps0] -0.4628288 0.2212039 -0.8963806 -0.02927703 0.03641016 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.6295004 0.4080439 0.9711474  fg2 <- cifregFG(Event(time,cause)~tcell+platelet+age,bmt,cause=2) summary(survivalG(fg2,bmt,time=50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.2127 0.02314 0.1674 0.2581 3.757e-20 #> risk1   0.3336 0.06799 0.2003 0.4668 9.281e-07 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err     2.5%  97.5% P-value #> ps0   0.1208 0.07189 -0.02009 0.2617 0.09285 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err         2.5%     97.5%   P-value #> [ps0] 0.4497465 0.2313601 -0.003710973 0.9032039 0.0519046 #> ratio:  #>  Estimate      2.5%     97.5%  #> 1.5679146 0.9962959 2.4674960  cif1time <- survivalGtime(fg1,bmt) plot(cif1time,type=\"risk\"); ss <- phreg(Surv(time,event)~tcell+platelet+age,bmt) sss <- survivalG(ss,bmt,time=50) summary(sss) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%    P-value #> risk0   0.6539 0.02709 0.6008 0.7070 9.218e-129 #> risk1   0.5640 0.05971 0.4470 0.6811  3.531e-21 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err    2.5%   97.5% P-value #> ps0 -0.08992  0.0629 -0.2132 0.03337  0.1529 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%      97.5%   P-value #> [ps0] -0.1479231 0.1095247 -0.3625876 0.06674132 0.1768263 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.8624974 0.6958733 1.0690189  #>  #> Average Treatment effect:  survival-difference (G-estimator) : #>       Estimate    Std.Err        2.5%     97.5%   P-value #> ps0 0.08991862 0.06290398 -0.03337092 0.2132082 0.1528725 #>  #> Average Treatment effect: 1-G (survival)-ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err       2.5%     97.5%   P-value #> [ps0] 0.2309818 0.1503867 -0.0637708 0.5257343 0.1245583 #> ratio:  #> Estimate     2.5%    97.5%  #> 1.259836 0.938220 1.691701  Gtime <- survivalGtime(ss,bmt) plot(Gtime)"},{"path":"http://kkholst.github.io/mets/articles/survival-ate.html","id":"g-computation-for-the-binomial-regression","dir":"Articles","previous_headings":"","what":"G-computation for the binomial regression","title":"G-Computation or standardization for the Cox, Fine-Gray and binomial regression models for survival data","text":"compare similar estimates using Doubly Robust estimating equations using binregATE. standardization G-computation can also computed using specialized function takes less memory quicker (large data). using specialized function","code":"## survival situation sr1 <- binregATE(Event(time,event)~tcell+platelet+age,bmt,cause=1,          time=40, treat.model=tcell~platelet+age) summary(sr1) #>    n events #>  408    241 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  0.676409  0.137007  0.407880  0.944939  0.0000 #> tcell1      -0.023675  0.346994 -0.703770  0.656420  0.9456 #> platelet    -0.492952  0.246158 -0.975412 -0.010492  0.0452 #> age          0.343939  0.115561  0.117444  0.570434  0.0029 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  1.96680 1.50363 2.5727 #> tcell1       0.97660 0.49472 1.9279 #> platelet     0.61082 0.37704 0.9896 #> age          1.41049 1.12462 1.7690 #>  #> Average Treatment effects (G-formula) : #>             Estimate    Std.Err       2.5%      97.5% P-value #> treat0     0.6230976  0.0273827  0.5694284  0.6767667  0.0000 #> treat1     0.6177595  0.0731712  0.4743466  0.7611723  0.0000 #> treat:1-0 -0.0053381  0.0783973 -0.1589940  0.1483179  0.9457 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.622698  0.027460  0.568878  0.676518   0.000 #> treat1     0.637785  0.085242  0.470714  0.804857   0.000 #> treat:1-0  0.015087  0.089442 -0.160215  0.190389   0.866  ## relative risk effect  estimate(coef=sr1$riskDR,vcov=sr1$var.riskDR,f=function(p) p[2]/p[1],null=1) #>          Estimate Std.Err   2.5% 97.5% P-value #> [treat1]    1.024   0.144 0.7421 1.306  0.8664 #>  #>  Null Hypothesis:  #>   [treat1] = 1  ## competing risks  br1 <- binregATE(Event(time,cause)~tcell+platelet+age,bmt,cause=1,          time=40,treat.model=tcell~platelet+age) summary(br1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.191519  0.130883 -0.448044  0.065007  0.1434 #> tcell1      -0.712880  0.351489 -1.401786 -0.023974  0.0425 #> platelet    -0.531919  0.244495 -1.011119 -0.052718  0.0296 #> age          0.432939  0.107314  0.222607  0.643271  0.0001 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.82570 0.63888 1.0672 #> tcell1       0.49023 0.24616 0.9763 #> platelet     0.58748 0.36381 0.9486 #> age          1.54178 1.24933 1.9027 #>  #> Average Treatment effects (G-formula) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.417746  0.027030  0.364768  0.470724  0.0000 #> treat1     0.267097  0.061849  0.145874  0.388319  0.0000 #> treat:1-0 -0.150649  0.067578 -0.283100 -0.018199  0.0258 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.417121  0.027112  0.363983  0.470259  0.0000 #> treat1     0.227776  0.061240  0.107748  0.347803  0.0002 #> treat:1-0 -0.189345  0.066600 -0.319878 -0.058812  0.0045 br1 <- binreg(Event(time,cause)~tcell+platelet+age,bmt,cause=1,time=40) Gbr1 <- binregG(br1,bmt,Avalues=NULL) summary(Gbr1) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.4177 0.02703 0.3648 0.4707 6.988e-54 #> risk1   0.2671 0.06185 0.1459 0.3883 1.571e-05 #>  #> Average Treatment effect: difference (G-estimator) : #>    Estimate Std.Err    2.5%   97.5% P-value #> pa  -0.1506 0.06758 -0.2831 -0.0182  0.0258 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err       2.5%      97.5%    P-value #> [pa] -0.4472628 0.2406332 -0.9188953 0.02436964 0.06307095 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.6393758 0.3989595 1.0246690  ## contrasting average age to 1+2-sd age, Avalues Gbr2 <- binregG(br1,bmt,varname=\"age\",Avalues=c(0,1,2)) summary(Gbr2) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.3932 0.02537 0.3434 0.4429 3.738e-54 #> risk1   0.4964 0.03655 0.4248 0.5681 5.044e-42 #> risk2   0.5997 0.05531 0.4913 0.7081 2.136e-27 #>  #> Average Treatment effect: difference (G-estimator) : #>      Estimate Std.Err    2.5%  97.5%   P-value #> pa     0.1033 0.02605 0.05222 0.1543 7.345e-05 #> pa.1   0.2066 0.04996 0.10863 0.3045 3.564e-05 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>       Estimate    Std.Err      2.5%     97.5%      P-value #> [pa] 0.2332376 0.05402806 0.1273445 0.3391307 1.581845e-05 #> [pa] 0.4222406 0.08691473 0.2518908 0.5925903 1.185167e-06 #> ratio:  #>      Estimate     2.5%    97.5% #> [pa] 1.262681 1.135808 1.403727 #> [pa] 1.525375 1.286456 1.808667"},{"path":"http://kkholst.github.io/mets/articles/survival-ate.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"G-Computation or standardization for the Cox, Fine-Gray and binomial regression models for survival data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/time-to-event-family-studies-arev.html","id":"survival","dir":"Articles","previous_headings":"","what":"Survival","title":"A practical guide to Human Genetics with Lifetime Data","text":"assumption random effects acting independently different cause specific hazards can analyse competing risks data considering cause-specific hazard. Typically, can questionable cumulative incidence modelling rely assumption. consider cause specific hazard cancer competing risks model death cancer. First estimating marginal hazards country.  see marginal Denmark particular quite different. fit two-stage random effects models country specific marginals random-effects variances differ MZ DZ twins. different random effects MZ DZ dependence MZ twins much stronger, summarized variance 5.425.42 contrast DZDZ variance 1.321.32. Now look polygenic modelling survival data, applied cause specific hazards. DE model fits quite well. summary shared variance due genes suggestion shared environmental effect.","code":"# Marginal Cox model here stratified on country without covariates   margph <- phreg(Surv(time,cancer)~strata(country)+cluster(id),data=prt)  plot(margph) # Clayton-Oakes, MLE , overall variance fitco1<-twostageMLE(margph,data=prt,theta=2.7) summary(fitco1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE        z        P-val Kendall tau        SE #> dependence1 2.782962 0.4225572 6.586001 4.518319e-11   0.5818491 0.0369421 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" fitco2 <- survival.twostage(margph,data=prt,theta=2.7,clusters=prt$id,var.link=0) summary(fitco2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE        z        P-val Kendall tau         SE #> dependence1 2.782962 0.4225529 6.586069 4.516254e-11   0.5818491 0.03694172 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" mm <- model.matrix(~-1+factor(zyg),prt)  fitco3<-twostageMLE(margph,data=prt,theta=1,theta.des=mm) summary(fitco3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                  Coef.        SE        z        P-val Kendall tau         SE #> factor(zyg)DZ 1.318966 0.3861577 3.415614 6.363831e-04   0.3974027 0.07011148 #> factor(zyg)MZ 5.421921 0.9626267 5.632423 1.776956e-08   0.7305280 0.03495065 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" fitco4 <- survival.twostage(margph,data=prt,theta=1,clusters=prt$id,var.link=0,theta.des=mm) summary(fitco4) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                  Coef.        SE        z        P-val Kendall tau         SE #> factor(zyg)DZ 1.318966 0.3861745 3.415466 6.367306e-04   0.3974027 0.07011454 #> factor(zyg)MZ 5.421920 0.9625027 5.633148 1.769493e-08   0.7305280 0.03494615 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  round(estimate(coef=fitco4$coef,vcov=fitco4$var.theta)$coefmat[,c(1,3:4)],2) #>               Estimate 2.5% 97.5% #> factor(zyg)DZ     1.32 0.56  2.08 #> factor(zyg)MZ     5.42 3.54  7.31   ## mz kendalls tau  kendall.ClaytonOakes.twin.ace(fitco4$theta[2],0,K=1000)$mz.kendall #> [1] 0.7380818  ## dz kendalls tau  kendall.ClaytonOakes.twin.ace(fitco4$theta[1],0,K=1000)$mz.kendall #> [1] 0.4154807 ### setting up design for random effects and parameters of random effects  desace <- twin.polygen.design(prt,type=\"ace\")   ### ace model   fitace <- survival.twostage(margph,data=prt,theta=1,        clusters=prt$id,var.link=0,model=\"clayton.oakes\",        numDeriv=1,random.design=desace$des.rv,theta.des=desace$pardes) summary(fitace) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.       SE         z        P-val Kendall tau          SE #> dependence1  7.223855 1.841525  3.922757 8.754161e-05   0.7831709  0.04328951 #> dependence2 -1.582396 1.088423 -1.453843 1.459898e-01  -3.7892257 12.48240540 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%   97.5%   P-value #> [1,]   1.2805  0.1698  0.9477 1.61324 4.615e-14 #> [2,]  -0.2805  0.1698 -0.6132 0.05226 9.850e-02 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    5.641  0.9894 3.702 7.581 1.184e-08 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" ### ace model with positive random effects variances  # fitacee <- survival.twostage(margph,data=prt,theta=1,  #      clusters=prt$id,var.link=1,model=\"clayton.oakes\",  #      numDeriv=1,random.design=desace$des.rv,theta.des=desace$pardes)  #summary(fitacee)    ### ae model   #desae <- twin.polygen.design(prt,type=\"ae\")  #fitae <- survival.twostage(margph,data=prt,theta=1,  #      clusters=prt$id,var.link=0,model=\"clayton.oakes\",  #      numDeriv=1,random.design=desae$des.rv,theta.des=desae$pardes)  #summary(fitae)   ### de model   desde <- twin.polygen.design(prt,type=\"de\")  fitde <- survival.twostage(margph,data=prt,theta=1,clusters=prt$id,var.link=0,model=\"clayton.oakes\", numDeriv=1,random.design=desde$des.rv,theta.des=desde$pardes) summary(fitde) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE        z        P-val Kendall tau         SE #> dependence1 5.940643 0.9837336 6.038873 1.551941e-09   0.7481312 0.03120299 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err 2.5% 97.5% P-value #> [1,]        1       0    1     1       0 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    5.941  0.9837 4.013 7.869 1.552e-09 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/time-to-event-family-studies-arev.html","id":"concordance-and-casewise","dir":"Articles","previous_headings":"","what":"Concordance and Casewise","title":"A practical guide to Human Genetics with Lifetime Data","text":"First estimate concordance joint prostate cancer. two-twins censored time, otherwise enforce data artificially censor twins first censoring time. Given, however, -censoring assumption satisfied can stanadar Aalen-Johansen product-limit estimator concordance probabilities MZ DZ twins. simplicity country even though show big differences countries.   Now compare concordance marginals get measure takes marginals account evaluating strength association.   standard errors slightly since reflect uncertainty concordance estimation. can improved specific calculations specific time-point uisng binomial regression function gives iid decomposition paramters. thus apply binomial regression estimate concordance well marginal, combine iid decompositions estimating standard error. also ignoring country differences. can useful also simply model concordance given covariates, case might find important adjust country, see differences MZ DZ comparable across contries even though clearly DK much lower cumulative incidence prostate cancer.","code":"prt <-  force.same.cens(prt,cause=\"status\")   dtable(prt,~status+cancer) #>  #>        cancer     0     1 #> status                    #> 0             21580    77 #> 1              6700     0 #> 2                 0   865  dtable(prt,~status+country) #>  #>        country Denmark Finland Norway Sweden #> status                                       #> 0                 7394    2622   3154   8487 #> 1                 2140    1138    830   2592 #> 2                  137     166    123    439  dtable(prt,~zyg+country) #>  #>     country Denmark Finland Norway Sweden #> zyg                                       #> DZ             6191    2833   2393   6574 #> MZ             3480    1093   1714   4944 ## cumulative incidence with cluster standard errors.  cif1 <- cif(Event(time,status)~strata(country)+cluster(id),prt,cause=2)  plot(cif1,se=1) cifa <- cif(Event(time,status)~+1,prt,cause=2)   ### concordance estimator, ignoring country differences.   p11 <- bicomprisk(Event(time,status)~strata(zyg)+id(id),data=prt,cause=c(2,2)) #> Strata 'DZ' #> Strata 'MZ' p11mz <- p11$model$\"MZ\" p11dz <- p11$model$\"DZ\" par(mfrow=c(1,2))  ## Concordance  plot(p11mz,ylim=c(0,0.1));  plot(p11dz,ylim=c(0,0.1)); library(prodlim)  outm <- prodlim(Hist(time,status)~+1,data=prt)   cifzyg <- cif(Event(time,status)~+strata(zyg)+cluster(id),data=prt,cause=2)  cifprt <- cif(Event(time,status)~country+cluster(id),data=prt,cause=2)        times <- 70:100  cifmz <- predict(outm,cause=2,time=times,newdata=data.frame(zyg=\"MZ\")) ## cause is 2 (second cause)   cifdz <- predict(outm,cause=2,time=times,newdata=data.frame(zyg=\"DZ\"))       ### concordance for MZ and DZ twins<  cc <- bicomprisk(Event(time,status)~strata(zyg)+id(id),data=prt,cause=c(2,2),prodlim=TRUE) #> Strata 'DZ' #> Strata 'MZ'  ccdz <- cc$model$\"DZ\"  ccmz <- cc$model$\"MZ\"        cdz <- casewise(ccdz,outm,cause.marg=2)   cmz <- casewise(ccmz,outm,cause.marg=2)   dd <- bicompriskData(Event(time,status)~country+strata(zyg)+id(id),data=prt,cause=c(2,2))  conczyg <- cif(Event(time,status)~strata(zyg)+cluster(id),data=dd,cause=1)   par(mfrow=c(1,2))  plot(conczyg,se=TRUE,col=cols[2:1], lty=ltys[2:1], legend=FALSE,xlab=\"Age\",ylab=\"Concordance\")  legend(\"topleft\",c(\"concordance-MZ\",\"concordance-DZ\"),col=cols[1:2],lty=ltys[1:2])   plot(cmz,ci=NULL,ylim=c(0,.8),xlim=c(70,97),legend=FALSE,col=cols[c(1,3,3)],lty=ltys[c(1,3,3)],       ylab=\"Casewise\",xlab=\"Age\")   plot(cdz,ci=NULL,ylim=c(0,.8),xlim=c(70,97),legend=FALSE,ylab=\"Casewise\",xlab=\"Age\",       col=c(cols[2],NA,NA), lty=ltys[c(2,3,3)], add=TRUE)  with(data.frame(cmz$casewise),plotConfRegionSE(time,casewise.conc,se.casewise,col=cols[1]))  with(data.frame(cdz$casewise),plotConfRegionSE(time,casewise.conc,se.casewise,col=cols[2]))  legend(\"topleft\",c(\"casewise-MZ\",\"casewise-DZ\",\"marginal\"),col=cols, lty=ltys, bg=\"white\") summary(cdz) #> Casewise concordance and standard errors  #>        time casewise conc se casewise #>  [1,]  59.5        0.0866      0.0865 #>  [2,]  60.5        0.0659      0.0659 #>  [3,]  61.6        0.0593      0.0593 #>  [4,]  62.7        0.0483      0.0483 #>  [5,]  63.7        0.0358      0.0358 #>  [6,]  64.8        0.0279      0.0279 #>  [7,]  65.8        0.0223      0.0223 #>  [8,]  66.9        0.0197      0.0197 #>  [9,]  68.0        0.0415      0.0297 #> [10,]  69.0        0.0335      0.0240 #> [11,]  70.1        0.0452      0.0264 #> [12,]  71.1        0.0855      0.0352 #> [13,]  72.2        0.0728      0.0300 #> [14,]  73.2        0.0888      0.0317 #> [15,]  74.3        0.1010      0.0321 #> [16,]  75.4        0.1020      0.0310 #> [17,]  76.4        0.1130      0.0318 #> [18,]  77.5        0.1230      0.0320 #> [19,]  78.5        0.1400      0.0334 #> [20,]  79.6        0.1470      0.0332 #> [21,]  80.7        0.1530      0.0329 #> [22,]  81.7        0.1460      0.0307 #> [23,]  82.8        0.1470      0.0298 #> [24,]  83.8        0.1600      0.0307 #> [25,]  84.9        0.1470      0.0282 #> [26,]  86.0        0.1620      0.0297 #> [27,]  87.0        0.1680      0.0300 #> [28,]  88.1        0.1820      0.0311 #> [29,]  89.1        0.1760      0.0301 #> [30,]  90.2        0.1950      0.0323 #> [31,]  91.2        0.2040      0.0332 #> [32,]  92.3        0.1970      0.0321 #> [33,]  93.4        0.1940      0.0315 #> [34,]  94.4        0.1970      0.0318 #> [35,]  95.5        0.1940      0.0314 #> [36,]  96.5        0.1930      0.0312 #> [37,]  97.6        0.2040      0.0330 #> [38,]  98.7        0.2010      0.0325 #> [39,]  99.7        0.1990      0.0322 #> [40,] 101.0        0.1980      0.0321 #> [41,] 102.0        0.1950      0.0316 #> [42,] 103.0        0.1940      0.0314 #> [43,] 104.0        0.1940      0.0314 #> [44,] 105.0        0.1930      0.0312 #> [45,] 106.0        0.1920      0.0311 #> [46,] 107.0        0.1920      0.0311 #> [47,] 108.0        0.1910         NaN  summary(cmz) #> Casewise concordance and standard errors  #>        time casewise conc se casewise #>  [1,]  60.6         0.519      0.2590 #>  [2,]  61.6         0.466      0.2330 #>  [3,]  62.7         0.380      0.1900 #>  [4,]  63.7         0.285      0.1420 #>  [5,]  64.8         0.286      0.1280 #>  [6,]  65.8         0.228      0.1020 #>  [7,]  66.9         0.295      0.1120 #>  [8,]  67.9         0.306      0.1090 #>  [9,]  68.9         0.327      0.1040 #> [10,]  70.0         0.338      0.0981 #> [11,]  71.0         0.345      0.0926 #> [12,]  72.1         0.399      0.0946 #> [13,]  73.1         0.414      0.0909 #> [14,]  74.2         0.426      0.0874 #> [15,]  75.2         0.388      0.0798 #> [16,]  76.3         0.391      0.0773 #> [17,]  77.3         0.410      0.0769 #> [18,]  78.4         0.392      0.0723 #> [19,]  79.4         0.410      0.0721 #> [20,]  80.5         0.423      0.0714 #> [21,]  81.5         0.400      0.0666 #> [22,]  82.6         0.442      0.0685 #> [23,]  83.6         0.446      0.0676 #> [24,]  84.7         0.433      0.0643 #> [25,]  85.7         0.413      0.0612 #> [26,]  86.8         0.389      0.0578 #> [27,]  87.8         0.396      0.0578 #> [28,]  88.9         0.396      0.0573 #> [29,]  89.9         0.399      0.0574 #> [30,]  91.0         0.386      0.0556 #> [31,]  92.0         0.400      0.0570 #> [32,]  93.1         0.393      0.0560 #> [33,]  94.1         0.415      0.0590 #> [34,]  95.2         0.477      0.0669 #> [35,]  96.2         0.493      0.0690 #> [36,]  97.3         0.511      0.0714 #> [37,]  98.3         0.507      0.0708 #> [38,]  99.4         0.500      0.0699 #> [39,] 100.0         0.525      0.0739 #> [40,] 101.0         0.520      0.0731 #> [41,] 103.0         0.514      0.0723 #> [42,] 104.0         0.541      0.0767 #> [43,] 105.0         0.541      0.0767   cpred(cmz$casewise,c(70,80)) #>      new.time casewise conc se casewise #> [1,]       70     0.3381311  0.09811729 #> [2,]       80     0.4096715  0.07211013  cpred(cdz$casewise,c(70,80)) #>      new.time casewise conc se casewise #> [1,]       70    0.03351513  0.02398337 #> [2,]       80    0.14694869  0.03321234 dd <- bicompriskData(Event(time,status)~country+strata(zyg)+id(id),data=prt,cause=c(2,2))  conczyg <- cif(Event(time,status)~strata(zyg)+cluster(id),data=dd,cause=1)   par(mfrow=c(1,2))  plot(conczyg,se=TRUE,legend=FALSE,xlab=\"Age\",ylab=\"Concordance\")  legend(\"topleft\",c(\"concordance-DZ\",\"concordance-MZ\"),col=c(1,2),lty=1)  plot(cmz,ci=NULL,ylim=c(0,0.6),xlim=c(70,100),legend=FALSE,col=c(2,3,3),ylab=\"Casewise\",xlab=\"Age\",lty=c(1,3))  plot(cdz,ci=NULL,ylim=c(0,0.6),xlim=c(70,100),legend=FALSE,ylab=\"Casewise\",xlab=\"Age\",       col=c(1,3,3), add=TRUE, lty=c(2,3))  legend(\"topleft\",c(\"casewise-MZ\",\"casewise-DZ\",\"marginal\"),col=c(2,1,3),lty=1)  with(data.frame(cmz$casewise),plotConfRegionSE(time,casewise.conc,se.casewise,col=2))  with(data.frame(cdz$casewise),plotConfRegionSE(time,casewise.conc,se.casewise,col=1)) ### new version of Casewise for specific time-point based on binreg   dd <- bicompriskData(Event(time,status)~country+strata(zyg)+id(id),data=prt,cause=c(2,2))  newdata <- data.frame(zyg=c(\"DZ\",\"MZ\"),id=1)   ## concordance   bcif1 <- binreg(Event(time,status)~-1+factor(zyg)+cluster(id),dd,time=80,cause=1,cens.model=~strata(zyg))  pconc <- predict(bcif1,newdata)   ## marginal estimates  mbcif1 <- binreg(Event(time,status)~cluster(id),prt,time=80,cause=2)  mc <- predict(mbcif1,newdata)   ### casewise with improved se's from log-scale   cse <- binregCasewise(bcif1,mbcif1) cse  #> $coef #>     Estimate      2.5%     97.5% #> p1 0.1586277 0.1445496 0.1740770 #> p2 0.4041311 0.3682646 0.4434908 #>  #> $logcoef #>    Estimate Std.Err   2.5%   97.5%   P-value #> p1   -1.841 0.04742 -1.934 -1.7483 0.000e+00 #> p2   -0.906 0.04742 -0.999 -0.8131 2.208e-81 ### semi-parametric modelling of concordance   dd <- bicompriskData(Event(time,status)~country+strata(zyg)+id(id),data=prt,cause=c(2,2))  regconc <- cifreg(Event(time,status)~country*zyg,data=dd,prop=NULL)  regconc  ### interaction test  wald.test(regconc,coef.null=5:7)   regconc <- cifreg(Event(time,status)~country+zyg,data=dd,prop=NULL)  regconc   ## logistic link   logitregconc <- cifreg(Event(time,status)~country+zyg,data=dd)  slr <- summary(logitregconc) slr #>  #>      n events #>  14222    106 #>  #>  14222 clusters #> coeffients: #>                Estimate    S.E. dU^-1/2 P-value #> countryFinland  1.30427 0.35262 0.35146  0.0002 #> countryNorway   0.94077 0.39999 0.39365  0.0187 #> countrySweden   1.08494 0.32247 0.31871  0.0008 #> zygMZ           1.02335 0.20283 0.19873  0.0000 #>  #> exp(coeffients): #>                Estimate   2.5%  97.5% #> countryFinland   3.6850 1.8462 7.3552 #> countryNorway    2.5619 1.1698 5.6111 #> countrySweden    2.9592 1.5729 5.5676 #> zygMZ            2.7825 1.8697 4.1408 ### library(Publish) ### publish(round(slr$exp.coef[,-c(2,5)],2),latex=TRUE,digits=2)"},{"path":"http://kkholst.github.io/mets/articles/time-to-event-family-studies-arev.html","id":"competing-risk-using-additive-gamma","dir":"Articles","previous_headings":"","what":"Competing risk using additive Gamma","title":"A practical guide to Human Genetics with Lifetime Data","text":"cumulative incidence random effects modelling (commented avoid timereg dependence)","code":"timereg <- 0 if (timereg==1) {   times <- seq(50,90,length.out=5)   cif1 <- timereg::comp.risk(Event(time,status)~-1+factor(country)+cluster(id),prt,            cause=2,times=times,max.clust=NULL)    mm <- model.matrix(~-1+factor(zyg),prt)   out1<-random.cif(cif1,data=prt,cause1=2,cause2=2,theta=1,           theta.des=mm,same.cens=TRUE,step=0.5)   summary(out1)   round(estimate(coef=out1$theta,vcov=out1$var.theta)$coefmat[,c(1,3:4)],2)    desace <- twin.polygen.design(prt,type=\"ace\")     outacem <- Grandom.cif(cif1,data=prt,cause1=2,cause2=2,      same.cens=TRUE,theta=c(0.45,0.15),var.link=0,          step=0.5,theta.des=desace$pardes,random.design=desace$des.rv)   ##outacem$score } timereg <- 0 if (timereg==1) {   summary(outacem)   ###  variances  estimate(coef=outacem$theta,vcov=outacem$var.theta,f=function(p) p/sum(p)^2)   ## AE polygenic model  # desae <- twin.polygen.design(prt,type=\"ae\")  # outaem <- Grandom.cif(cif1,data=prt,cause1=2,cause2=2,  #    same.cens=TRUE,theta=c(0.45,0.15),var.link=0,  #        step=0.5,theta.des=desae$pardes,random.design=desae$des.rv)  # outaem$score  # summary(outaem)  # estimate(coef=outaem$theta,vcov=outaem$var.theta,f=function(p)     p/sum(p)^2)   ## AE polygenic model  # desde <- twin.polygen.design(prt,type=\"de\")  # outaem <- Grandom.cif(cif1,data=prt,cause1=2,cause2=2,  #   same.cens=TRUE,theta=c(0.35),var.link=0,  #   step=0.5,theta.des=desde$pardes,random.design=desde$des.rv)  # outaem$score  # summary(outaem)  # estimate(coef=outaem$theta,vcov=outaem$var.theta,f=function(p) p/sum(p)^2)    times <- 90   cif1 <- timereg::comp.risk(Event(time,status)~-1+factor(country)+cluster(id),prt,            cause=2,times=times,max.clust=NULL)    mm <- model.matrix(~-1+factor(zyg),prt)   out1<-random.cif(cif1,data=prt,cause1=2,cause2=2,theta=1,           theta.des=mm,same.cens=TRUE,step=0.5)   summary(out1)   round(estimate(coef=out1$theta,vcov=out1$var.theta)$coefmat[,c(1,3:4)],2)   desde <- twin.polygen.design(prt,type=\"de\")  outaem <- Grandom.cif(cif1,data=prt,cause1=2,cause2=2,     same.cens=TRUE,theta=c(0.35),var.link=0,         step=0.5,theta.des=desde$pardes,random.design=desde$des.rv)  outaem$score  summary(outaem)  estimate(coef=outaem$theta,vcov=outaem$var.theta,f=function(p) p/sum(p)^2) }"},{"path":"http://kkholst.github.io/mets/articles/time-to-event-family-studies-arev.html","id":"competing-risk-modeling-using-the-liabilty-threshold-model","dir":"Articles","previous_headings":"","what":"Competing risk modeling using the Liabilty Threshold model","title":"A practical guide to Human Genetics with Lifetime Data","text":"First fit bivariate probit model (marginals MZ DZ twins different correlation parameter). evaluate risk getting cancer last double cancer event (95 years) Liability threshold model ACE random effects structure case ACE model fits data well - fact indistinguishable flexible bivariate Probit model seen IPCW weighted AIC measure ACE model marginal adjusted country  Bivariate probit model time different time points  Bivariate probit model adjusting country","code":"rm(prt) data(prt) prt0 <-  force.same.cens(prt, cause=\"status\", cens.code=0, time=\"time\", id=\"id\") prt0$country <- relevel(prt0$country, ref=\"Sweden\") prt_wide <- fast.reshape(prt0, id=\"id\", num=\"num\", varying=c(\"time\",\"status\",\"cancer\")) prt_time <- subset(prt_wide,  cancer1 & cancer2, select=c(time1, time2, zyg)) tau <- 95 tt <- seq(70, tau, length.out=5) ## Time points to evaluate model in b0 <- bptwin.time(cancer ~ 1, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"cor\",               cens.formula=Surv(time,status==0)~zyg, breaks=tau) summary(b0) #>  #>                Estimate   Std.Err        Z   p-value     #> (Intercept)   -1.348188  0.026276 -51.3086 < 2.2e-16 *** #> atanh(rho) MZ  0.735992  0.087838   8.3789 < 2.2e-16 *** #> atanh(rho) DZ  0.353023  0.068234   5.1737 2.295e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  1994/3618   997/1809             #>  #>                            Estimate 2.5%    97.5%   #> Tetrachoric correlation MZ 0.62672  0.51081 0.72024 #> Tetrachoric correlation DZ 0.33905  0.21584 0.45164 #>  #> MZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.03504  0.02779 0.04409 #> Casewise Concordance 0.39458  0.31876 0.47584 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       4.44351  3.50521 5.38182 #> log(OR)              2.34131  1.87105 2.81157 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.01952  0.01449 0.02625 #> Casewise Concordance 0.21983  0.16667 0.28415 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       2.47556  1.81095 3.14016 #> log(OR)              1.23088  0.81020 1.65156 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.57533  0.25790 0.89276 #>  #>  #> Event of interest before time 95 b1 <- bptwin.time(cancer ~ 1, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"ace\",               cens.formula=Surv(time,status==0)~zyg, breaks=tau) summary(b1) #>  #>             Estimate  Std.Err        Z p-value     #> (Intercept) -2.20664  0.16463 -13.4034  <2e-16 *** #> log(var(A))  0.43260  0.39149   1.1050  0.2691     #> log(var(C)) -1.98290  2.52343  -0.7858  0.4320     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  1994/3618   997/1809             #>  #>                    Estimate 2.5%     97.5%    #> A                   0.57533  0.25790  0.89276 #> C                   0.05139 -0.20836  0.31114 #> E                   0.37328  0.26874  0.47782 #> MZ Tetrachoric Cor  0.62672  0.51081  0.72024 #> DZ Tetrachoric Cor  0.33905  0.21584  0.45164 #>  #> MZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.03504  0.02779 0.04409 #> Casewise Concordance 0.39458  0.31876 0.47584 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       4.44351  3.50521 5.38182 #> log(OR)              2.34131  1.87105 2.81157 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.01952  0.01449 0.02625 #> Casewise Concordance 0.21983  0.16667 0.28415 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       2.47556  1.81096 3.14017 #> log(OR)              1.23088  0.81020 1.65156 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.57533  0.25790 0.89276 #>  #>  #> Event of interest before time 95 AIC(b0, b1) #>    df AIC #> b0  3   6 #> b1  3   6 b2 <- bptwin.time(cancer ~ country, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"ace\",               cens.formula=Surv(time,status==0)~zyg+country, breaks=95) summary(b2) #>  #>                Estimate  Std.Err        Z   p-value     #> (Intercept)    -1.97165  0.15371 -12.8267 < 2.2e-16 *** #> countryDenmark -0.72489  0.11920  -6.0812 1.193e-09 *** #> countryFinland  0.18968  0.12518   1.5152    0.1297     #> countryNorway  -0.11611  0.16621  -0.6986    0.4848     #> log(var(A))     0.40388  0.40524   0.9966    0.3189     #> log(var(C))    -3.88761 17.56413  -0.2213    0.8248     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  1994/3618   997/1809             #>  #>                    Estimate 2.5%     97.5%    #> A                   0.59474  0.25169  0.93779 #> C                   0.00814 -0.27297  0.28925 #> E                   0.39712  0.28435  0.50989 #> MZ Tetrachoric Cor  0.60288  0.47809  0.70381 #> DZ Tetrachoric Cor  0.30551  0.17238  0.42767 #>  #> MZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.04295  0.03307 0.05561 #> Casewise Concordance 0.40128  0.32263 0.48535 #> Marginal             0.10703  0.09453 0.12096 #> Rel.Recur.Risk       3.74923  2.94156 4.55691 #> log(OR)              2.15979  1.67935 2.64023 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.02439  0.01747 0.03396 #> Casewise Concordance 0.22788  0.17060 0.29749 #> Marginal             0.10703  0.09453 0.12096 #> Rel.Recur.Risk       2.12912  1.54508 2.71315 #> log(OR)              1.06262  0.62460 1.50064 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.59474  0.25169 0.93779 #>  #>  #> Event of interest before time 95 bt0 <- bptwin.time(cancer ~ 1, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"ace\",                cens.formula=Surv(time,status==0)~zyg,               summary.function=function(x) x, breaks=tt) h2 <- Reduce(rbind, lapply(bt0$coef, function(x) x$heritability))[,c(1,3,4),drop=FALSE] concMZ <- Reduce(rbind, lapply(bt0$coef, function(x) x$probMZ[\"Concordance\",,drop=TRUE])) par(mfrow=c(1,2)) plot(tt, h2[,1], type=\"s\", lty=1, col=cols[3], xlab=\"Age\", ylab=\"Heritability\", ylim=c(0,1)) lava::confband(tt, h2[,2], h2[,3],polygon=TRUE, step=TRUE, col=lava::Col(cols[3], 0.1), border=NA) plot(tt, concMZ[,1], type=\"s\", lty=1, col=cols[1], xlab=\"Age\", ylab=\"Concordance\", ylim=c(0,.1)) lava::confband(tt, concMZ[,2], concMZ[,3],polygon=TRUE, step=TRUE, col=lava::Col(cols[1], 0.1), border=NA) system.time(a.mz <- biprobit.time(cancer~1, id=\"id\", data=subset(prt0, zyg==\"MZ\"),                                cens.formula = Surv(time,status==0)~1, pairs.only=TRUE,                                 breaks=tt)) #>    user  system elapsed  #>   0.284   0.272   0.227 system.time(a.dz <- biprobit.time(cancer~1, id=\"id\", data=subset(prt0, zyg==\"DZ\"),                                cens.formula = Event(time,status==0)~1, pairs.only=TRUE,                                breaks=tt)) #>    user  system elapsed  #>   0.367   0.285   0.323  #system.time(a.zyg <- biprobit.time(cancer~1, rho=~1+zyg, id=\"id\", data=prt,  #                               cens.formula = Event(time,status==0)~1, #                               eqmarg=FALSE, fix.cens.weight #                               breaks=seq(75,100,by=10)))  a.mz #>                            #>  1:Concordance             #>  2:Casewise Concordance    #>  3:Marginal                #>  4:Rel.Recur.Risk          #>  5:OR                      #>  6:Tetrachoric correlation #>  #>       Time 1:Concor... 2:Casewi... 3:Marginal 4:Rel.Re...    5:OR 6:Tetrac... #> [1,] 70.00      0.0049      0.2976     0.0166     17.9645 35.3860      0.6973 #> [2,] 76.25      0.0125      0.3468     0.0362      9.5939 21.1447      0.6834 #> [3,] 82.50      0.0247      0.3759     0.0656      5.7307 13.1452      0.6481 #> [4,] 88.75      0.0308      0.3675     0.0839      4.3780  9.4430      0.5993 #> [5,] 95.00      0.0409      0.4144     0.0988      4.1952 10.3179      0.6352 a.dz #>                            #>  1:Concordance             #>  2:Casewise Concordance    #>  3:Marginal                #>  4:Rel.Recur.Risk          #>  5:OR                      #>  6:Tetrachoric correlation #>  #>       Time 1:Concor... 2:Casewi... 3:Marginal 4:Rel.Re...   5:OR 6:Tetrac... #> [1,] 70.00      0.0007      0.0767     0.0088      8.6699 9.9965      0.3855 #> [2,] 76.25      0.0037      0.1612     0.0228      7.0632 9.6179      0.4682 #> [3,] 82.50      0.0074      0.1660     0.0445      3.7328 4.9289      0.3752 #> [4,] 88.75      0.0136      0.2001     0.0680      2.9417 4.0347      0.3614 #> [5,] 95.00      0.0174      0.2091     0.0831      2.5163 3.4242      0.3335  plot(conczyg,se=TRUE,legend=FALSE,xlab=\"Age\",ylab=\"Concordance\", ylim=c(0,0.07)) plot(a.mz, ylim=c(0,.07), col=cols[1], lty=ltys[1], legend=FALSE, add=TRUE) plot(a.dz, col=cols[2], lty=ltys[2], add=TRUE) a.mz_country <- biprobit.time(cancer~country, id=\"id\", data=subset(prt0, zyg==\"MZ\"),                                cens.formula = Surv(time,status==0)~country, pairs.only=TRUE,                                 breaks=tt) system.time(a.dz_country <- biprobit.time(cancer~country, id=\"id\", data=subset(prt0, zyg==\"DZ\"),                                cens.formula = Event(time,status==0)~country, pairs.only=TRUE,                                breaks=tt))  s_mz_country <- summary(a.mz_country) s_dz_country <- summary(a.dz_country) s_mz_country #> $Concordance #>    Time    Estimate        2.5%      97.5% #> 1 70.00 0.005921119 0.002986156 0.01170686 #> 2 76.25 0.014322088 0.008753641 0.02334935 #> 3 82.50 0.029210106 0.020110767 0.04224903 #> 4 88.75 0.040377886 0.029107601 0.05576133 #> 5 95.00 0.049707538 0.035563146 0.06907461 #>  #> $`Casewise Concordance` #>    Time  Estimate      2.5%     97.5% #> 1 70.00 0.3061371 0.1804979 0.4691630 #> 2 76.25 0.3502181 0.2491875 0.4667465 #> 3 82.50 0.3867062 0.3017293 0.4791916 #> 4 88.75 0.3906921 0.3129417 0.4744209 #> 5 95.00 0.4190261 0.3369792 0.5058097 #>  #> $Marginal #>    Time   Estimate       2.5%      97.5% #> 1 70.00 0.01934140 0.01265755 0.02944940 #> 2 76.25 0.04089476 0.02990124 0.05569812 #> 3 82.50 0.07553566 0.05950394 0.09544834 #> 4 88.75 0.10334964 0.08432130 0.12608075 #> 5 95.00 0.11862636 0.09701847 0.14427781 #>  #> $Rel.Recur.Risk #>    Time  Estimate     2.5%     97.5% #> 1 70.00 15.828074 6.402374 25.253774 #> 2 76.25  8.563886 5.232398 11.895375 #> 3 82.50  5.119518 3.659925  6.579110 #> 4 88.75  3.780295 2.838451  4.722138 #> 5 95.00  3.532318 2.733224  4.331412 #>  #> $OR #>    Time  Estimate      2.5%    97.5% #> 1 70.00 31.799055 12.251068 82.53810 #> 2 76.25 18.914711  9.701256 36.87835 #> 3 82.50 11.952397  7.033118 20.31244 #> 4 88.75  8.488885  5.238209 13.75683 #> 5 95.00  8.502483  5.276008 13.70207 #>  #> $`Tetrachoric correlation` #>    Time  Estimate      2.5%     97.5% #> 1 70.00 0.6943823 0.4989166 0.8226225 #> 2 76.25 0.6744434 0.5302710 0.7807066 #> 3 82.50 0.6416812 0.5173782 0.7394660 #> 4 88.75 0.5945675 0.4712828 0.6950631 #> 5 95.00 0.6079229 0.4838930 0.7080114 s_dz_country #> $Concordance #>    Time     Estimate         2.5%       97.5% #> 1 70.00 0.0009355048 0.0003014643 0.002899194 #> 2 76.25 0.0053848911 0.0030067721 0.009625755 #> 3 82.50 0.0090012330 0.0055138848 0.014661695 #> 4 88.75 0.0172555605 0.0117010082 0.025379187 #> 5 95.00 0.0221107866 0.0153703841 0.031711870 #>  #> $`Casewise Concordance` #>    Time   Estimate      2.5%     97.5% #> 1 70.00 0.08242727 0.0282508 0.2172686 #> 2 76.25 0.17830235 0.1097646 0.2763507 #> 3 82.50 0.16491567 0.1103378 0.2392308 #> 4 88.75 0.20763649 0.1517489 0.2773763 #> 5 95.00 0.21840901 0.1627051 0.2866548 #>  #> $Marginal #>    Time   Estimate        2.5%      97.5% #> 1 70.00 0.01134946 0.007572602 0.01697780 #> 2 76.25 0.03020090 0.022824941 0.03986318 #> 3 82.50 0.05458082 0.044179949 0.06725798 #> 4 88.75 0.08310466 0.069910511 0.09852516 #> 5 95.00 0.10123569 0.086502805 0.11815328 #>  #> $Rel.Recur.Risk #>    Time Estimate       2.5%     97.5% #> 1 70.00 7.262662 -0.6527461 15.178071 #> 2 76.25 5.903876  2.9616475  8.846104 #> 3 82.50 3.021495  1.8476374  4.195352 #> 4 88.75 2.498494  1.7411952  3.255792 #> 5 95.00 2.157431  1.5524847  2.762377 #>  #> $OR #>    Time Estimate     2.5%     97.5% #> 1 70.00 8.438374 2.374773 29.984401 #> 2 76.25 8.262989 4.176810 16.346681 #> 3 82.50 3.898758 2.289207  6.639992 #> 4 88.75 3.386745 2.150865  5.332756 #> 5 95.00 2.894681 1.876189  4.466063 #>  #> $`Tetrachoric correlation` #>    Time  Estimate      2.5%     97.5% #> 1 70.00 0.3735045 0.1024582 0.5929219 #> 2 76.25 0.4623453 0.2958921 0.6015477 #> 3 82.50 0.3333246 0.1903659 0.4624401 #> 4 88.75 0.3304659 0.1996083 0.4497427 #> 5 95.00 0.3013345 0.1715485 0.4208396 ## ACE model (time-varying) with and without adjustment for country a1 <- bptwin.time(cancer~1, id=\"id\", data=prt0, type=\"ace\",                               zyg=\"zyg\", DZ=\"DZ\",                                cens.formula=Surv(time,status==0)~zyg,                               breaks=tt)  #a2 <- bptwin.time(cancer~country, id=\"id\", data=prt0, #type=\"ace\", #                              zyg=\"zyg\", DZ=\"DZ\",  #                              #cens.formula=Surv(time,status==0)~country+zyg, #                              breaks=tt) plot(a.mz, which=c(6), xlab=\"Age\", ylab=\"Correlation\", ylim=c(0,1), col=cols[1], lty=ltys[1], legend=NULL, alpha=.1) plot(a.dz, which=c(6), col=cols[2], lty=ltys[2], legend=NULL, add=TRUE, alpha=.1) legend(\"topleft\", c(\"MZ tetrachoric correlation\", \"DZ tetrachoric correlation\"),        col=cols, lty=ltys, lwd=2) plot(a.mz, which=c(4), xlab=\"Age\", ylab=\"Relative Recurrence Risk\",      ylim=c(1,20), col=cols[1], lty=ltys[1], legend=NULL, lwd=2, alpha=.1) plot(a.dz, which=c(4), col=cols[2], lty=ltys[2], legend=NULL, add=TRUE, lwd=2, alpha=.1) legend(\"topright\", c(\"MZ relative recurrence risk\", \"DZ relative recurrence risk\"),        col=cols, lty=ltys, lwd=2) plot(a1, which=c(5,6), xlab=\"Age\", ylab=\"Correlation\", ylim=c(0,1), col=cols[1:2], lty=ltys[1:2], lwd=2, alpha=0.1,      legend=c(\"MZ tetrachoric correlation\", \"DZ tetrachoric correlation\")) plot(a1, which=c(1), xlab=\"Age\", ylim=c(0,1), col=\"black\", lty=1, ylab=\"Heritability\", legend=NULL, alpha=.1)"},{"path":"http://kkholst.github.io/mets/articles/time-to-event-family-studies-arev.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"A practical guide to Human Genetics with Lifetime Data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] prodlim_2025.04.28 mets_1.3.9         #>  #> loaded via a namespace (and not attached): #>  [1] Matrix_1.7-4        future.apply_1.20.1 jsonlite_2.0.0      #>  [4] compiler_4.5.2      Rcpp_1.1.0          parallel_4.5.2      #>  [7] jquerylib_0.1.4     globals_0.18.0      splines_4.5.2       #> [10] systemfonts_1.3.1   textshaping_1.0.4   yaml_2.3.12         #> [13] fastmap_1.2.0       lattice_0.22-7      R6_2.6.1            #> [16] knitr_1.51          htmlwidgets_1.6.4   future_1.68.0       #> [19] desc_1.4.3          bslib_0.9.0         rlang_1.1.6         #> [22] cachem_1.1.0        xfun_0.55           fs_1.6.6            #> [25] sass_0.4.10         cli_3.6.5           pkgdown_2.2.0       #> [28] digest_0.6.39       grid_4.5.2          mvtnorm_1.3-3       #> [31] lifecycle_1.0.4     lava_1.8.2          timereg_2.0.7       #> [34] evaluate_1.0.5      data.table_1.18.0   numDeriv_2016.8-1.1 #> [37] listenv_0.10.0      codetools_0.2-20    ragg_1.5.0          #> [40] survival_3.8-3      parallelly_1.46.0   rmarkdown_2.30      #> [43] tools_4.5.2         htmltools_0.5.9"},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Analysis of multivariate survival data","text":"looking multivariate survival data aim learning dependence present, possibly correcting covariates different approaches available mets package biprobit model regression structure dependence parameter additive gamma distributed random effects Special functionality polygenic random effects modelling ACE, ADE ,AE forth. regression structure dependence parameter Cluster stratified Cox Typically can hard impossible specify random effects models special structure among parameters random effects. possible specification random effects models. concrete model structure assume paired survival data (T1,δ1,T2,δ2,X1,X2)(T_1, \\delta_1, T_2, \\delta_2,  X_1, X_2) censored survival responses (T1,δ1,T2,δ2)(T_1, \\delta_1, T_2, \\delta_2) covariates (X1,X2)(X_1, X_2). basic models assumes subject marginal Cox-form λs(k,)(t)exp(XkiTβ)  \\lambda_{s(k,)}(t) \\exp( X_{ki}^T \\beta)  s(k,)s(k,) strata variable. constructed likelihood composite likehood based pairs within cluster (pairs argument used) specified pairs pairs argument used. addition clusters specified construction composite likelihood can added summed using se.clusters argument sums influence functions se.clusters. se.clusters specified cluster argument. clusters dependence parametaters marginal model phreg function used standard errors corrected uncertainty marginal models, otherwise returned standard errors computed marginals known.","code":""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"gamma-distributed-frailties","dir":"Articles","previous_headings":"","what":"Gamma distributed frailties","title":"Analysis of multivariate survival data","text":"focus vignette describe work bivariate survival data using addtive gamma-random effects models. present two different ways specifying different dependence structures. Univariate models single random effect cluster regression design variance. Multivariate models multiple random effects cluster. univariate models given given cluster random effects ZkZ_k parameter θ\\theta joint survival function given Clayton copula form ψ(θ,ψ−1(θ,S1(t,Xk1))+ψ−1(θ,S1(t,Xk1))    \\psi(\\theta, \\psi^{-1}(\\theta,S_1(t,X_{k1}) ) + \\psi^{-1}(\\theta, S_1(t,X_{k1}) )   ψ\\psi Laplace transform gamma distributed random variable mean 1 variance θ\\theta. model variance within clusters cluster specific regression design θ=h(zjTα)    \\theta = h(z_j^T \\alpha)  zz regression design (specified theta.des software), hh link function, either expexp identity. model can fitted using pairwise likelihood pseudo-likelihood using either twostage twostageMLE make twostage approach possible need model specific structure marginals. Therefore given random effect clusters survival distributions within cluster independent form P(Tj>t|Xj,Z)=exp(−Z⋅Ψ−1(ν−1,S(t|Xj)))   P(T_j > t| X_j,Z) = exp( -Z \\cdot \\Psi^{-1}(\\nu^{-1},S(t|X_j)) )    Ψ\\Psi laplace gamma distribution mean 1 variance 1/ν1/\\nu.","code":""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"additive-gamma-frailties","dir":"Articles","previous_headings":"","what":"Additive Gamma frailties","title":"Analysis of multivariate survival data","text":"multivariate models given multivarite random effect cluster Z=(Z1,...,Zd)Z=(Z_1,...,Z_d) d random effects. total random effect subject jj cluster specified using regression design random effects, regression vector VjV_j total random effect VjT(Z1,...,Zd)V_j^T (Z_1,...,Z_d). elements VJV_J 1/0. random effects (Z1,...,Zd)(Z_1,...,Z_d) associated parameters (λ1,...,λd)(\\lambda_1,...,\\lambda_d) ZjZ_j Gamma distributed mean λj/V1Tλ\\lambda_j/V_1^T \\lambda variance λj/(V1Tλ)2\\lambda_j/(V_1^T \\lambda)^2 key assumption make two-stage fitting possible ν=VjTλ\\begin{align*}    \\nu =V_j^T \\lambda \\end{align*} constant within clusters. consequence total random effect subject within cluster, VjT(Z1,...,Zd)V_j^T (Z_1,...,Z_d), gamma distributed variance 1/ν1/\\nu. DEFAULT parametrization (var.par=1) uses variances random effecs θj=λj/ν2\\begin{align*}  \\theta_j  = \\lambda_j/\\nu^2 \\end{align*} alternative parametrizations one can specify parameters θj=λj\\theta_j=\\lambda_j argument var.par=0. Finally parameters (θ1,...,θd)(\\theta_1,...,\\theta_d) related parameters model regression construction MM (d x k), links ddθ\\theta parameters kk underlying α\\alpha parameters θ=Mα.\\begin{align*}  \\theta & = M  \\alpha.  \\end{align*} default diagonal matrix MM. can used make structural assumptions variances random-effects needed ACE model example. software $ M $ called theta.des Assume marginal survival distribution subject ii within cluster kk given SXk,(t)S_{X_{k,}}(t) given covariates Xk,iX_{k,}. Now given random effects cluster ZkZ_k covariatesXk,iX_{k,}=1,…,nki=1,\\dots,n_k assume subjects within cluster independent survival distributions exp(−(Vk,iZk)Ψ−1(ν,SXk,(t))).\\begin{align*}   \\exp(-  ( V_{k,} Z_k)  \\Psi^{-1} (\\nu,S_{X_{k,}}(t)) ). \\end{align*} consequence hazards given covariates Xk,iX_{k,} random effects ZkZ_k given λk,(t;Xk,,Zk,)=(Vk,iVk)D3Ψ−1(ν,SXk,(t))DtSXk,(t)\\begin{align}   \\lambda_{k,}(t;X_{k,},Z_{k,}) = ( V_{k,} V_k) D_3 \\Psi^{-1} (\\nu,S_{X_{k,}}(t))  D_t S_{X_{k,}}(t)    \\label{eq-cond-haz} \\end{align} DtD_t D3D_3 denotes partial derivatives respect tt third argument, respectively. , can express multivariate survival distribution S(t1,…,tm)=exp(−∑=1m(ViZ)Ψ−1(ηl,νl,SXk,(ti)))=∏l=1pΨ(ηl,η,∑=1mQk,iΨ−1(η,η,SXk,(ti))).\\begin{align}   S(t_1,\\dots,t_m) & =  \\exp( -\\sum_{=1}^m (V_i Z) \\Psi^{-1}(\\eta_l,\\nu_l,S_{X_{k,}}(t_i)) )  \\nonumber \\\\   & =  \\prod_{l=1}^p  \\Psi(\\eta_l,\\eta , \\sum_{=1}^m Q_{k,} \\Psi^{-1}(\\eta,\\eta,S_{X_{k,}}(t_i))).   \\label{eq-multivariate-surv} \\end{align} case considering just pairs, write function C(Sk,(t),Sk,j(t))C(S_{k,}(t),S_{k,j}(t)). addition survival times model, assume independent right censoring present Uk,iU_{k,} given VkV_k covariatesXk,iX_{k,}=1,…,nki=1,\\dots,n_k(Uk,1,…,Uk,nk)(U_{k,1},\\dots,U_{k,n_k}) (Tk,1,…,Tk,nk)(T_{k,1},\\dots,T_{k,n_k}), conditional censoring distribution depend VkV_k. One consequence model strucure Kendall’s can computed two-subjects (,j)(,j) across two clusters 1'' and2’’ E((V1iZ1−V1jZ2)(V2iZ1−V2jZ2)(V1iZ1+V2iZ2)(V1jZ1+V2jZ2))\\begin{align} E( \\frac{( V_{1i} Z_1-  V_{1j}Z_2)( V_{2i}Z_1 -  V_{2j}Z_2 )}{( V_{1i}Z_1 + V_{2i}Z_2 ) ( V_{1j}Z_1 + V_{2j}Z_2 )} )  \\end{align} assumption compare pairs equivalent marginals, SX1,(t)=SX2,(t)S_{X_{1,}}(t)= S_{X_{2,}}(t) SX1,j(t)=SX2,j(t)S_{X_{1,j}}(t)= S_{X_{2,j}}(t), SX1,(∞)=SX1,j(∞)=0S_{X_{1,}}(\\infty)= S_{X_{1,j}}(\\infty)=0. also use η\\eta across clusters. Kendall’s tau due additive structure frailty terms, random effects thus interpretation terms Kendall’s tau.","code":""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"univariate-gamma-clayton-oakes-model-twostage-models","dir":"Articles","previous_headings":"","what":"Univariate gamma (clayton-oakes) model twostage models","title":"Analysis of multivariate survival data","text":"start fitting simple Clayton-Oakes models data, overall random effect Gamma distrubuted variance θ\\theta. can fit model pseudo-MLE (twostageMLE) pairwise composite likelihood approach (twostage). pseudo-liklihood composite pairwise likelhood give model since paired data. addition log-parametrization illustrated var.link=1 option. addition specified want “clayton.oakes” model. note standard errors differs twostage include variance due baseline parameters type modelling, better use twostageMLE. Note, standard errors slightly different comparing fitco1 fitco3 since survival.twostage uses numerical derivatives hessian derivative direction marginal model. marginal models can either structured Cox model baseline strata. gives quite similar results .","code":"library(mets)  data(diabetes)  set.seed(100)    # Marginal Cox model  with treat as covariate  margph <- phreg(Surv(time,status)~treat+cluster(id),data=diabetes)  # Clayton-Oakes, MLE   fitco1<-twostageMLE(margph,data=diabetes,theta=1.0)  summary(fitco1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE       z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543033 2.68883 0.007170289    0.322645 0.08127892 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    # Clayton-Oakes  fitco2 <- survival.twostage(margph,data=diabetes,theta=0.0,                   clusters=diabetes$id,var.link=1,model=\"clayton.oakes\")  summary(fitco2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>              log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.0484957 0.3718487 -0.1304178 0.8962359    0.322645 0.08126576 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9527  0.3542 0.2584 1.647 0.007161 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  fitco3 <- survival.twostage(margph,data=diabetes,theta=1.0,                   clusters=diabetes$id,var.link=0,model=\"clayton.oakes\")  summary(fitco3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.     SE        z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543 2.688855 0.007169754    0.322645 0.08127816 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # without covariates but marginal model stratified    marg <- phreg(Surv(time,status)~+strata(treat)+cluster(id),data=diabetes)  fitco<-twostageMLE(marg,data=diabetes,theta=1.0)  summary(fitco) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z       P-val Kendall tau         SE #> dependence1 0.9447446 0.3516204 2.686831 0.007213349    0.320824 0.08109776 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    fitcoa <- survival.twostage(marg,data=diabetes,theta=1.0,clusters=diabetes$id,            model=\"clayton.oakes\",var.link=0)   summary(fitcoa) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z       P-val Kendall tau         SE #> dependence1 0.9447446 0.3516129 2.686889 0.007212097    0.320824 0.08109601 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"piecewise-constant-clayton-oakes-model","dir":"Articles","previous_headings":"","what":"Piecewise constant Clayton-Oakes model","title":"Analysis of multivariate survival data","text":"Let cross-hazard ratio (CHR) defined η(t1,t2)=λ1(t1|T2=t2)λ1(t1|T2≥t2)=λ2(t2|T1=t1)λ2(t2|T1≥t1)\\begin{align}   \\eta(t_1,t_2) =  \\frac{ \\lambda_1(t_1| T_2=t_2)}{ \\lambda_1(t_1| T_2 \\ge t_2)}   =  \\frac{ \\lambda_2(t_2| T_1=t_1)}{ \\lambda_2(t_2| T_1 \\ge t_1)} \\end{align} λ1\\lambda_1 λ2\\lambda_2 conditional hazard functions T1T_1 T2T_2 given covariates. Clayton-Oakes model ratio η(t1,t2)=1+θ\\eta(t_1,t_2) = 1+\\theta, consequence see co-twin dead time increase risk assessment hazard scale constant η(t1,t2)\\eta(t_1,t_2). Clayton-Oakes model also nice property Kendall’s tau linked directly dependence parameter θ\\theta 1/(1+2/θ)1/(1+2/\\theta). useful extension model constant cross-hazard ratio (CHR) model piecewise constant cross-hazard ratio (CHR) bivariate survival data , model extended competing risks . survival setting let CHR η(t1,t2)=∑ηi,jI(t1∈Ii,t2∈Ij)\\begin{align}   \\eta(t_1,t_2) & = \\sum \\eta_{,j} (t_1 \\I_i, t_2 \\I_j) \\end{align} model lets CHR constant different part plane. can thought also separate Clayton-Oakes model regions specified plane cut-points c(0,0.5,2)c(0,0.5,2) thus defining 9 regions. provides constructive goodness fit test whether Clayton-Oakes model valid. Indeed valid parameter regions. First generate data Clayton-Oakes model variance 0.50.5 2000 pairs. fit related model. Now cut region cut-points c(0,0.5,2)c(0,0.5,2) thus defining 9 regions fit separate model region. see parameter indeed rather constant 9 regions. formal test can constructed.","code":"d <- simClaytonOakes(200,2,0.5,0,3)   margph <- phreg(Surv(time,status)~x+cluster(cluster),data=d)  # Clayton-Oakes, MLE   fitco1<-twostageMLE(margph,data=d)  summary(fitco1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE       z        P-val Kendall tau         SE #> dependence1 2.283394 0.3164797 7.21498 5.393463e-13   0.5330806 0.03449846 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" udp <- piecewise.twostage(c(0,0.5,2),data=d,id=\"cluster\",timevar=\"time\",status=\"status\",model=\"clayton.oakes\",silent=0) #> Data-set  1 out of  4 #>   Number of joint events: 55 of  200 #> Data-set  2 out of  4 #>   Number of joint events: 25 of  116 #> Data-set  3 out of  4 #>   Number of joint events: 28 of  120 #> Data-set  4 out of  4 #>   Number of joint events: 50 of  91  summary(udp) #> [1] 1 #> Dependence parameter for Clayton-Oakes model  #> log-coefficient for dependence parameter (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  0.566 (0.222)  1.278 (0.223) #> 0.5 - 2  0.519 (0.268)  1.092 (0.205) #>  #> Kendall's tau (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  0.468 (0.055)  0.642 (0.051) #> 0.5 - 2  0.457 (0.066)  0.598 (0.049)"},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"multivariate-gamma-twostage-models","dir":"Articles","previous_headings":"","what":"Multivariate gamma twostage models","title":"Analysis of multivariate survival data","text":"illustrate multivariate models can used, first set twin data ACE structure. two shared random effects, one genes σg2\\sigma_g^2 one environmental effect σe2\\sigma_e^2. Monozygotic twins share genes whereas dizygotic twins share half genes. can expressed via 5 random effect twin pair (example). start setting . pardes matrix tells parameters 5 random effects related, matrix first one random effect parameter θ1\\theta_1 (σg2\\sigma_g^2 ), next 3 random effects parameters 0.5θ10.5 \\theta_1 (0.5σg20.5 \\sigma_g^2 ), last random effect given parameter θ2\\theta_2 (σe2\\sigma_e^2 ). last part model structure decide random effects shared different pairs (MZ DZ), specfied random effects design (V1V_1 V2V_2) pair. specified overall designmatrix subject (since enter pairs random effects design). MZ pair two share full gene random effect full environmental random effect. contrast DZ pairs share 2nd random effect half gene-variance non-shared gene-random effect half variance, finally fully shared environmental random effect. Now call twostage function. see essentially recover true values, note output also compares sizes genetic environmental random effect. number sometimes called heritability. addition total variance subject also computed around 33, indeed constructed. 1 mill pairs running time around 100 seconds. estimates can transformed Kendall’s tau estimates MZ DZ twins. Kendall’s tau output reflects gamma distributed random effect normal Clayton-Oakes model related Kendall’s tau. setting Kendall’s MZ DZ, however, reflect random effects. based simulations. Kendall’s tau MZ around 0.60, DZ around 0.33. quite high due large shared environmental effect large genetic effect.","code":"data <- simClaytonOakes.twin.ace(200,2,1,0,3)   out <- twin.polygen.design(data,id=\"cluster\",zyg=\"DZ\",zygname=\"zyg\",type=\"ace\")  pardes <- out$pardes  pardes  #>      [,1] [,2] #> [1,]  1.0    0 #> [2,]  0.5    0 #> [3,]  0.5    0 #> [4,]  0.5    0 #> [5,]  0.0    1 des.rv <- out$des.rv  # MZ  head(des.rv,2) #>   MZ DZ DZns1 DZns2 env #> 1  1  0     0     0   1 #> 2  1  0     0     0   1  # DZ   tail(des.rv,2) #>     MZ DZ DZns1 DZns2 env #> 399  0  1     1     0   1 #> 400  0  1     0     1   1 ### data <- simClaytonOakes.twin.ace(2000,2,1,0,3) ### out <- twin.polygen.design(data,id=\"cluster\",zyg=\"DZ\",zygname=\"zyg\",type=\"ace\")  aa <- phreg(Surv(time,status)~x+cluster(cluster),data=data)  ts <- twostage(aa,data=data,clusters=data$cluster,       theta=c(2,1),var.link=0,random.design=out$des.rv,theta.des=out$pardes)  summary(ts) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE        z       P-val Kendall tau         SE #> dependence1 2.051732 0.6981882 2.938652 0.003296431   0.5063840 0.08505916 #> dependence2 1.025707 0.5674045 1.807717 0.070650519   0.3389974 0.12395643 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err     2.5%  97.5%   P-value #> [1,]   0.6667    0.19  0.29431 1.0391 0.0004498 #> [2,]   0.3333    0.19 -0.03909 0.7057 0.0793902 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    3.077  0.3982 2.297 3.858 1.092e-14 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" run <- 0 if (run==1) {  data <- simClaytonOakes.twin.ace(1000000,2,1,0,3)   out <- twin.polygen.design(data,id=\"cluster\",zyg=\"DZ\",zygname=\"zyg\",type=\"ace\")  pardes <- out$pardes  aa <- phreg(Surv(time,status)~x+cluster(cluster),data=data)  system.time(  ts <- twostage(aa,data=data,clusters=data$cluster,       theta=c(2,1),var.link=0,random.design=out$des.rv,theta.des=out$pardes)  )  summary(ts) } kendall.ClaytonOakes.twin.ace(ts$theta[1],ts$theta[2],K=10000)  #> $mz.kendall #> [1] 0.6186546 #>  #> $dz.kendall #> [1] 0.3350827"},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"family-data","dir":"Articles","previous_headings":"","what":"Family data","title":"Analysis of multivariate survival data","text":"family data, things quite similar since use pairwise structure. show designs specified. First simulate data ACE model. 2000 families two-parents share environment, two-children share genes parents. set random effects functions can used. set ACE model 9 random effects one shared environmental effect (last random effect) 4 genetic random effects parent, variance σg2/4\\sigma_g^2/4. random effect set-overall designmatrix subject comparisons across family members. demonstrate model can specified various ways. child share 2 genetic random effects parent, also share 2 genetic random effects /sibling. fit model model can also fitted specifying pairs one wants pairwise likelhood. done specifying pairs argument. start considering pairs also . pairs can written calling familycluster.index function. xx pairs consider, first 6 pairs first family written . fitting model using specified pairs Now use random sample pairs sampling . pairs picked still refers data given data argument, clusters (families) also specified . Sometimes one data pairs addition example cohort estimate marginal surival models. now demonstrate dealt . Everything essentially need organize design differently compared specified design everybody cohort. addition bring uncertainty baseline estimates, even though formally possible, data marginal model twostage data , specify want decomposition uncertainty due baseline (baseline.iid=0). Now fitting model using pair data. Estimates changed used either marginal full-data, case standard errors reflect uncertainty baseline, marginal estimated sub-sample case marginals slightly different.","code":"library(mets) set.seed(1000) data <- simClaytonOakes.family.ace(200,2,1,0,3) head(data) #>        time status x cluster   type   mintime lefttime truncated #> 1 0.4139376      1 0       1 mother 0.4139376        0         0 #> 2 1.6435053      1 1       1 father 0.4139376        0         0 #> 3 1.2485275      1 0       1  child 0.4139376        0         0 #> 4 1.1079118      1 1       1  child 0.4139376        0         0 #> 5 3.0000000      0 0       2 mother 1.6718726        0         0 #> 6 2.0085566      1 1       2 father 1.6718726        0         0 data$number <- c(1,2,3,4) data$child <- 1*(data$number==3) out <- ace.family.design(data,member=\"type\",id=\"cluster\") out$pardes #>       [,1] [,2] #>  [1,] 0.25    0 #>  [2,] 0.25    0 #>  [3,] 0.25    0 #>  [4,] 0.25    0 #>  [5,] 0.25    0 #>  [6,] 0.25    0 #>  [7,] 0.25    0 #>  [8,] 0.25    0 #>  [9,] 0.00    1 head(out$des.rv,4) #>      m1 m2 m3 m4 f1 f2 f3 f4 env #> [1,]  1  1  1  1  0  0  0  0   1 #> [2,]  0  0  0  0  1  1  1  1   1 #> [3,]  1  1  0  0  1  1  0  0   1 #> [4,]  1  0  1  0  1  0  1  0   1 pa <- phreg(Surv(time,status)~+1+cluster(cluster),data=data)  # make ace random effects design ts <- twostage(pa,data=data,clusters=data$cluster,var.par=1,var.link=0,theta=c(2,1),         random.design=out$des.rv,theta.des=out$pardes) summary(ts) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z        P-val Kendall tau         SE #> dependence1 2.0094622 0.5369170 3.742594 0.0001821303   0.5011800 0.06679822 #> dependence2 0.6503464 0.2136795 3.043561 0.0023379628   0.2453817 0.06083976 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5% 97.5%   P-value #> [1,]   0.7555 0.09823 0.56297 0.948 1.458e-14 #> [2,]   0.2445 0.09823 0.05199 0.437 1.280e-02 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1     2.66  0.4459 1.786 3.534 2.437e-09 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # now specify fitting via specific pairs  # first all pairs  mm <- familycluster.index(data$cluster) head(mm$familypairindex,n=12) #>  [1] 1 2 1 3 1 4 2 3 2 4 3 4 pairs <- matrix(mm$familypairindex,ncol=2,byrow=TRUE) head(pairs,n=6) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    1    3 #> [3,]    1    4 #> [4,]    2    3 #> [5,]    2    4 #> [6,]    3    4 ts <- twostage(pa,data=data,clusters=data$cluster, theta=c(2,1),var.link=0,step=1.0,         random.design=out$des.rv, theta.des=out$pardes,pairs=pairs) summary(ts) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z        P-val Kendall tau         SE #> dependence1 2.0094622 0.5369170 3.742594 0.0001821303   0.5011800 0.06679822 #> dependence2 0.6503464 0.2136795 3.043561 0.0023379628   0.2453817 0.06083976 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5% 97.5%   P-value #> [1,]   0.7555 0.09823 0.56297 0.948 1.458e-14 #> [2,]   0.2445 0.09823 0.05199 0.437 1.280e-02 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1     2.66  0.4459 1.786 3.534 2.437e-09 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" ssid <- sort(sample(1:nrow(pairs),200)) tsd <- twostage(pa,data=data,clusters=data$cluster,     theta=c(2,1)/10,var.link=0,random.design=out$des.rv,    theta.des=out$pardes,pairs=pairs[ssid,]) summary(tsd) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9237395 0.6070493 1.521688 0.1280873   0.3159445 0.14202885 #> dependence2 0.4087673 0.2660432 1.536469 0.1244233   0.1696998 0.09170489 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%  97.5% P-value #> [1,]   0.6932  0.2523  0.1987 1.1878 0.00601 #> [2,]   0.3068  0.2523 -0.1878 0.8013 0.22410 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5% P-value #> p1    1.333  0.4803 0.3912 2.274 0.00553 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" ids <- sort(unique(c(pairs[ssid,])))  pairsids <- c(pairs[ssid,]) pair.new <- matrix(fast.approx(ids,c(pairs[ssid,])),ncol=2) head(pair.new) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    3    4 #> [3,]    3    6 #> [4,]    5    6 #> [5,]    7    8 #> [6,]    9   11  # this requires that pair.new refers to id's in dataid (survival, status and so forth) # random.design and theta.des are constructed to be the array 3 dims via individual specfication from ace.family.design dataid <- dsort(data[ids,],\"cluster\") outid <- ace.family.design(dataid,member=\"type\",id=\"cluster\") outid$pardes #>       [,1] [,2] #>  [1,] 0.25    0 #>  [2,] 0.25    0 #>  [3,] 0.25    0 #>  [4,] 0.25    0 #>  [5,] 0.25    0 #>  [6,] 0.25    0 #>  [7,] 0.25    0 #>  [8,] 0.25    0 #>  [9,] 0.00    1 head(outid$des.rv) #>      m1 m2 m3 m4 f1 f2 f3 f4 env #> [1,]  0  0  0  0  1  1  1  1   1 #> [2,]  1  0  1  0  1  0  1  0   1 #> [3,]  1  1  1  1  0  0  0  0   1 #> [4,]  0  0  0  0  1  1  1  1   1 #> [5,]  1  1  0  0  1  1  0  0   1 #> [6,]  1  0  1  0  1  0  1  0   1 tsdid <- twostage(pa,data=dataid,clusters=dataid$cluster,theta=c(2,1)/10,var.link=0,baseline.iid=0,           random.design=outid$des.rv,theta.des=outid$pardes,pairs=pair.new) summary(tsdid) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9237395 0.6040859 1.529153 0.1262266   0.3159445 0.14133553 #> dependence2 0.4087673 0.2590853 1.577732 0.1146271   0.1696998 0.08930652 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%  97.5%  P-value #> [1,]   0.6932  0.2487  0.2058 1.1807 0.005311 #> [2,]   0.3068  0.2487 -0.1807 0.7942 0.217376 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5%  P-value #> p1    1.333  0.4784 0.3949  2.27 0.005345 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  paid <- phreg(Surv(time,status)~+1+cluster(cluster),data=dataid) tsdidb <- twostage(paid,data=dataid,clusters=dataid$cluster,theta=c(2,1)/10,   var.link=0,random.design=outid$des.rv,theta.des=outid$pardes,pairs=pair.new) summary(tsdidb) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9405413 0.6151098 1.529062 0.1262490   0.3198531 0.14227484 #> dependence2 0.3381704 0.2557053 1.322501 0.1860015   0.1446303 0.09354431 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%  97.5%  P-value #> [1,]   0.7355  0.2499  0.2458 1.2253 0.003247 #> [2,]   0.2645  0.2499 -0.2253 0.7542 0.289925 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5%  P-value #> p1    1.279  0.4865 0.3251 2.232 0.008583 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" coef(tsdid) #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9237395 0.6040859 1.529153 0.1262266   0.3159445 0.14133553 #> dependence2 0.4087673 0.2590853 1.577732 0.1146271   0.1696998 0.08930652"},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"pairwise-specification-of-random-effects-and-variances","dir":"Articles","previous_headings":"Family data","what":"Pairwise specification of random effects and variances","title":"Analysis of multivariate survival data","text":"Now illustrate one can also directly specify random.design theta.design pair, rather taking overall specification can used whole family via rows des.rv relevant pairs. can much simpler situations. pairs.new matix columns 1:2 giving indeces data points columns 3:4 giving indeces random.design different pairs columns 5 giving indeces theta.des written rows columns 6 giving number random variables pair Looking first three rows. see composite likehood based data-points (1,2), (3,4) (5,6), (mother, father), (mother, child), (father, child), respectively. random effects specified random effects design read random.design, using rows (1,2), (3,4) (3,4), respecively, random effects variances given theta.des rows, 1,2, 2 respectively three cases. first pair (1,2), random vectors variances given , (mother, father) pair, thus sharing third random effect variance σe2\\sigma_e^2 two non-shared random effects variances σg2\\sigma_g^2, finally last 4th random effect variance 00 thus omitted. length rows theta.des maximum number random effects ×\\times number parameters. two numbers given call. case 4 ×\\times 2. theta.des rows length 88, possibly including 0’s rows relevant due fewer random effects, case pairs share genetic effects. Now considering parent child, thus sharing first random effect variance 0.5σg20.5 \\sigma_g^2 two non-shared random effects variances 0.5σg20.5 \\sigma_g^2, finally shared environment variance σe2\\sigma_e^2. fitting model Finally model structure can setup based Kinship coefficient. output Now fitting AE model without “C” component shared environment:","code":"pair.types <-  matrix(dataid[c(t(pair.new)),\"type\"],byrow=T,ncol=2) head(pair.new) #>      [,1] [,2] #> [1,]    1    2 #> [2,]    3    4 #> [3,]    3    6 #> [4,]    5    6 #> [5,]    7    8 #> [6,]    9   11 head(pair.types) #>      [,1]     [,2]     #> [1,] \"father\" \"child\"  #> [2,] \"mother\" \"father\" #> [3,] \"mother\" \"child\"  #> [4,] \"child\"  \"child\"  #> [5,] \"father\" \"child\"  #> [6,] \"mother\" \"child\"  theta.des  <- rbind( c(rbind(c(1,0),c(1,0),c(0,1),c(0,0))),         c(rbind(c(0.5,0),c(0.5,0),c(0.5,0),c(0,1)))) random.des <- rbind(          c(1,0,1,0),c(0,1,1,0),         c(1,1,0,1),c(1,0,1,1)) mf <- 1*(pair.types[,1]==\"mother\" & pair.types[,2]==\"father\") ##          pair, rv related to pairs,  theta.des related to pair  pairs.new <- cbind(pair.new,(mf==1)*1+(mf==0)*3,(mf==1)*2+(mf==0)*4,(mf==1)*1+(mf==0)*2,(mf==1)*3+(mf==0)*4) head(pairs.new[1:3,]) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    1    2    3    4    2    4 #> [2,]    3    4    1    2    1    3 #> [3,]    3    6    3    4    2    4 head(dataid) #>       time status x cluster   type   mintime lefttime truncated number child #> 2 1.643505      1 1       1 father 0.4139376        0         0      2     0 #> 4 1.107912      1 1       1  child 0.4139376        0         0      4     0 #> 5 3.000000      0 0       2 mother 1.6718726        0         0      1     0 #> 6 2.008557      1 1       2 father 1.6718726        0         0      2     0 #> 7 1.671873      1 1       2  child 1.6718726        0         0      3     1 #> 8 1.763022      1 1       2  child 1.6718726        0         0      4     0 random.des[1,] #> [1] 1 0 1 0 random.des[2,] #> [1] 0 1 1 0 matrix(theta.des[1,],4,2) #>      [,1] [,2] #> [1,]    1    0 #> [2,]    1    0 #> [3,]    0    1 #> [4,]    0    0 head(dataid) #>       time status x cluster   type   mintime lefttime truncated number child #> 2 1.643505      1 1       1 father 0.4139376        0         0      2     0 #> 4 1.107912      1 1       1  child 0.4139376        0         0      4     0 #> 5 3.000000      0 0       2 mother 1.6718726        0         0      1     0 #> 6 2.008557      1 1       2 father 1.6718726        0         0      2     0 #> 7 1.671873      1 1       2  child 1.6718726        0         0      3     1 #> 8 1.763022      1 1       2  child 1.6718726        0         0      4     0 matrix(theta.des[2,],4,2) #>      [,1] [,2] #> [1,]  0.5    0 #> [2,]  0.5    0 #> [3,]  0.5    0 #> [4,]  0.0    1 random.des[3,] #> [1] 1 1 0 1 random.des[4,] #> [1] 1 0 1 1 tsdid2 <- twostage(pa,data=dataid,clusters=dataid$cluster,        theta=c(2,1)/10,var.link=0,step=1.0,random.design=random.des,        baseline.iid=0, theta.des=theta.des,pairs=pairs.new,dim.theta=2) summary(tsdid2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9237395 0.6040859 1.529153 0.1262266   0.3159445 0.14133553 #> dependence2 0.4087673 0.2590853 1.577732 0.1146271   0.1696998 0.08930652 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%  97.5%  P-value #> [1,]   0.6932  0.2487  0.2058 1.1807 0.005311 #> [2,]   0.3068  0.2487 -0.1807 0.7942 0.217376 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5%  P-value #> p1    1.333  0.4784 0.3949  2.27 0.005345 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" tsd$theta #>           [,1] #> [1,] 0.9237395 #> [2,] 0.4087673 tsdid2$theta #>           [,1] #> [1,] 0.9237395 #> [2,] 0.4087673 tsdid$theta #>           [,1] #> [1,] 0.9237395 #> [2,] 0.4087673 kinship  <- rep(0.5,nrow(pair.types)) kinship[pair.types[,1]==\"mother\" & pair.types[,2]==\"father\"] <- 0 head(kinship,n=10) #>  [1] 0.5 0.0 0.5 0.5 0.5 0.5 0.5 0.0 0.5 0.0  out <- make.pairwise.design(pair.new,kinship,type=\"ace\") tsdid3 <- twostage(pa,data=dataid,clusters=dataid$cluster,    theta=c(2,1)/10,var.link=0,step=1.0,random.design=out$random.design,    baseline.iid=0,theta.des=out$theta.des,pairs=out$new.pairs,dim.theta=2) summary(tsdid3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE        z     P-val Kendall tau         SE #> dependence1 0.9237395 0.6040859 1.529153 0.1262266   0.3159445 0.14133553 #> dependence2 0.4087673 0.2590853 1.577732 0.1146271   0.1696998 0.08930652 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err    2.5%  97.5%  P-value #> [1,]   0.6932  0.2487  0.2058 1.1807 0.005311 #> [2,]   0.3068  0.2487 -0.1807 0.7942 0.217376 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5%  P-value #> p1    1.333  0.4784 0.3949  2.27 0.005345 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" tsdid2$theta #>           [,1] #> [1,] 0.9237395 #> [2,] 0.4087673 tsdid$theta #>           [,1] #> [1,] 0.9237395 #> [2,] 0.4087673 outae <- make.pairwise.design(pair.new,kinship,type=\"ae\")  tsdid4 <- twostage(pa,data=dataid,clusters=dataid$cluster,    theta=c(2,1)/10,var.link=0,random.design=outae$random.design,    baseline.iid=0,theta.des=outae$theta.des,pairs=outae$new.pairs,dim.theta=1) summary(tsdid4) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                Coef.        SE        z        P-val Kendall tau        SE #> dependence1 1.932485 0.5702673 3.388735 0.0007021583   0.4914157 0.0737521 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err 2.5% 97.5% P-value #> [1,]        1       0    1     1       0 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err   2.5% 97.5%   P-value #> p1    1.932  0.5703 0.8148  3.05 0.0007022 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"pairwise-dependence-modelling","dir":"Articles","previous_headings":"Family data","what":"Pairwise dependence modelling","title":"Analysis of multivariate survival data","text":"now illustate estimate pairwise associations different family-members using twostage function. key specify pairs composite likelihood directly associated design-matrix directly. needs done looking subjects pairs, design therefore follows pairs matrix row given third column pairs argument. First get pairs considered Now construct design matrix related pairs fit model: note mother-father smallest correlation since share environmental random effect, whereas pairs similar correlation (fact due way data simulated) consisting half genetic effect environmental effect also shared.","code":"library(mets) set.seed(1000) data <- simClaytonOakes.family.ace(200,2,1,0,3) head(data) #>        time status x cluster   type   mintime lefttime truncated #> 1 0.4139376      1 0       1 mother 0.4139376        0         0 #> 2 1.6435053      1 1       1 father 0.4139376        0         0 #> 3 1.2485275      1 0       1  child 0.4139376        0         0 #> 4 1.1079118      1 1       1  child 0.4139376        0         0 #> 5 3.0000000      0 0       2 mother 1.6718726        0         0 #> 6 2.0085566      1 1       2 father 1.6718726        0         0 data$number <- c(1,2,3,4) data$child <- 1*(data$number==3)  mm <- familycluster.index(data$cluster) head(mm$familypairindex,n=20) #>  [1] 1 2 1 3 1 4 2 3 2 4 3 4 5 6 5 7 5 8 6 7 pairs <- mm$pairs dim(pairs) #> [1] 1200    2 head(pairs,12) #>       [,1] [,2] #>  [1,]    1    2 #>  [2,]    1    3 #>  [3,]    1    4 #>  [4,]    2    3 #>  [5,]    2    4 #>  [6,]    3    4 #>  [7,]    5    6 #>  [8,]    5    7 #>  [9,]    5    8 #> [10,]    6    7 #> [11,]    6    8 #> [12,]    7    8 dtypes <- interaction( data[pairs[,1],\"type\"], data[pairs[,2],\"type\"])  dtypes <- droplevels(dtypes)  table(dtypes) #> dtypes #>   child.child  father.child  mother.child mother.father  #>           200           400           400           200  dm <- model.matrix(~-1+factor(dtypes))  head(dm) #>   factor(dtypes)child.child factor(dtypes)father.child #> 1                         0                          0 #> 2                         0                          0 #> 3                         0                          0 #> 4                         0                          1 #> 5                         0                          1 #> 6                         1                          0 #>   factor(dtypes)mother.child factor(dtypes)mother.father #> 1                          0                           1 #> 2                          1                           0 #> 3                          1                           0 #> 4                          0                           0 #> 5                          0                           0 #> 6                          0                           0 pa <- phreg(Surv(time,status)~cluster(cluster),data)  tsp <- twostage(pa,data=data,theta.des=dm,pairs=cbind(pairs,1:nrow(dm)),se.clusters=data$clust) summary(tsp) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>                               log-Coef.        SE           z       P-val #> factor(dtypes)child.child    0.01865073 0.2087690  0.08933666 0.928814359 #> factor(dtypes)father.child  -0.29471237 0.2167688 -1.35956992 0.173966062 #> factor(dtypes)mother.child  -0.24879997 0.1949185 -1.27643105 0.201803188 #> factor(dtypes)mother.father -1.42550068 0.5159729 -2.76274312 0.005731786 #>                             Kendall tau         SE #> factor(dtypes)child.child     0.3374907 0.04667882 #> factor(dtypes)father.child    0.2713351 0.04285787 #> factor(dtypes)mother.child    0.2805072 0.03933901 #> factor(dtypes)mother.father   0.1072975 0.04942234 #>  #> $vargam #>             Estimate Std.Err      2.5%  97.5%   P-value #> dependence1   1.0188  0.2127  0.601943 1.4357 1.668e-06 #> dependence2   0.7447  0.1614  0.428334 1.0612 3.965e-06 #> dependence3   0.7797  0.1520  0.481851 1.0776 2.892e-07 #> dependence4   0.2404  0.1240 -0.002714 0.4835 5.261e-02 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"univariate-plackett-model-twostage-models","dir":"Articles","previous_headings":"","what":"Univariate plackett model twostage models","title":"Analysis of multivariate survival data","text":"copula known Plackett distribution, see , form C(u,v;θ)={S−(S2−4uvθ(θ−))2(θ−1) θ≠1uv θ=1\\begin{align}   C(u,v; \\theta) =   \\begin{cases}     \\frac{ S - (S^2 - 4 u v \\theta (\\theta-))}{2 (\\theta -1)} & \\mbox{ } \\theta \\ne 1 \\\\     u v & \\mbox{ } \\theta = 1   \\end{cases} \\end{align} S=1+(θ−1)(u+v)S=1+(\\theta-1) (u + v). marginals SiS_i now define bivariate survival function C(u1,u2)=H(S1(t1),S2(t2))C(u_1,u_2)=H(S_1(t_1),S_2(t_2)) ui=Si(ti)u_i=S_i(t_i). dependence parameter θ\\theta nice interpretation equivalent odds-ratio 2×22 \\times 2 tables surviving past cut plane (t1,t2)(t_1,t_2), θ=P(T1>t1|T2>t2)P(T1≤t1|T2>t2)P(T1>t1|T2≤t2)P(T1≤t1|T2≤t2). \\theta = \\frac{ P(T_1 > t_1 | T_2 >t_2) P(T_1 \\leq t_1 | T_2>t_2) }{P(T_1 > t_1 | T_2 \\leq t_2) P(T_1 \\leq t_1 | T_2 \\leq t_2 ) }. One additional nice feature odds-ratio measure directly linked Spearman correlation, ρ\\rho, can computed θ+1θ−1−2θ(θ−1)2log(θ)\\begin{align}   \\frac{\\theta+1}{\\theta -1} - \\frac{2 \\theta}{(\\theta-1)^2} \\log(\\theta) \\end{align} θ≠1\\theta \\ne 1, θ=1\\theta=1 ρ=0\\rho=0. model free parameter Clayton-Oakes model. regression design","code":"library(mets)  data(diabetes)    # Marginal Cox model  with treat as covariate  margph <- phreg(Surv(time,status)~treat+cluster(id),data=diabetes)  # Clayton-Oakes, MLE   fitco1<-twostageMLE(margph,data=diabetes,theta=1.0)  summary(fitco1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects #> $estimates #>                 Coef.        SE       z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543033 2.68883 0.007170289    0.322645 0.08127892 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    # Plackett model  mph <- phreg(Surv(time,status)~treat+cluster(id),data=diabetes)  fitp <- survival.twostage(mph,data=diabetes,theta=3.0,                 clusters=diabetes$id,var.link=1,model=\"plackett\")  summary(fitp) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>             log-Coef.        SE        z        P-val Spearman Corr.         SE #> dependence1   1.14188 0.3026537 3.772891 0.0001613666      0.3648216 0.08869229 #>  #> $or #>             Estimate Std.Err  2.5% 97.5%   P-value #> dependence1    3.133  0.9481 1.274 4.991 0.0009528 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    # without covariates but with stratafied   marg <- phreg(Surv(time,status)~+strata(treat)+cluster(id),data=diabetes)  fitpa <- survival.twostage(marg,data=diabetes,theta=1.0,                  clusters=diabetes$id)  summary(fitpa) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>               log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.05684062 0.3721207 -0.1527478 0.8785971    0.320824 0.08108359 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9447  0.3516 0.2557 1.634 0.007203 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    fitcoa <- survival.twostage(marg,data=diabetes,theta=1.0,clusters=diabetes$id,                   model=\"clayton.oakes\")  summary(fitcoa) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link #> $estimates #>               log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.05684062 0.3721207 -0.1527478 0.8785971    0.320824 0.08108359 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9447  0.3516 0.2557 1.634 0.007203 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" mm <- model.matrix(~-1+factor(adult),diabetes)  fitp <- survival.twostage(mph,data=diabetes,theta=3.0,Nit=40,                 clusters=diabetes$id,var.link=1,model=\"plackett\",         theta.des=mm)  summary(fitp) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link #> $estimates #>                log-Coef.        SE        z       P-val Spearman Corr. #> factor(adult)1  1.098332 0.3630524 3.025272 0.002484097      0.3519987 #> factor(adult)2  1.231962 0.5233261 2.354100 0.018567607      0.3909504 #>                       SE #> factor(adult)1 0.1074107 #> factor(adult)2 0.1501987 #>  #> $or #>             Estimate Std.Err    2.5% 97.5% P-value #> dependence1    2.999   1.089  0.8650 5.133 0.00588 #> dependence2    3.428   1.794 -0.0881 6.944 0.05602 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" # Piecewise constant cross hazards ratio modelling   d <- subset(simClaytonOakes(1000,2,0.5,0,stoptime=2,left=0),!truncated)  udp <- piecewise.twostage(c(0,0.5,2),data=d,id=\"cluster\",timevar=\"time\",                            status=\"status\",model=\"plackett\",silent=0) #> Data-set  1 out of  4 #>   Number of joint events: 254 of  1000 #> Data-set  2 out of  4 #>   Number of joint events: 106 of  609 #> Data-set  3 out of  4 #>   Number of joint events: 136 of  640 #> Data-set  4 out of  4 #>   Number of joint events: 328 of  503  summary(udp) #> [1] 1 #> Dependence parameter for Plackett model  #> log-coefficient for dependence parameter (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  1.794 (0.106)  1.888 (0.139) #> 0.5 - 2  1.582 (0.141)  2.18  (0.109) #>  #> Spearman Correlation (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  0.54  (0.026)  0.563 (0.033) #> 0.5 - 2  0.487 (0.037)  0.628 (0.023)"},{"path":"http://kkholst.github.io/mets/articles/twostage-survival.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"Analysis of multivariate survival data","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/articles/while-alive.html","id":"while-alive-estimands-for-recurrent-events","dir":"Articles","previous_headings":"","what":"While Alive estimands for Recurrent Events","title":"While Alive estimands for Recurrent Events","text":"consider two -alive estimands recurrent events data E(N(D∧t))E(D∧t)\\begin{align*}  \\frac{E(N(D \\wedge t))}{E(D \\wedge t)} \\end{align*} mean subject specific events per time-unit E(N(D∧t)D∧t)\\begin{align*}  E( \\frac{N(D \\wedge t)}{D \\wedge t} )  \\end{align*} two treatment-groups case RCT. mean events per time-unit seen sample size small one can improve finite sample properties employing transformation square cube-root, thus consider E((N(D∧t)D∧t).33)\\begin{align*}  E( (\\frac{N(D \\wedge t)}{D \\wedge t})^.33 )  \\end{align*} see ratio means different, subject specific mean events per time-unit shows active treatment fewer events per time-unit average.","code":"data(hfactioncpx12)  dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124 dd <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2,death.code=2) summary(dd) #> While-Alive summaries:   #>  #> RMST,  E(min(D,t))  #>            Estimate Std.Err  2.5% 97.5% P-value #> treatment0    1.859 0.02108 1.817 1.900       0 #> treatment1    1.924 0.01502 1.894 1.953       0 #>   #>                           Estimate Std.Err    2.5%    97.5% P-value #> [treatment0] - [treat.... -0.06517 0.02588 -0.1159 -0.01444  0.0118 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> mean events, E(N(min(D,t))):  #>            Estimate Std.Err  2.5% 97.5%   P-value #> treatment0    1.572 0.09573 1.384 1.759 1.375e-60 #> treatment1    1.453 0.10315 1.251 1.656 4.376e-45 #>   #>                           Estimate Std.Err    2.5%  97.5% P-value #> [treatment0] - [treat....   0.1185  0.1407 -0.1574 0.3943     0.4 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> _______________________________________________________  #> Ratio of means E(N(min(D,t)))/E(min(D,t))  #>    Estimate Std.Err   2.5%  97.5%   P-value #> p1   0.8457 0.05264 0.7425 0.9488 4.411e-58 #> p2   0.7555 0.05433 0.6490 0.8619 5.963e-44 #>   #>             Estimate Std.Err     2.5%  97.5% P-value #> [p1] - [p2]  0.09022 0.07565 -0.05805 0.2385   0.233 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0  #> _______________________________________________________  #> Mean of Events per time-unit E(N(min(D,t))/min(D,t))  #>        Estimate Std.Err   2.5%  97.5%   P-value #> treat0   1.0725  0.1222 0.8331 1.3119 1.645e-18 #> treat1   0.7552  0.0643 0.6291 0.8812 7.508e-32 #>   #>                     Estimate Std.Err    2.5%  97.5% P-value #> [treat0] - [treat1]   0.3173  0.1381 0.04675 0.5879 0.02153 #>  #>  Null Hypothesis:  #>   [treat0] - [treat1] = 0  dd <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2,            death.code=2,trans=.333) summary(dd,type=\"log\") #> While-Alive summaries, log-scale:   #>  #> RMST,  E(min(D,t))  #>            Estimate  Std.Err   2.5%  97.5% P-value #> treatment0   0.6199 0.011340 0.5977 0.6421       0 #> treatment1   0.6543 0.007807 0.6390 0.6696       0 #>   #>                           Estimate Std.Err     2.5%     97.5% P-value #> [treatment0] - [treat.... -0.03446 0.01377 -0.06145 -0.007478 0.01231 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> mean events, E(N(min(D,t))):  #>            Estimate Std.Err   2.5%  97.5%   P-value #> treatment0   0.4523 0.06090 0.3329 0.5716 1.119e-13 #> treatment1   0.3739 0.07097 0.2348 0.5130 1.376e-07 #>   #>                           Estimate Std.Err    2.5%  97.5% P-value #> [treatment0] - [treat....  0.07835 0.09352 -0.1049 0.2616  0.4022 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> _______________________________________________________  #> Ratio of means E(N(min(D,t)))/E(min(D,t))  #>    Estimate Std.Err    2.5%    97.5%   P-value #> p1  -0.1676 0.06224 -0.2896 -0.04563 7.081e-03 #> p2  -0.2804 0.07192 -0.4214 -0.13947 9.651e-05 #>   #>             Estimate Std.Err     2.5%  97.5% P-value #> [p1] - [p2]   0.1128 0.09511 -0.07361 0.2992  0.2356 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0  #> _______________________________________________________  #> Mean of Events per time-unit E(N(min(D,t))/min(D,t))  #>        Estimate Std.Err    2.5%   97.5%   P-value #> treat0  -0.3833 0.04939 -0.4801 -0.2865 8.487e-15 #> treat1  -0.5380 0.05666 -0.6491 -0.4270 2.191e-21 #>   #>                     Estimate Std.Err     2.5%  97.5% P-value #> [treat0] - [treat1]   0.1548 0.07517 0.007459 0.3021 0.03948 #>  #>  Null Hypothesis:  #>   [treat0] - [treat1] = 0"},{"path":"http://kkholst.github.io/mets/articles/while-alive.html","id":"composite-outcomes-involving-death-and-marks","dir":"Articles","previous_headings":"","what":"Composite outcomes involving death and marks","title":"While Alive estimands for Recurrent Events","text":"number events can generalized various ways using outcomes N(D∧t)N(D \\wedge t), example,Ñ(D∧t)=∫0tI(D≥s)M(s)dN(s)+∑jMjI(D≤t,ϵ=j))\\begin{align*}  \\tilde N(D \\wedge t) = \\int_0^t (D \\geq s) M(s) dN(s)  +  \\sum_j  M_j (D \\leq t,\\epsilon=j) )  \\end{align*} M(s)M(s) marks related N(s)N(s) MjM_j marks associated different causes terminal event. provides extension weighted composite outcomes measure Mao & Lin (2022). marks (weights) can stochastic couting hosptial expenses, example, vector data-frame. marks event times (defined causes) used. weighting death weight 2 otherwise couting recurrent events (weight 1)","code":"hfactioncpx12$marks <- runif(nrow(hfactioncpx12))  ##ddmg <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2, ##cause=1:2,death.code=2,marks=hfactioncpx12$marks) ##summary(ddmg)  ddm <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2, cause=1:2,death.code=2,marks=hfactioncpx12$status)"},{"path":"http://kkholst.github.io/mets/articles/while-alive.html","id":"sessioninfo","dir":"Articles","previous_headings":"","what":"SessionInfo","title":"While Alive estimands for Recurrent Events","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] mets_1.3.9 #>  #> loaded via a namespace (and not attached): #>  [1] cli_3.6.5           knitr_1.51          rlang_1.1.6         #>  [4] xfun_0.55           textshaping_1.0.4   jsonlite_2.0.0      #>  [7] listenv_0.10.0      future.apply_1.20.1 lava_1.8.2          #> [10] htmltools_0.5.9     ragg_1.5.0          sass_0.4.10         #> [13] rmarkdown_2.30      grid_4.5.2          evaluate_1.0.5      #> [16] jquerylib_0.1.4     fastmap_1.2.0       numDeriv_2016.8-1.1 #> [19] yaml_2.3.12         mvtnorm_1.3-3       lifecycle_1.0.4     #> [22] timereg_2.0.7       compiler_4.5.2      codetools_0.2-20    #> [25] fs_1.6.6            htmlwidgets_1.6.4   Rcpp_1.1.0          #> [28] future_1.68.0       lattice_0.22-7      systemfonts_1.3.1   #> [31] digest_0.6.39       R6_2.6.1            parallelly_1.46.0   #> [34] parallel_4.5.2      splines_4.5.2       Matrix_1.7-4        #> [37] bslib_0.9.0         tools_4.5.2         globals_0.18.0      #> [40] survival_3.8-3      pkgdown_2.2.0       cachem_1.1.0        #> [43] desc_1.4.3"},{"path":"http://kkholst.github.io/mets/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Klaus K. Holst. Author, maintainer. Thomas Scheike. Author.","code":""},{"path":"http://kkholst.github.io/mets/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Thomas H. Scheike Klaus K. Holst (2022). Practical Guide Family Studies Lifetime Data. Annual Review Statistics Application 9, pp. 47-69. doi: 10.1146/annurev-statistics-040120-024253 Klaus K. Holst Thomas H. Scheike Jacob B. Hjelmborg (2016). Liability Threshold Model Censored Twin Data. Computational Statistics Data Analysis 93, pp. 324-335. doi: 10.1016/j.csda.2015.01.014 Thomas H. Scheike Klaus K. Holst Jacob B. Hjelmborg (2014). Estimating heritability cause specific mortality based twin studies. Lifetime Data Analysis 20 (2), pp. 210-233. doi: 10.1007/s10985-013-9244-x","code":"@Article{,   title = {A Practical Guide to Family Studies with Lifetime Data},   author = {Thomas H. Scheike and Klaus K. Holst},   year = {2014},   volume = {9},   pages = {47-69},   journal = {Annual Review of Statistics and Its Application},   doi = {10.1146/annurev-statistics-040120-024253}, } @Article{,   title = {The Liability Threshold Model for Censored Twin Data},   author = {Klaus K. Holst and Thomas H. Scheike and Jacob B. Hjelmborg},   year = {2016},   volume = {93},   pages = {324-335},   journal = {Computational Statistics and Data Analysis},   doi = {10.1016/j.csda.2015.01.014}, } @Article{,   title = {Estimating heritability for cause specific mortality based on twin studies},   author = {Thomas H. Scheike and Klaus K. Holst and Jacob B. Hjelmborg},   year = {2014},   volume = {20},   number = {2},   pages = {210-233},   journal = {Lifetime Data Analysis},   doi = {10.1007/s10985-013-9244-x}, }"},{"path":"http://kkholst.github.io/mets/index.html","id":"multivariate-event-times-mets-","dir":"","previous_headings":"","what":"Analysis of Multivariate Event Times","title":"Analysis of Multivariate Event Times","text":"Implementation various statistical models multivariate event history data doi:10.1007/s10985-013-9244-x. Including multivariate cumulative incidence models doi:10.1002/sim.6016, bivariate random effects probit models (Liability models) doi:10.1016/j.csda.2015.01.014. Modern methods survival analysis, including regression modelling (Cox, Fine-Gray, Ghosh-Lin, Binomial regression) fast computation influence functions. Restricted mean survival time regression years lost competing risks. Average treatment effects G-computation. functions can used clusters work large data.","code":""},{"path":"http://kkholst.github.io/mets/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Analysis of Multivariate Event Times","text":"development version may installed directly github (requires Rtools windows development tools (+Xcode) Mac OS X): get development version","code":"install.packages(\"mets\") remotes::install_github(\"kkholst/mets\", dependencies=\"Suggests\") remotes::install_github(\"kkholst/mets\",ref=\"develop\")"},{"path":"http://kkholst.github.io/mets/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Analysis of Multivariate Event Times","text":"cite mets package please use one following references Thomas H. Scheike Klaus K. Holst (2022). Practical Guide Family Studies Lifetime Data. Annual Review Statistics Application 9, pp. 47-69. doi: 10.1146/annurev-statistics-040120-024253 Thomas H. Scheike Klaus K. Holst Jacob B. Hjelmborg (2013). Estimating heritability cause specific mortality based twin studies. Lifetime Data Analysis. http://dx.doi.org/10.1007/s10985-013-9244-x Klaus K. Holst Thomas H. Scheike Jacob B. Hjelmborg (2015). Liability Threshold Model Censored Twin Data. Computational Statistics Data Analysis. http://dx.doi.org/10.1016/j.csda.2015.01.014 BibTeX: }","code":"@Article{,   title = {A Practical Guide to Family Studies with Lifetime Data},   author = {Thomas H. Scheike and Klaus K. Holst},   year = {2014},   volume = {9},   pages = {47-69},   journal = {Annual Review of Statistics and Its Application},   doi = {10.1146/annurev-statistics-040120-024253}, }  @Article{,   title={Estimating heritability for cause specific mortality based on twin studies},   author={Scheike, Thomas H. and Holst, Klaus K. and Hjelmborg, Jacob B.},   year={2013},   issn={1380-7870},   journal={Lifetime Data Analysis},   doi={10.1007/s10985-013-9244-x},   url={http://dx.doi.org/10.1007/s10985-013-9244-x},   publisher={Springer US},   keywords={Cause specific hazards; Competing risks; Delayed entry;         Left truncation; Heritability; Survival analysis},   pages={1-24},   language={English} @Article{,   title={The Liability Threshold Model for Censored Twin Data},   author={Holst, Klaus K. and Scheike, Thomas H. and Hjelmborg, Jacob B.},   year={2015},   doi={10.1016/j.csda.2015.01.014},   url={http://dx.doi.org/10.1016/j.csda.2015.01.014},   journal={Computational Statistics and Data Analysis} }"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-twins-polygenic-modelling","dir":"","previous_headings":"","what":"Examples: Twins Polygenic modelling","title":"Analysis of Multivariate Event Times","text":"First considering standard twin modelling (ACE, AE, ADE, models)","code":"# simulated data with pairs of observations in twins on long #data format set.seed(1) d <- twinsim(1000, b1=c(1,-1), b2=c(), acde=c(1,1,0,1)) # Polygenic model with Additive genetic effects, and shared and invidual environmental effects (ACE) ace <- twinlm(y ~ 1, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\") ace #>        Estimate Std. Error Z value  Pr(>|z|) #> y     -0.019439   0.041817 -0.4649     0.642 #> sd(A)  0.902004   0.203739  4.4273 9.544e-06 #> sd(C)  1.137025   0.132852  8.5586 < 2.2e-16 #> sd(E)  1.728992   0.037408 46.2194 < 2.2e-16 #>  #> MZ-pairs DZ-pairs  #>     1000     1000  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.15966  0.01867 0.30065 #> C 0.25370  0.13920 0.36820 #> E 0.58664  0.53677 0.63650 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.15966  0.01867 0.30065 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.41336  0.36229 0.46196 #> Correlation within DZ: 0.33353  0.27933 0.38561 #>  #> 'log Lik.' -8779.953 (df=4) #> AIC: 17567.91  #> BIC: 17590.31 # An AE-model could be fitted as ae <- twinlm(y ~ 1, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ae\") # AIC AIC(ae)-AIC(ace) #> [1] 15.20656 # To adjust for the covariates we simply alter the formula statement ace2 <- twinlm(y ~ x1+x2, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ace\")  ## Summary/GOF summary(ace2) #>        Estimate Std. Error  Z value Pr(>|z|) #> y     -0.026049   0.034844  -0.7476   0.4547 #> sd(A)  1.066060   0.072890  14.6256   <2e-16 #> sd(C)  0.980740   0.073569  13.3309   <2e-16 #> sd(E)  0.979980   0.021887  44.7736   <2e-16 #> y~x1   1.006963   0.021900  45.9807   <2e-16 #> y~x2  -0.993802   0.021962 -45.2512   <2e-16 #>  #> MZ-pairs DZ-pairs  #>     1000     1000  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.37156  0.27300 0.47012 #> C 0.31446  0.22643 0.40250 #> E 0.31398  0.28381 0.34414 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.37156  0.27300 0.47012 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.68602  0.65467 0.71502 #> Correlation within DZ: 0.50024  0.45538 0.54257 #>  #> 'log Lik.' -7449.697 (df=6) #> AIC: 14911.39  #> BIC: 14945"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-twins-polygenic-modelling-time-to-events-data","dir":"","previous_headings":"","what":"Examples: Twins Polygenic modelling time-to-events Data","title":"Analysis of Multivariate Event Times","text":"context time--events data consider “Liabilty Threshold model” IPCW adjustment censoring. First fit bivariate probit model (marginals MZ DZ twins different correlation parameter). evaluate risk getting cancer last double cancer event (95 years) Liability threshold model ACE random effects structure","code":"data(prt) prt0 <-  force.same.cens(prt, cause=\"status\", cens.code=0, time=\"time\", id=\"id\") prt0$country <- relevel(prt0$country, ref=\"Sweden\") prt_wide <- fast.reshape(prt0, id=\"id\", num=\"num\", varying=c(\"time\",\"status\",\"cancer\")) prt_time <- subset(prt_wide,  cancer1 & cancer2, select=c(time1, time2, zyg)) tau <- 95 tt <- seq(70, tau, length.out=5) ## Time points to evaluate model in  b0 <- bptwin.time(cancer ~ 1, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"cor\",               cens.formula=Surv(time,status==0)~zyg, breaks=tau) summary(b0) #>  #>                Estimate   Std.Err        Z   p-value     #> (Intercept)   -1.348188  0.026276 -51.3086 < 2.2e-16 *** #> atanh(rho) MZ  0.735992  0.087838   8.3789 < 2.2e-16 *** #> atanh(rho) DZ  0.353023  0.068234   5.1737 2.295e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  1994/3618   997/1809             #>  #>                            Estimate 2.5%    97.5%   #> Tetrachoric correlation MZ 0.62672  0.51081 0.72024 #> Tetrachoric correlation DZ 0.33905  0.21584 0.45164 #>  #> MZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.03504  0.02779 0.04409 #> Casewise Concordance 0.39458  0.31876 0.47584 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       4.44351  3.50521 5.38182 #> log(OR)              2.34131  1.87105 2.81157 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.01952  0.01449 0.02625 #> Casewise Concordance 0.21983  0.16667 0.28415 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       2.47556  1.81096 3.14016 #> log(OR)              1.23088  0.81020 1.65156 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.57533  0.25790 0.89276 #>  #>  #> Event of interest before time 95 b1 <- bptwin.time(cancer ~ 1, data=prt0, id=\"id\", zyg=\"zyg\", DZ=\"DZ\", type=\"ace\",            cens.formula=Surv(time,status==0)~zyg, breaks=tau) summary(b1) #>  #>             Estimate  Std.Err        Z p-value     #> (Intercept) -2.20664  0.16463 -13.4034  <2e-16 *** #> log(var(A))  0.43260  0.39149   1.1050  0.2691     #> log(var(C)) -1.98289  2.52342  -0.7858  0.4320     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  1994/3618   997/1809             #>  #>                    Estimate 2.5%     97.5%    #> A                   0.57533  0.25790  0.89276 #> C                   0.05139 -0.20836  0.31114 #> E                   0.37328  0.26874  0.47782 #> MZ Tetrachoric Cor  0.62672  0.51081  0.72024 #> DZ Tetrachoric Cor  0.33905  0.21584  0.45164 #>  #> MZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.03504  0.02779 0.04409 #> Casewise Concordance 0.39458  0.31876 0.47584 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       4.44351  3.50520 5.38182 #> log(OR)              2.34131  1.87104 2.81157 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.01952  0.01449 0.02625 #> Casewise Concordance 0.21983  0.16667 0.28415 #> Marginal             0.08880  0.08086 0.09743 #> Rel.Recur.Risk       2.47556  1.81096 3.14016 #> log(OR)              1.23088  0.81020 1.65156 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.57533  0.25790 0.89276 #>  #>  #> Event of interest before time 95"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-twins-concordance-for-time-to-events-data","dir":"","previous_headings":"","what":"Examples: Twins Concordance for time-to-events Data","title":"Analysis of Multivariate Event Times","text":"","code":"data(prt) ## Prostate data example (sim)  ## Bivariate competing risk, concordance estimates p33 <- bicomprisk(Event(time,status)~strata(zyg)+id(id),                   data=prt, cause=c(2,2), return.data=1, prodlim=TRUE) #> Strata 'DZ' #> Strata 'MZ'  p33dz <- p33$model$\"DZ\"$comp.risk p33mz <- p33$model$\"MZ\"$comp.risk  ## Probability weights based on Aalen's additive model (same censoring within pair) prtw <- ipw(Surv(time,status==0)~country+zyg, data=prt,             obs.only=TRUE, same.cens=TRUE,              cluster=\"id\", weight.name=\"w\")  ## Marginal model (wrongly ignoring censorings) bpmz <- biprobit(cancer~1 + cluster(id),                   data=subset(prt,zyg==\"MZ\"), eqmarg=TRUE)  ## Extended liability model bpmzIPW <- biprobit(cancer~1 + cluster(id),                     data=subset(prtw,zyg==\"MZ\"),                     weights=\"w\") smz <- summary(bpmzIPW)  ## Concordance plot(p33mz,ylim=c(0,0.1),axes=FALSE, automar=FALSE,atrisk=FALSE,background=TRUE,background.fg=\"white\") axis(2); axis(1)  abline(h=smz$prob[\"Concordance\",],lwd=c(2,1,1),col=\"darkblue\") ## Wrong estimates: abline(h=summary(bpmz)$prob[\"Concordance\",],lwd=c(2,1,1),col=\"lightgray\",lty=2)"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-cox-model-rmst","dir":"","previous_headings":"","what":"Examples: Cox model, RMST","title":"Analysis of Multivariate Event Times","text":"can fit Cox model compute many useful summaries, restricted mean survival stanardized treatment effects (G-estimation). First estimating standardized survival  Based phreg can also compute restricted mean survival time years lost (via Kaplan-Meier estimates). function times can plotted restricted mean survival years lost different time horizons  competing risks years lost can decomposed different causes based integrated Aalen-Johansen estimators different strata  Computations done time horizons illustrated plot.","code":"data(bmt)  bmt$time <- bmt$time+runif(408)*0.001  bmt$event <- (bmt$cause!=0)*1  dfactor(bmt) <- tcell.f~tcell   ss <- phreg(Surv(time,event)~tcell.f+platelet+age,bmt)   summary(survivalG(ss,bmt,50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%    P-value #> risk0   0.6539 0.02708 0.6008 0.7070 9.119e-129 #> risk1   0.5641 0.05973 0.4470 0.6811  3.600e-21 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err    2.5%   97.5% P-value #> ps0 -0.08982 0.06293 -0.2132 0.03352  0.1535 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate  Std.Err       2.5%      97.5%   P-value #> [ps0] -0.1477619 0.109562 -0.3624994 0.06697567 0.1774462 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.8626365 0.6959347 1.0692695  #>  #> Average Treatment effect:  survival-difference (G-estimator) : #>       Estimate    Std.Err        2.5%     97.5%   P-value #> ps0 0.08981829 0.06292811 -0.03351854 0.2131551 0.1534889 #>  #> Average Treatment effect: 1-G (survival)-ratio (G-estimator) : #> log-ratio:  #>       Estimate   Std.Err        2.5%     97.5%   P-value #> [ps0] 0.230711 0.1504459 -0.06415759 0.5255796 0.1251491 #> ratio:  #>  Estimate      2.5%     97.5%  #> 1.2594952 0.9378572 1.6914390   sst <- survivalGtime(ss,bmt,n=50)  plot(sst,type=c(\"survival\",\"risk\",\"survival.ratio\")[1]) out1 <- phreg(Surv(time,cause!=0)~strata(tcell,platelet),data=bmt)    rm1 <- resmean.phreg(out1, times=c(50))  summary(rm1) #>                     strata times    rmean se.rmean    lower    upper years.lost #> tcell=0, platelet=0      0    50 20.48245 1.411055 17.89542 23.44348   29.51755 #> tcell=0, platelet=1      1    50 28.33071 2.196175 24.33733 32.97934   21.66929 #> tcell=1, platelet=0      2    50 22.74596 4.053717 16.04005 32.25544   27.25404 #> tcell=1, platelet=1      3    50 26.11565 4.230688 19.01112 35.87517   23.88435  par(mfrow=c(1, 2))  plot(rm1,se=1)  plot(rm1,years.lost=TRUE,se=1) ## years.lost decomposed into causes  drm1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=50)  par(mfrow=c(1,2)); plot(drm1,cause=1,se=1); title(main=\"Cause 1\"); plot(drm1,cause=2,se=1); title(main=\"Cause 2\") summary(drm1) #> $estimate #>                     strata times   intF_1    intF_2 se.intF_1 se.intF_2 #> tcell=0, platelet=0      0    50 21.36784  8.149711  1.476647  1.094520 #> tcell=0, platelet=1      1    50 12.97924  8.690047  2.047516  1.712441 #> tcell=1, platelet=0      2    50 12.64543 14.608610  4.089981  3.730259 #> tcell=1, platelet=1      3    50 11.80934 12.075008  3.673701  3.890207 #>                     total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell=0, platelet=0         29.51755    18.661106     24.46717     6.263606 #> tcell=0, platelet=1         21.66929     9.527297     17.68191     5.905902 #> tcell=1, platelet=0         27.25404     6.708487     23.83649     8.856404 #> tcell=1, platelet=1         23.88435     6.418453     21.72807     6.421784 #>                     upper_intF_2 #> tcell=0, platelet=0     10.60376 #> tcell=0, platelet=1     12.78669 #> tcell=1, platelet=0     24.09685 #> tcell=1, platelet=1     22.70487"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-cox-model-iptw","dir":"","previous_headings":"","what":"Examples: Cox model IPTW","title":"Analysis of Multivariate Event Times","text":"can fit Cox model inverse probabilty treatment weights based logistic regression. treatment weights can time-dependent mutiplicative weights applied (see details vignette).","code":"data(bmt) bmt$time <- bmt$time+runif(408)*0.001 bmt$id <- seq_len(nrow(bmt)) bmt$event <- (bmt$cause!=0)*1 dfactor(bmt) <- tcell.f~tcell  fit <- phreg_IPTW(Surv(time,event)~tcell.f+cluster(id),data=bmt,treat.model=tcell.f~platelet+age)  summary(fit) #>  #>    n events #>  408    248 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> tcell.f1 -0.108497  0.199556  0.089653  0.5867 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> tcell.f1  0.89718 0.60676 1.3266 head(IC(fit)) #>    tcell.f1 #> 1 -1.639241 #> 2 -1.669074 #> 3 -1.749761 #> 4 -1.745988 #> 5 -1.625416 #> 6 -1.793372"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-competing-risks-regression-binomial-regression","dir":"","previous_headings":"","what":"Examples: Competing risks regression, Binomial Regression","title":"Analysis of Multivariate Event Times","text":"can fit logistic regression model specific time-point IPCW adjustment","code":"data(bmt); bmt$time <- bmt$time+runif(408)*0.001 # logistic regresion with IPCW binomial regression  out <- binreg(Event(time,cause)~tcell+platelet,bmt,time=50) summary(out) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.180371  0.126757 -0.428811  0.068068  0.1547 #> tcell       -0.418682  0.345438 -1.095729  0.258364  0.2255 #> platelet    -0.436959  0.240977 -0.909266  0.035349  0.0698 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.83496 0.65128 1.0704 #> tcell        0.65791 0.33430 1.2948 #> platelet     0.64600 0.40282 1.0360 head(IC(out)) #>           [,1]     [,2]    [,3] #> [1,] -2.834084 1.633524 2.52025 #> [2,] -2.834084 1.633524 2.52025 #> [3,] -2.834084 1.633524 2.52025 #> [4,] -2.834084 1.633524 2.52025 #> [5,] -2.834084 1.633524 2.52025 #> [6,] -2.834084 1.633524 2.52025  predict(out,data.frame(tcell=c(0,1),platelet=c(1,1)),se=TRUE) #>        pred         se     lower     upper #> 1 0.3503890 0.04848653 0.2553554 0.4454226 #> 2 0.2619201 0.06969710 0.1253138 0.3985265"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-competing-risks-regression-fine-graylogistic-link","dir":"","previous_headings":"","what":"Examples: Competing risks regression, Fine-Gray/Logistic link","title":"Analysis of Multivariate Event Times","text":"can fit Fine-Gray model logit-link competing risks model (using IPCW adjustment). Starting logit-link model  Similarly, Fine-Gray model can estimated using IPCW adjustment   can get standard errors predictions based influence functions baseline regression coefiicients (used predict function) G-estimation can done","code":"data(bmt) bmt$time <- bmt$time+runif(nrow(bmt))*0.01 bmt$id <- 1:nrow(bmt) ## logistic link  OR interpretation  or=cifreg(Event(time,cause)~strata(tcell)+platelet+age,data=bmt,cause=1) summary(or) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> platelet -0.454572  0.235415  0.187997  0.0535 #> age       0.390181  0.097675  0.083636  0.0001 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> platelet  0.63472 0.40013 1.0069 #> age       1.47725 1.21987 1.7889 par(mfrow=c(1,2))  ## to see baseline  plot(or)   # predictions  nd <- data.frame(tcell=c(1,0),platelet=0,age=0) pll <- predict(or,nd) plot(pll) ## Fine-Gray model  fg=cifreg(Event(time,cause)~strata(tcell)+platelet+age,data=bmt,cause=1,propodds=NULL)  summary(fg) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> platelet -0.424749  0.180772  0.187820  0.0188 #> age       0.341971  0.079862  0.086284  0.0000 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> platelet  0.65393 0.45884 0.9320 #> age       1.40772 1.20375 1.6462 ## baselines  plot(fg) nd <- data.frame(tcell=c(1,0),platelet=0,age=0) pfg <- predict(fg,nd,se=1) plot(pfg,se=1) ## influence functions of regression coefficients head(iid(fg)) #>         platelet           age #> [1,] 0.004953478  0.0001245648 #> [2,] 0.005348496 -0.0022341772 #> [3,] 0.006069271 -0.0087212019 #> [4,] 0.006043180 -0.0084186443 #> [5,] 0.004732097  0.0011839243 #> [6,] 0.006331457 -0.0121685409 baseid <- iidBaseline(fg,time=40) FGprediid(baseid,nd) #>           pred     se-log     lower     upper #> [1,] 0.2787465 0.23977109 0.1742272 0.4459672 #> [2,] 0.4506249 0.07265694 0.3908134 0.5195901 dfactor(bmt) <- tcell.f~tcell  fg1 <- cifreg(Event(time,cause)~tcell.f+platelet+age,bmt,cause=1,propodds=NULL)  summary(survivalG(fg1,bmt,50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.4332 0.02749 0.3793 0.4871 6.331e-56 #> risk1   0.2726 0.05861 0.1577 0.3875 3.301e-06 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err   2.5%    97.5% P-value #> ps0  -0.1606 0.06351 -0.285 -0.03609 0.01146 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err       2.5%       97.5%    P-value #> [ps0] -0.463091 0.2211651 -0.8965667 -0.02961528 0.03627159 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.6293354 0.4079679 0.9708190"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-marginal-mean-for-recurrent-events","dir":"","previous_headings":"","what":"Examples: Marginal mean for recurrent events","title":"Analysis of Multivariate Event Times","text":"can estimate expected number events non-parametrically get standard errors estimator","code":"data(hfactioncpx12) dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124  gl1 <- recurrentMarginal(Event(entry,time,status)~strata(treatment)+cluster(id),hfactioncpx12,cause=1,death.code=2) summary(gl1,times=1:5) #> [[1]] #>       new.time      mean         se   CI-2.5% CI-97.5% strata #> 325          1 0.8737156 0.06783343 0.7503858 1.017315      0 #> 555          2 1.5718563 0.09572955 1.3949953 1.771140      0 #> 682          3 2.1184963 0.11385721 1.9066915 2.353829      0 #> 748          4 2.6815219 0.15451005 2.3951619 3.002118      0 #> 748.1        5 2.6815219 0.15451005 2.3951619 3.002118      0 #>  #> [[2]] #>       new.time      mean         se   CI-2.5%  CI-97.5% strata #> 284          1 0.7815557 0.06908585 0.6572305 0.9293989      1 #> 499          2 1.4534055 0.10315606 1.2646561 1.6703258      1 #> 601          3 1.9240624 0.12165771 1.6998008 2.1779119      1 #> 645          4 2.3134997 0.14963892 2.0380418 2.6261880      1 #> 645.1        5 2.3134997 0.14963892 2.0380418 2.6261880      1 plot(gl1,se=1)"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-ghosh-lin-for-recurrent-events","dir":"","previous_headings":"","what":"Examples: Ghosh-Lin for recurrent events","title":"Analysis of Multivariate Event Times","text":"can fit Ghosh-Lin model expected number events observed dying (using IPCW adjustment get predictions) can get standard errors predictions based influence functions baseline regression coefiicients  influence functions baseline regression coefficients specific time-point can obtained G-computation","code":"data(hfactioncpx12) dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124  gl1 <- recreg(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,cause=1,death.code=2) summary(gl1) #>  #>     n events #>  2132   1391 #>  #>  741 clusters #> coeffients: #>             Estimate      S.E.   dU^-1/2 P-value #> treatment1 -0.110404  0.078656  0.053776  0.1604 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.89547 0.76754 1.0447  ## influence functions of regression coefficients head(iid(gl1)) #>      treatment1 #> 1 -1.266428e-04 #> 2 -6.112340e-04 #> 3  2.885192e-03 #> 4  1.308207e-03 #> 5  5.404664e-05 #> 6  2.229380e-03 nd=data.frame(treatment=levels(hfactioncpx12$treatment),id=1)  pfg <- predict(gl1,nd,se=1)  summary(pfg,times=1:5) #> $pred #>              Lamt     Lamt     Lamt     Lamt     Lamt #> strata0 0.8573256 1.592252 2.121181 2.635437 2.635437 #> strata0 0.7677110 1.425817 1.899458 2.359959 2.359959 #>  #> $se.pred #>       seLamt     seLamt    seLamt    seLamt    seLamt #> 1 0.05719895 0.08818784 0.1096157 0.1429941 0.1429941 #> 2 0.05763288 0.09495475 0.1184567 0.1484200 0.1484200 #>  #> $lower #>              [,1]     [,2]     [,3]     [,4]     [,5] #> strata0 0.7522383 1.428458 1.916860 2.369561 2.369561 #> strata0 0.6626698 1.251343 1.680916 2.086276 2.086276 #>  #> $upper #>              [,1]     [,2]     [,3]     [,4]     [,5] #> strata0 0.9770936 1.774827 2.347281 2.931145 2.931145 #> strata0 0.8894025 1.624617 2.146415 2.669546 2.669546 #>  #> $times #> [1] 1 2 3 4 5 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\"  plot(pfg,se=1) baseid <- iidBaseline(gl1,time=2) dd <- data.frame(treatment=levels(hfactioncpx12$treatment),id=1) GLprediid(baseid,dd) #>          pred     se-log    lower    upper #> [1,] 1.596065 0.05530215 1.432113 1.778786 #> [2,] 1.429231 0.06660096 1.254329 1.628521 hfactioncpx12$age <- (50+rnorm(741)*4)[hfactioncpx12$id]   GLout <- recreg(Event(entry,time,status)~treatment+age,data=hfactioncpx12,cause=1,death.code=2)  summary(GLout) #>  #>     n events #>  2132   1391 #>  #>  2132 clusters #> coeffients: #>              Estimate       S.E.    dU^-1/2 P-value #> treatment1 -0.1131085  0.0640898  0.0538154  0.0776 #> age         0.0086223  0.0079803  0.0066607  0.2799 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.89305 0.78763 1.0126 #> age         1.00866 0.99301 1.0246  summary(survivalG(GLout,hfactioncpx12,time=4)) #> G-estimator : #>       Estimate Std.Err  2.5% 97.5%    P-value #> risk0    2.640  0.1203 2.404 2.876 1.067e-106 #> risk1    2.358  0.1165 2.130 2.586  3.838e-91 #>  #> Average Treatment effect: difference (G-estimator) : #>    Estimate Std.Err    2.5%   97.5% P-value #> p1  -0.2824  0.1597 -0.5953 0.03059 0.07699 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>        Estimate    Std.Err       2.5%      97.5%    P-value #> [p1] -0.1131085 0.06408982 -0.2387222 0.01250527 0.07759015 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.8930538 0.7876336 1.0125838"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-fixed-time-modelling-for-recurrent-events","dir":"","previous_headings":"","what":"Examples: Fixed time modelling for recurrent events","title":"Analysis of Multivariate Event Times","text":"can fit log-link regression model 2 years expected number events observed dying (using IPCW adjustment)","code":"data(hfactioncpx12)  e2 <- recregIPCW(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,cause=1,death.code=2,time=2) summary(e2) #>    n events #>  741   1052 #>  #>  741 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  0.452430  0.060814  0.333236  0.571624  0.0000 #> treatment1  -0.078322  0.093560 -0.261696  0.105052  0.4025 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  1.57213 1.39548 1.7711 #> treatment1   0.92467 0.76974 1.1108 head(iid(e2)) #>            [,1]          [,2] #> 1  1.959479e-04 -2.266440e-04 #> 2  2.237613e-03 -2.227140e-03 #> 3 -9.349773e-06  1.293789e-03 #> 4 -9.653029e-04  9.653029e-04 #> 5 -1.203962e-04  6.744236e-05 #> 6 -2.861359e-03  2.871831e-03"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-regression-for-rmstrestricted-mean-survival-for-survival-and-competing-risks-using-ipcw","dir":"","previous_headings":"","what":"Examples: Regression for RMST/Restricted mean survival for survival and competing risks using IPCW","title":"Analysis of Multivariate Event Times","text":"RMST can computed using Kaplan-Meier (via phreg) competing risks via cumulative incidence functions, can also get estimates via IPCW adjustment can regression","code":"### same as Kaplan-Meier for full censoring model   bmt$int <- with(bmt,strata(tcell,platelet))  out <- resmeanIPCW(Event(time,cause!=0)~-1+int,bmt,time=30,                          cens.model=~strata(platelet,tcell),model=\"lin\")  estimate(out) #>                        Estimate Std.Err  2.5% 97.5%   P-value #> inttcell=0, platelet=0    13.61  0.8314 11.98 15.24 3.453e-60 #> inttcell=0, platelet=1    18.90  1.2694 16.42 21.39 3.717e-50 #> inttcell=1, platelet=0    16.19  2.4057 11.48 20.91 1.678e-11 #> inttcell=1, platelet=1    17.77  2.4532 12.96 22.58 4.391e-13  head(iid(out)) #>             [,1] [,2] [,3] [,4] #> [1,] -0.05341125    0    0    0 #> [2,] -0.05342611    0    0    0 #> [3,] -0.05343207    0    0    0 #> [4,] -0.05341706    0    0    0 #> [5,] -0.05342052    0    0    0 #> [6,] -0.05341259    0    0    0  ## same as   out1 <- phreg(Surv(time,cause!=0)~strata(tcell,platelet),data=bmt)  rm1 <- resmean.phreg(out1,times=30)  summary(rm1) #>                     strata times    rmean  se.rmean    lower    upper #> tcell=0, platelet=0      0    30 13.60584 0.8314012 12.07012 15.33695 #> tcell=0, platelet=1      1    30 18.90350 1.2690639 16.57288 21.56188 #> tcell=1, platelet=0      2    30 16.19410 2.4002390 12.11140 21.65306 #> tcell=1, platelet=1      3    30 17.76830 2.4417528 13.57289 23.26053 #>                     years.lost #> tcell=0, platelet=0   16.39416 #> tcell=0, platelet=1   11.09650 #> tcell=1, platelet=0   13.80590 #> tcell=1, platelet=1   12.23170    ## competing risks years-lost for cause 1    out1 <- resmeanIPCW(Event(time,cause)~-1+int,bmt,time=30,cause=1,                        cens.model=~strata(platelet,tcell),model=\"lin\")  estimate(out1) #>                        Estimate Std.Err   2.5%  97.5%   P-value #> inttcell=0, platelet=0   12.103  0.8507 10.436 13.770 6.168e-46 #> inttcell=0, platelet=1    6.883  1.1739  4.582  9.184 4.533e-09 #> inttcell=1, platelet=0    7.260  2.3529  2.648 11.871 2.033e-03 #> inttcell=1, platelet=1    5.779  2.0921  1.679  9.880 5.737e-03  ## same as   drm1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=30)  summary(drm1) #> $estimate #>                     strata times    intF_1   intF_2 se.intF_1 se.intF_2 #> tcell=0, platelet=0      0    30 12.103113 4.291051 0.8506728 0.6160195 #> tcell=0, platelet=1      1    30  6.882894 4.213603 1.1738590 0.9055124 #> tcell=1, platelet=0      2    30  7.259595 6.546309 2.3529175 1.9699198 #> tcell=1, platelet=1      3    30  5.779287 6.452411 2.0920912 2.0811678 #>                     total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell=0, platelet=0         16.39416    10.545569    13.890702     3.238664 #> tcell=0, platelet=1         11.09650     4.927208     9.614821     2.765212 #> tcell=1, platelet=0         13.80590     3.846168    13.702396     3.629546 #> tcell=1, platelet=1         12.23170     2.842764    11.749182     3.429056 #>                     upper_intF_2 #> tcell=0, platelet=0     5.685405 #> tcell=0, platelet=1     6.420645 #> tcell=1, platelet=0    11.807030 #> tcell=1, platelet=1    12.141421"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-average-treatment-effects-ate-for-survival-or-competing-risks","dir":"","previous_headings":"","what":"Examples: Average treatment effects (ATE) for survival or competing risks","title":"Analysis of Multivariate Event Times","text":"can compute ATE survival competing risks data probabilty dying restricted mean survival years-lost cause 1 event 0/1 thus leading restricted mean cause taking values 0,1,2 produces regression years lost due cause 1.","code":"bmt$event <- bmt$cause!=0; dfactor(bmt) <- tcell~tcell  brs <- binregATE(Event(time,cause)~tcell+platelet+age,bmt,time=50,cause=1,       treat.model=tcell~platelet+age)  summary(brs) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept) -0.19901  0.13098 -0.45574  0.05771  0.1287 #> tcell1      -0.63788  0.35668 -1.33696  0.06120  0.0737 #> platelet    -0.34411  0.24604 -0.82634  0.13811  0.1619 #> age          0.43737  0.10727  0.22712  0.64762  0.0000 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.81954 0.63398 1.0594 #> tcell1       0.52841 0.26264 1.0631 #> platelet     0.70885 0.43765 1.1481 #> age          1.54862 1.25497 1.9110 #>  #> Average Treatment effects (G-formula) : #>             Estimate    Std.Err       2.5%      97.5% P-value #> treat0     0.4288003  0.0275149  0.3748722  0.4827284  0.0000 #> treat1     0.2898471  0.0659033  0.1606789  0.4190153  0.0000 #> treat:1-0 -0.1389532  0.0717737 -0.2796272  0.0017208  0.0529 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.428211  0.027617  0.374084  0.482339  0.0000 #> treat1     0.250336  0.064792  0.123346  0.377325  0.0001 #> treat:1-0 -0.177876  0.070147 -0.315361 -0.040390  0.0112  head(brs$riskDR.iid) #>          iidriska      iidriska #> [1,] -0.001159043 -3.524810e-05 #> [2,] -0.001201108  7.613126e-05 #> [3,] -0.001326534  3.362333e-04 #> [4,] -0.001320393  3.250252e-04 #> [5,] -0.001140791 -9.095525e-05 #> [6,] -0.001398307  4.597688e-04  head(brs$riskG.iid) #>        riskGa.iid    riskGa.iid #> [1,] -0.001190759 -0.0001528426 #> [2,] -0.001242465  0.0001088968 #> [3,] -0.001355317  0.0006916069 #> [4,] -0.001350729  0.0006676909 #> [5,] -0.001164523 -0.0002838563 #> [6,] -0.001404170  0.0009471848 out <- resmeanATE(Event(time,event)~tcell+platelet,data=bmt,time=40,treat.model=tcell~platelet)  summary(out) #>    n events #>  408    241 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  2.852872  0.062472  2.730429  2.975315  0.0000 #> tcell1       0.021472  0.122886 -0.219381  0.262325  0.8613 #> platelet     0.303325  0.090731  0.125495  0.481155  0.0008 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 17.33750 15.33947 19.5958 #> tcell1       1.02170  0.80302  1.2999 #> platelet     1.35435  1.13371  1.6179 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.26491  0.95910 17.38511 21.14472  0.0000 #> treat1    19.68305  2.22794 15.31637 24.04973  0.0000 #> treat:1-0  0.41813  2.41074 -4.30684  5.14310  0.8623 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.28397  0.95792 17.40649 21.16146  0.0000 #> treat1    20.34809  2.54086 15.36811 25.32808  0.0000 #> treat:1-0  1.06412  2.70957 -4.24654  6.37478  0.6945  head(out$riskDR.iid) #>         iidriska    iidriska #> [1,] -0.05143041 0.005890787 #> [2,] -0.05144061 0.005890787 #> [3,] -0.05144470 0.005890787 #> [4,] -0.05143440 0.005890787 #> [5,] -0.05143678 0.005890787 #> [6,] -0.05143133 0.005890787  head(out$riskG.iid) #>       riskGa.iid  riskGa.iid #> [1,] -0.05185784 -0.01866183 #> [2,] -0.05186812 -0.01866485 #> [3,] -0.05187225 -0.01866606 #> [4,] -0.05186186 -0.01866301 #> [5,] -0.05186425 -0.01866372 #> [6,] -0.05185876 -0.01866211   out1 <- resmeanATE(Event(time,cause)~tcell+platelet,data=bmt,cause=1,time=40,                     treat.model=tcell~platelet)  summary(out1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  2.806167  0.069617  2.669721  2.942614  0.0000 #> tcell1      -0.374457  0.247756 -0.860051  0.111137  0.1307 #> platelet    -0.491638  0.164932 -0.814899 -0.168377  0.0029 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 16.54638 14.43594 18.9654 #> tcell1       0.68766  0.42314  1.1175 #> platelet     0.61162  0.44268  0.8450 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    14.53031  0.95690 12.65481 16.40581   0.000 #> treat1     9.99195  2.37789  5.33137 14.65253   0.000 #> treat:1-0 -4.53836  2.57483 -9.58494  0.50822   0.078 #>  #> Average Treatment effects (double robust) : #>             Estimate    Std.Err       2.5%      97.5% P-value #> treat0     14.512256   0.957862  12.634880  16.389632  0.0000 #> treat1      9.362018   2.416771   4.625234  14.098802  0.0001 #> treat:1-0  -5.150238   2.597631 -10.241501  -0.058975  0.0474"},{"path":"http://kkholst.github.io/mets/index.html","id":"examples-while-alive-estimands-for-recurrent-events","dir":"","previous_headings":"","what":"Examples: While Alive estimands for recurrent events","title":"Analysis of Multivariate Event Times","text":"consider RCT aim describe treatment effect via alive estimands","code":"data(hfactioncpx12)  dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124 dd <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2,death.code=2) summary(dd) #> While-Alive summaries:   #>  #> RMST,  E(min(D,t))  #>            Estimate Std.Err  2.5% 97.5% P-value #> treatment0    1.859 0.02108 1.817 1.900       0 #> treatment1    1.924 0.01502 1.894 1.953       0 #>   #>                           Estimate Std.Err    2.5%    97.5% P-value #> [treatment0] - [treat.... -0.06517 0.02588 -0.1159 -0.01444  0.0118 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> mean events, E(N(min(D,t))):  #>            Estimate Std.Err  2.5% 97.5%   P-value #> treatment0    1.572 0.09573 1.384 1.759 1.375e-60 #> treatment1    1.453 0.10315 1.251 1.656 4.376e-45 #>   #>                           Estimate Std.Err    2.5%  97.5% P-value #> [treatment0] - [treat....   0.1185  0.1407 -0.1574 0.3943     0.4 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> _______________________________________________________  #> Ratio of means E(N(min(D,t)))/E(min(D,t))  #>    Estimate Std.Err   2.5%  97.5%   P-value #> p1   0.8457 0.05264 0.7425 0.9488 4.411e-58 #> p2   0.7555 0.05433 0.6490 0.8619 5.963e-44 #>   #>             Estimate Std.Err     2.5%  97.5% P-value #> [p1] - [p2]  0.09022 0.07565 -0.05805 0.2385   0.233 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0  #> _______________________________________________________  #> Mean of Events per time-unit E(N(min(D,t))/min(D,t))  #>        Estimate Std.Err   2.5%  97.5%   P-value #> treat0   1.0725  0.1222 0.8331 1.3119 1.645e-18 #> treat1   0.7552  0.0643 0.6291 0.8812 7.508e-32 #>   #>                     Estimate Std.Err    2.5%  97.5% P-value #> [treat0] - [treat1]   0.3173  0.1381 0.04675 0.5879 0.02153 #>  #>  Null Hypothesis:  #>   [treat0] - [treat1] = 0  dd <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2,            death.code=2,trans=.333) summary(dd,type=\"log\") #> While-Alive summaries, log-scale:   #>  #> RMST,  E(min(D,t))  #>            Estimate  Std.Err   2.5%  97.5% P-value #> treatment0   0.6199 0.011340 0.5977 0.6421       0 #> treatment1   0.6543 0.007807 0.6390 0.6696       0 #>   #>                           Estimate Std.Err     2.5%     97.5% P-value #> [treatment0] - [treat.... -0.03446 0.01377 -0.06145 -0.007478 0.01231 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> mean events, E(N(min(D,t))):  #>            Estimate Std.Err   2.5%  97.5%   P-value #> treatment0   0.4523 0.06090 0.3329 0.5716 1.119e-13 #> treatment1   0.3739 0.07097 0.2348 0.5130 1.376e-07 #>   #>                           Estimate Std.Err    2.5%  97.5% P-value #> [treatment0] - [treat....  0.07835 0.09352 -0.1049 0.2616  0.4022 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> _______________________________________________________  #> Ratio of means E(N(min(D,t)))/E(min(D,t))  #>    Estimate Std.Err    2.5%    97.5%   P-value #> p1  -0.1676 0.06224 -0.2896 -0.04563 7.081e-03 #> p2  -0.2804 0.07192 -0.4214 -0.13947 9.651e-05 #>   #>             Estimate Std.Err     2.5%  97.5% P-value #> [p1] - [p2]   0.1128 0.09511 -0.07361 0.2992  0.2356 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0  #> _______________________________________________________  #> Mean of Events per time-unit E(N(min(D,t))/min(D,t))  #>        Estimate Std.Err    2.5%   97.5%   P-value #> treat0  -0.3833 0.04939 -0.4801 -0.2865 8.487e-15 #> treat1  -0.5380 0.05666 -0.6491 -0.4270 2.191e-21 #>   #>                     Estimate Std.Err     2.5%  97.5% P-value #> [treat0] - [treat1]   0.1548 0.07517 0.007459 0.3021 0.03948 #>  #>  Null Hypothesis:  #>   [treat0] - [treat1] = 0"},{"path":"http://kkholst.github.io/mets/reference/ACTG175.html","id":null,"dir":"Reference","previous_headings":"","what":"ACTG175, block randmized study from speff2trial package — ACTG175","title":"ACTG175, block randmized study from speff2trial package — ACTG175","text":"Data speff2trial","code":""},{"path":"http://kkholst.github.io/mets/reference/ACTG175.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACTG175, block randmized study from speff2trial package — ACTG175","text":"Randomized study","code":""},{"path":"http://kkholst.github.io/mets/reference/ACTG175.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACTG175, block randmized study from speff2trial package — ACTG175","text":"Hammer et al. 1996, speff2trial package.","code":""},{"path":"http://kkholst.github.io/mets/reference/ACTG175.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACTG175, block randmized study from speff2trial package — ACTG175","text":"","code":"data(ACTG175)"},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":null,"dir":"Reference","previous_headings":"","what":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"Computes  augmentation term individual well sum $$ = \\int_0^t H(u,X) \\frac{1}{S^*(u,s)} \\frac{1}{G_c(u)} dM_c(u) $$ $$ H(u,X) = F_1^*(t,s) - F_1^*(u,s) $$ using KM $$G_c(t)$$ working model cumulative baseline related $$F_1^*(t,s)$$ $$s$$ strata, $$S^*(t,s) = 1 - F_1^*(t,s) - F_2^*(t,s)$$.","code":""},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"","code":"BinAugmentCifstrata(   formula,   data = data,   cause = 1,   cens.code = 0,   km = TRUE,   time = NULL,   weights = NULL,   offset = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"formula formula 'Event', strata model CIF given strata, strataC specifies censoring strata data data frame cause interest cens.code code censoring km use Kaplan-Meier time interest weights weights estimating equations offset offsets logistic regression ... Additional arguments binreg function.","code":""},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"Standard errors computed assumption correct $$G_c(s)$$ model.","code":""},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/BinAugmentCifstrata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Augmentation for Binomial regression based on stratified NPMLE Cif (Aalen-Johansen) — BinAugmentCifstrata","text":"","code":"library(mets) data(bmt) dcut(bmt,breaks=2) <- ~age  out1<-BinAugmentCifstrata(Event(time,cause)~platelet+agecat.2+       strata(platelet,agecat.2),data=bmt,cause=1,time=40) summary(out1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>                      Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)          -0.51295  0.17090 -0.84791 -0.17799  0.0027 #> platelet             -0.63011  0.23585 -1.09237 -0.16785  0.0075 #> agecat.2(0.203,1.94]  0.55926  0.21211  0.14353  0.97500  0.0084 #>  #> exp(coeffients): #>                      Estimate    2.5%  97.5% #> (Intercept)           0.59873 0.42831 0.8370 #> platelet              0.53253 0.33542 0.8455 #> agecat.2(0.203,1.94]  1.74938 1.15434 2.6512 #>  #>   out2<-BinAugmentCifstrata(Event(time,cause)~platelet+agecat.2+     strata(platelet,agecat.2)+strataC(platelet),data=bmt,cause=1,time=40) summary(out2) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>                      Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)          -0.51346  0.17109 -0.84879 -0.17814  0.0027 #> platelet             -0.63636  0.23653 -1.09996 -0.17276  0.0071 #> agecat.2(0.203,1.94]  0.56280  0.21229  0.14672  0.97889  0.0080 #>  #> exp(coeffients): #>                      Estimate    2.5%  97.5% #> (Intercept)           0.59842 0.42793 0.8368 #> platelet              0.52922 0.33288 0.8413 #> agecat.2(0.203,1.94]  1.75559 1.15803 2.6615 #>  #>"},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Wild bootstrap for Cox PH regression — Bootphreg","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"wild bootstrap uniform bands Cox models","code":""},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"","code":"Bootphreg(   formula,   data,   offset = NULL,   weights = NULL,   B = 1000,   type = c(\"exp\", \"poisson\", \"normal\"),   ... )"},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"formula formula 'Surv' outcome (see coxph) data data frame offset offsets cox model weights weights Cox score equations B bootstraps type distribution multiplier ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"Wild bootstrap based confidence intervals multiplicative hazards models, Dobler, Pauly, Scheike (2018),","code":""},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"Klaus K. Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/Bootphreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wild bootstrap for Cox PH regression — Bootphreg","text":"","code":"n <- 100  x <- 4*rnorm(n)  time1 <- 2*rexp(n)/exp(x*0.3)  time2 <- 2*rexp(n)/exp(x*(-0.3))  status <- ifelse(time1<time2,1,2)  time <- pmin(time1,time2)  rbin <- rbinom(n,1,0.5)  cc <-rexp(n)*(rbin==1)+(rbin==0)*rep(3,n)  status <- ifelse(time < cc,status,0)  time  <- ifelse(time < cc,time,cc)  data <- data.frame(time=time,status=status,x=x)   b1 <- Bootphreg(Surv(time,status==1)~x,data,B=1000)  b2 <- Bootphreg(Surv(time,status==2)~x,data,B=1000)  c1 <- phreg(Surv(time,status==1)~x,data)  c2 <- phreg(Surv(time,status==2)~x,data)   ### exp to make all bootstraps positive  out <- pred.cif.boot(b1,b2,c1,c2,gplot=0)   cif.true <- (1-exp(-out$time))*.5  with(out,plot(time,cif,ylim=c(0,1),type=\"l\"))  lines(out$time,cif.true,col=3)  with(out,plotConfRegion(time,band.EE,col=1))  with(out,plotConfRegion(time,band.EE.log,col=3))  with(out,plotConfRegion(time,band.EE.log.o,col=2))"},{"path":"http://kkholst.github.io/mets/reference/CPH_HPN_CRBSI.html","id":null,"dir":"Reference","previous_headings":"","what":"Rates for HPN program for patients of Copenhagen Cohort — CPH_HPN_CRBSI","title":"Rates for HPN program for patients of Copenhagen Cohort — CPH_HPN_CRBSI","text":"Rates HPN program patients Copenhagen Cohort","code":""},{"path":"http://kkholst.github.io/mets/reference/CPH_HPN_CRBSI.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Rates for HPN program for patients of Copenhagen Cohort — CPH_HPN_CRBSI","text":"crbsi: cumulative rate catheter related bloodstream infection HPN patients Copenhagen  mechanical: cumulative rate Mechanical (hole/defect) complication catheter HPN patients Copenhagen trombo: cumulative rate Occlusion/Thrombosis complication catheter HPN patients Copenhagen terminal: rate terminal event, patients leaving HPN program","code":""},{"path":"http://kkholst.github.io/mets/reference/CPH_HPN_CRBSI.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Rates for HPN program for patients of Copenhagen Cohort — CPH_HPN_CRBSI","text":"Estimated data","code":""},{"path":"http://kkholst.github.io/mets/reference/ClaytonOakes.html","id":null,"dir":"Reference","previous_headings":"","what":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","title":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","text":"Clayton-Oakes frailty model","code":""},{"path":"http://kkholst.github.io/mets/reference/ClaytonOakes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","text":"","code":"ClaytonOakes(   formula,   data = parent.frame(),   cluster,   var.formula = ~1,   cuts = NULL,   type = \"piecewise\",   start,   control = list(),   var.invlink = exp,   ... )"},{"path":"http://kkholst.github.io/mets/reference/ClaytonOakes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","text":"formula formula specifying marginal proportional (piecewise constant) hazard structure right-hand-side survival object (Surv) specifying entry time (optional), follow-time, event/censoring status follow-. clustering can specified using special function cluster (see example ). data Data frame cluster Variable defining clustering (given formula) var.formula Formula specifying variance component structure (given via cluster special function formula) using linear model log-link. cuts Cut points defining piecewise constant hazard type equal two.stage, Clayton-Oakes-Glidden estimator calculated via timereg package start Optional starting values control Control parameters optimization routine var.invlink Inverse link function variance structure model ... Additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/ClaytonOakes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/ClaytonOakes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clayton-Oakes model with piece-wise constant hazards — ClaytonOakes","text":"","code":"set.seed(1) d <- subset(simClaytonOakes(500,4,2,1,stoptime=2,left=2),truncated) e <- ClaytonOakes(survival::Surv(lefttime,time,status)~x+cluster(~1,cluster),                   cuts=c(0,0.5,1,2),data=d) e #>                     Estimate     2.5%   97.5% #> log-Var:(Intercept) -0.92226 -1.16926 -0.6753 #> x                    2.88805  2.58174  3.2307 #> (0,0.5]              1.07892  0.95530  1.2185 #> (0.5,1]              1.21696  1.06966  1.3845 #> (1,2]                1.16268  1.01451  1.3325 #>  #> Dependence parameters: #>             Variance    2.5%   97.5% Kendall's tau    2.5%  97.5% #> (Intercept)  0.39762 0.31060 0.50902       0.16584 0.13442 0.2029  d2 <- simClaytonOakes(500,4,2,1,stoptime=2,left=0) d2$z <- rep(1,nrow(d2)); d2$z[d2$cluster%in%sample(d2$cluster,100)] <- 0 ## Marginal=Cox Proportional Hazards model: ## ts <- ClaytonOakes(survival::Surv(time,status)~timereg::prop(x)+cluster(~1,cluster), ##                   data=d2,type=\"two.stage\") ## Marginal=Aalens additive model: ## ts2 <- ClaytonOakes(survival::Surv(time,status)~x+cluster(~1,cluster), ##                    data=d2,type=\"two.stage\") ## Marginal=Piecewise constant: e2 <- ClaytonOakes(survival::Surv(time,status)~x+cluster(~-1+factor(z),cluster),                    cuts=c(0,0.5,1,2),data=d2) e2 #>                    Estimate     2.5%   97.5% #> log-Var:factor(z)0 -0.58294 -0.95495 -0.2109 #> log-Var:factor(z)1 -0.52286 -0.71031 -0.3354 #> x                   2.57827  2.37165  2.8029 #> (0,0.5]             1.03878  0.94306  1.1442 #> (0.5,1]             1.00684  0.89868  1.1280 #> (1,2]               0.89075  0.78213  1.0144 #>  #> Dependence parameters: #>            Variance    2.5%   97.5% Kendall's tau    2.5%  97.5% #> factor(z)0  0.55826 0.38483 0.80983       0.21822 0.16137 0.2882 #> factor(z)1  0.59282 0.49149 0.71505       0.22864 0.19727 0.2634   e0 <- ClaytonOakes(survival::Surv(time,status)~cluster(~-1+factor(z),cluster),                    cuts=c(0,0.5,1,2),data=d2) ##ts0 <- ClaytonOakes(survival::Surv(time,status)~cluster(~1,cluster), ##                   data=d2,type=\"two.stage\") ##plot(ts0) plot(e0) #> Error in plot.xy(xy.coords(x, y), type = type, ...): plot.new has not been called yet  e3 <- ClaytonOakes(survival::Surv(time,status)~x+cluster(~1,cluster),cuts=c(0,0.5,1,2),                    data=d,var.invlink=identity) e3 #>                 Estimate    2.5%  97.5% #> Var:(Intercept)  0.50280 0.37659 0.6290 #> x                3.07849 2.75254 3.4430 #> (0,0.5]          0.82748 0.73495 0.9317 #> (0.5,1]          0.97306 0.85420 1.1085 #> (1,2]            1.00635 0.87488 1.1576 #>  #> Dependence parameters: #>             Variance    2.5%   97.5% Kendall's tau    2.5%  97.5% #> (Intercept)  0.50280 0.37659 0.62901       0.20090 0.15846 0.2393"},{"path":"http://kkholst.github.io/mets/reference/Dbvn.html","id":null,"dir":"Reference","previous_headings":"","what":"Derivatives of the bivariate normal cumulative distribution function — Dbvn","title":"Derivatives of the bivariate normal cumulative distribution function — Dbvn","text":"Derivatives bivariate normal cumulative distribution function","code":""},{"path":"http://kkholst.github.io/mets/reference/Dbvn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Derivatives of the bivariate normal cumulative distribution function — Dbvn","text":"","code":"Dbvn(p,design=function(p,...) {      return(list(mu=cbind(p[1],p[1]),                dmu=cbind(1,1),                S=matrix(c(p[2],p[3],p[3],p[4]),ncol=2),                dS=rbind(c(1,0,0,0),c(0,1,1,0),c(0,0,0,1)))  )},                       Y=cbind(0,0))"},{"path":"http://kkholst.github.io/mets/reference/Dbvn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Derivatives of the bivariate normal cumulative distribution function — Dbvn","text":"p Parameter vector design Design function defines mean, derivative mean, variance, derivative variance respect parameter p Y column vector CDF evaluated","code":""},{"path":"http://kkholst.github.io/mets/reference/Dbvn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Derivatives of the bivariate normal cumulative distribution function — Dbvn","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":null,"dir":"Reference","previous_headings":"","what":"Relative risk for additive gamma model — EVaddGam","title":"Relative risk for additive gamma model — EVaddGam","text":"Computes relative risk additive gamma model time 0","code":""},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative risk for additive gamma model — EVaddGam","text":"","code":"EVaddGam(theta, x1, x2, thetades, ags)"},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative risk for additive gamma model — EVaddGam","text":"theta theta x1 x1 x2 x2 thetades thetades ags ags","code":""},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Relative risk for additive gamma model — EVaddGam","text":"Eriksson Scheike (2015), Additive Gamma frailty models competing risks data, Biometrics (2015)","code":""},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Relative risk for additive gamma model — EVaddGam","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/EVaddGam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relative risk for additive gamma model — EVaddGam","text":"","code":"lam0 <- c(0.5,0.3) pars <- c(1,1,1,1,0,1) ## genetic random effects, cause1, cause2 and overall parg <- pars[c(1,3,5)] ## environmental random effects, cause1, cause2 and overall parc <- pars[c(2,4,6)]  ## simulate competing risks with two causes with hazards 0.5 and 0.3 ## ace for each cause, and overall ace out <- simCompete.twin.ace(10000,parg,parc,0,2,lam0=lam0,overall=1,all.sum=1)  ## setting up design for running the model mm <- familycluster.index(out$cluster) head(mm$familypairindex,n=10) #>  [1]  1  2  3  4  5  6  7  8  9 10 pairs <- matrix(mm$familypairindex,ncol=2,byrow=TRUE) tail(pairs,n=12) #>           [,1]  [,2] #>  [9989,] 19977 19978 #>  [9990,] 19979 19980 #>  [9991,] 19981 19982 #>  [9992,] 19983 19984 #>  [9993,] 19985 19986 #>  [9994,] 19987 19988 #>  [9995,] 19989 19990 #>  [9996,] 19991 19992 #>  [9997,] 19993 19994 #>  [9998,] 19995 19996 #>  [9999,] 19997 19998 #> [10000,] 19999 20000 # kinship <- (out[pairs[,1],\"zyg\"]==\"MZ\")+ (out[pairs[,1],\"zyg\"]==\"DZ\")*0.5  # dout <- make.pairwise.design.competing(pairs,kinship, #          type=\"ace\",compete=length(lam0),overall=1) # head(dout$ant.rvs) ## MZ # dim(dout$theta.des) # dout$random.design[,,1] ## DZ # dout$theta.des[,,nrow(pairs)] # dout$random.design[,,nrow(pairs)] # # thetades <- dout$theta.des[,,1] # x <- dout$random.design[,,1] # x ##EVaddGam(rep(1,6),x[1,],x[3,],thetades,matrix(1,18,6))  # thetades <- dout$theta.des[,,nrow(out)/2] # x <- dout$random.design[,,nrow(out)/2] ##EVaddGam(rep(1,6),x[1,],x[4,],thetades,matrix(1,18,6))"},{"path":"http://kkholst.github.io/mets/reference/Effbinreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Efficient IPCW for binary data — Effbinreg","title":"Efficient IPCW for binary data — Effbinreg","text":"Simple version comp.risk function timereg just one time-point thus fitting model $$E(T \\leq t | X ) = expit( X^T beta) $$","code":""},{"path":"http://kkholst.github.io/mets/reference/Effbinreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Efficient IPCW for binary data — Effbinreg","text":"","code":"Effbinreg(   formula,   data,   cause = 1,   time = NULL,   beta = NULL,   offset = NULL,   weights = NULL,   cens.weights = NULL,   cens.model = ~+1,   se = TRUE,   kaplan.meier = TRUE,   cens.code = 0,   no.opt = FALSE,   method = \"nr\",   augmentation = NULL,   h = NULL,   MCaugment = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/Effbinreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Efficient IPCW for binary data — Effbinreg","text":"formula formula outcome (see coxph) data data frame cause cause interest time time interest beta starting values offset offsets partial likelihood weights score equations cens.weights censoring weights cens.model stratified cox model without covariates se compute se's  based IPCW kaplan.meier uses Kaplan-Meier IPCW contrast exp(-Baseline) cens.code gives censoring code .opt optimize method optimization augmentation augment binomial regression h h estimating equation MCaugment iid h censoring model ... Additional arguments lower level funtions model exp linear","code":""},{"path":"http://kkholst.github.io/mets/reference/Effbinreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Efficient IPCW for binary data — Effbinreg","text":"Based binomial regresion IPCW response estimating equation: $$ X  ( \\Delta (T \\leq t)/G_c(T_i-) - expit( X^T beta)) = 0 $$ IPCW adjusted responses. Based binomial regresion IPCW response estimating equation: $$ h(X) X ( \\Delta (T \\leq t)/G_c(T_i-) - expit( X^T beta)) = 0 $$ IPCW adjusted responses $h$ given argument together iid censoring h. using appropriately h argument can also efficient IPCW estimator estimator works prepsurv prepcif survival competing risks data. case also censoring martingale given variance calculation also comes prepsurv prepcif functions. (Experimental version stage). Variance based  $$ \\sum w_i^2 $$ also IPCW adjustment, naive.var variance known censoring model. Censoring model may depend strata.","code":""},{"path":"http://kkholst.github.io/mets/reference/Effbinreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Efficient IPCW for binary data — Effbinreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":null,"dir":"Reference","previous_headings":"","what":"Event history object — Event","title":"Event history object — Event","text":"Constructur Event History objects","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Event history object — Event","text":"","code":"Event(time, time2 = TRUE, cause = NULL, cens.code = 0, ...)"},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Event history object — Event","text":"time Time time2 Time 2 cause Cause cens.code Censoring code (default 0) ... Additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Event history object — Event","text":"Object class Event (matrix)","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Event history object — Event","text":"... content details","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Event history object — Event","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/Event.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Event history object — Event","text":"","code":"t1 <- 1:10   t2 <- t1+runif(10)   ca <- rbinom(10,2,0.4)   (x <- Event(t1,t2,ca)) #>  [1] ( 1; 1.171086:0] ( 2; 2.333062:0] ( 3; 3.183510:1] ( 4; 4.128726:1] #>  [5] ( 5; 5.503349:1] ( 6; 6.767420:1] ( 7; 7.931043:1] ( 8; 8.471621:1] #>  [9] ( 9; 9.953553:1] (10;10.700965:0]"},{"path":"http://kkholst.github.io/mets/reference/EventSplit2.html","id":null,"dir":"Reference","previous_headings":"","what":"Event split with two time-scales, time and gaptime — EventSplit2","title":"Event split with two time-scales, time and gaptime — EventSplit2","text":"Cuts time two time-scales, event.split","code":""},{"path":"http://kkholst.github.io/mets/reference/EventSplit2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Event split with two time-scales, time and gaptime — EventSplit2","text":"","code":"EventSplit2(   data,   time = \"time\",   status = \"status\",   entry = \"start\",   cuts = \"cuts\",   name.id = \"id\",   gaptime = NULL,   gaptime.entry = NULL,   cuttime = c(\"time\", \"gaptime\"),   cens.code = 0,   order.id = TRUE )"},{"path":"http://kkholst.github.io/mets/reference/EventSplit2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Event split with two time-scales, time and gaptime — EventSplit2","text":"data data split time time variable. status status variable. entry name entry variable. cuts cuts variable numeric cut (one value) name.id name id variable. gaptime gaptime variable. gaptime.entry name entry variable gaptime. cuttime cut time gaptime cens.code code censoring. order.id order data id start.","code":""},{"path":"http://kkholst.github.io/mets/reference/EventSplit2.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Event split with two time-scales, time and gaptime — EventSplit2","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/EventSplit2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Event split with two time-scales, time and gaptime — EventSplit2","text":"","code":"rr  <- data.frame(time=c(500,1000),start=c(0,500),status=c(1,1),id=c(1,1)) rr$gaptime <-  rr$time-rr$start rr$gapstart <- 0  rr1 <- EventSplit2(rr,cuts=600,cuttime=\"time\",   gaptime=\"gaptime\",gaptime.entry=\"gapstart\") rr2 <- EventSplit2(rr1,cuts=100,cuttime=\"gaptime\",gaptime=\"gaptime\",gaptime.entry=\"gapstart\")  dlist(rr1,start-time+status+gapstart+gaptime~id) #> id: 1 #>     start time status gapstart gaptime #> 1     0    500 1        0      500     #> 2   500    600 0        0      100     #> 2.1 600   1000 1      100      500     dlist(rr2,start-time+status+gapstart+gaptime~id) #> id: 1 #>     start time status gapstart gaptime #> 1     0    100 0        0      100     #> 1.1 100    500 1      100      500     #> 2   500    600 0        0      100     #> 2.1 600   1000 1      100      500"},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":null,"dir":"Reference","previous_headings":"","what":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"Computes  augmentation term individual well sum $$ (\\beta) = \\int H(t,X,\\beta) \\frac{F_2^*(t,s)}{S^*(t,s)} \\frac{1}{G_c(t)} dM_c $$ $$ H(t,X,\\beta) = \\int_t^\\infty (X - E(\\beta,t) ) G_c(t) d\\Lambda_1^*(t,s) $$ using KM $$G_c(t)$$ working model cumulative baseline related $$F_1^*(t,s)$$ $$s$$ strata, $$S^*(t,s) = 1 - F_1^*(t,s) - F_2^*(t,s)$$, $$E(\\beta^p,t)$$ given. Assumes strata baseline ine-Gay model augmented.","code":""},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"","code":"FG_AugmentCifstrata(   formula,   data = data,   E = NULL,   cause = NULL,   cens.code = 0,   km = TRUE,   case.weights = NULL,   weights = NULL,   offset = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"formula formula 'Event', strata model CIF given strata, strataC specifies censoring strata data data frame E FG-model cause interest cens.code code censoring km use Kaplan-Meier case.weights weights FG score equations (follow dN_1) weights weights FG score equations offset offsets FG   model ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"couple iterations end solution $$ \\int (X - E(\\beta) ) Y_1(t) w(t) dM_1 + (\\beta) $$ augmented FG-score. Standard errors computed assumption correct $$G_c$$ model.","code":""},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/FG_AugmentCifstrata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Augmentation for Fine-Gray model based on stratified NPMLE Cif (Aalen-Johansen) — FG_AugmentCifstrata","text":"","code":"library(mets) set.seed(100) rho1 <- 0.2; rho2 <- 10 n <- 100 beta=c(0.0,-0.1,-0.5,0.3) dats <- simul.cifs(n,rho1,rho2,beta,rc=0.2) dtable(dats,~status) #>  #> status #>  0  1  2  #>  6 13 81  #>  dsort(dats) <- ~time fg <- cifreg(Event(time,status)~Z1+Z2,data=dats,cause=1,propodds=NULL) summary(fg) #>  #>    n events #>  100     13 #>  #>  100 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1 -0.25559  0.27563  0.28698  0.3538 #> Z2  0.43883  0.55113  0.57407  0.4259 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  0.77446 0.45121 1.3293 #> Z2  1.55089 0.52658 4.5677 #>  plot(fg);  lines(attr(dats,\"Lam1\"),col=2)   fgaugS <- FG_AugmentCifstrata(Event(time,status)~Z1+Z2+strata(Z1,Z2),data=dats,cause=1,E=fg$E) summary(fgaugS) #>  #>    n events #>  100     13 #>  #>  100 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1 -0.25559  0.27360  0.28698  0.3502 #> Z2  0.43883  0.54675  0.57407  0.4222 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  0.77446 0.45301 1.3240 #> Z2  1.55089 0.53111 4.5287 #>  fgaugS2 <- FG_AugmentCifstrata(Event(time,status)~Z1+Z2+strata(Z1,Z2),data=dats,cause=1,E=fgaugS$E) summary(fgaugS2) #>  #>    n events #>  100     13 #>  #>  100 clusters #> coeffients: #>    Estimate     S.E.  dU^-1/2 P-value #> Z1 -0.25559  0.27360  0.28698  0.3502 #> Z2  0.43883  0.54675  0.57407  0.4222 #>  #> exp(coeffients): #>    Estimate    2.5%  97.5% #> Z1  0.77446 0.45301 1.3240 #> Z2  1.55089 0.53111 4.5287 #>"},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":null,"dir":"Reference","previous_headings":"","what":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"Fits random effects  model describing dependence cumulative incidence curves subjects within cluster.  Given gamma distributed random effects assumed cumulative incidence curves indpendent, marginal cumulative incidence curves additive form $$ P(T \\leq t, cause=1 | x,z) = P_1(t,x,z) = 1- exp( -x^T (t) - t z^T \\beta) $$","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"","code":"Grandom.cif(   cif,   data,   cause = NULL,   cif2 = NULL,   times = NULL,   cause1 = 1,   cause2 = 1,   cens.code = NULL,   cens.model = \"KM\",   Nit = 40,   detail = 0,   clusters = NULL,   theta = NULL,   theta.des = NULL,   weights = NULL,   step = 1,   sym = 0,   same.cens = FALSE,   censoring.weights = NULL,   silent = 1,   var.link = 0,   score.method = \"nr\",   entry = NULL,   estimator = 1,   trunkp = 1,   admin.cens = NULL,   random.design = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"cif model object timereg::comp.risk function marginal cumulative incidence cause2, .e., event conditioned , whose odds comparision made respect data data.frame variables. cause specifies causes  related death times, value cens.code censoring value. cif2 specificies model cause2 different cause1. times time points cause1 cause first coordinate. cause2 cause second coordinate. cens.code specificies code censoring NULL uses one marginal cif model. cens.model specified model use ICPW, KM Kaplan-Meier alternatively may \"cox\" Nit number iterations Newton-Raphson algorithm. detail 0 details printed iterations, 1 details given. clusters specifies cluster structure. theta specifies starting values cross-odds-ratio parameters model. theta.des specifies regression design cross-odds-ratio parameters. weights weights score equations. step specifies step size Newton-Raphson algorith.m sym 1 symmetri 0 otherwise .cens true censoring within clusters assumed variable, default independent censoring. censoring.weights Censoring probabilities silent debug information var.link var.link=1 var log-scale. score.method default uses \"nlminb\" optimzer, alternatively, use \"nr\" algorithm. entry entry-age case delayed entry. two causes must given. estimator estimator trunkp gives probability survival delayed entry, related entry-ages given . admin.cens Administrative censoring random.design specifies regression design 0/1's random effects. ... extra arguments.","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"returns object type 'random.cif'. following arguments: theta estimate parameters model. var.theta variance gamma. hess derivative used score. score scores final stage. theta.iid matrix iid decomposition parametric effects.","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"allow regression structure indenpendent gamma distributed random effects  variances may depend cluster covariates. random.design specificies random effects subject within cluster. matrix 1's 0's dimension n x d.  d random effects. cluster two subjects, let random.design rows \\(v_1\\) \\(v_2\\). random effects subject 1 $$v_1^T (Z_1,...,Z_d)$$, d random effects. random effect associated parameter \\((\\lambda_1,...,\\lambda_d)\\). construction subjects 1's random effect Gamma distributed mean \\(\\lambda_1/v_1^T \\lambda\\) variance \\(\\lambda_1/(v_1^T \\lambda)^2\\). Note random effect \\(v_1^T (Z_1,...,Z_d)\\) mean 1 variance \\(1/(v_1^T \\lambda)\\). parameters \\((\\lambda_1,...,\\lambda_d)\\) related parameters model regression construction \\(pard\\) (d x k), links \\(d\\) \\(\\lambda\\) parameters (k) underlying \\(\\theta\\) parameters $$ \\lambda = pard \\theta $$","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"Semiparametric Random Effects Model Multivariate Competing Risks Data, Scheike, Zhang, Sun, Jensen (2010), Biometrika. Cross odds ratio Modelling dependence Multivariate Competing Risks Data, Scheike Sun (2013), Biostatitistics. Scheike, Holst, Hjelmborg (2014),  LIDA, Estimating heritability cause specific hazards based twin data","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/Grandom.cif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Additive Random effects model for competing risks data for polygenetic modelling — Grandom.cif","text":"","code":"## Reduce Ex.Timings  d <- simnordic.random(5000,delayed=TRUE,        cordz=1.0,cormz=2,lam0=0.3,country=TRUE)  times <- seq(50,90,by=10)  addm <- timereg::comp.risk(Event(time,cause)~-1+factor(country)+cluster(id),data=d,  times=times,cause=1,max.clust=NULL)   ### making group indidcator   mm <- model.matrix(~-1+factor(zyg),d)   out1m<-random.cif(addm,data=d,cause1=1,cause2=1,theta=1,        theta.des=mm,same.cens=TRUE)  summary(out1m) #> Random effect variance for variation due to clusters #>  #> Cause 1 and cause 1 #>  #>  #>                   Coef.        SE        z        P-val Cross odds ratio #> factor(zyg)MZ 2.2995032 0.3309552 6.948079 3.703038e-12         3.299503 #> factor(zyg)DZ 0.8440647 0.1826187 4.622005 3.800487e-06         1.844065 #>                      SE #> factor(zyg)MZ 0.3309552 #> factor(zyg)DZ 0.1826187    ## this model can also be formulated as a random effects model   ## but with different parameters  out2m<-Grandom.cif(addm,data=d,cause1=1,cause2=1,         theta=c(0.5,1),step=1.0,         random.design=mm,same.cens=TRUE)  summary(out2m) #> Random effect parameters for additive gamma random effects  #>  #> Cause 1 and cause 1 #>  #>  #>      Coef.     SE    z   P-val #> [1,] 0.435 0.0626 6.95 3.7e-12 #> [2,] 1.180 0.2560 4.62 3.8e-06 #> $estimate #>      Coef.     SE    z   P-val #> [1,] 0.435 0.0626 6.95 3.7e-12 #> [2,] 1.180 0.2560 4.62 3.8e-06 #>  #> $h #>    Estimate Std.Err   2.5% 97.5%   P-value #> p1   0.3671 0.09538 0.1801 0.554 0.0001189 #> p2   1.0000 0.00000 1.0000 1.000 0.0000000 #>   1/out2m$theta #>           [,1] #> [1,] 2.2995032 #> [2,] 0.8440647  out1m$theta #>           [,1] #> [1,] 2.2995032 #> [2,] 0.8440647    ####################################################################  ################### ACE modelling of twin data #####################  ####################################################################  ### assume that zygbin gives the zygosity of mono and dizygotic twins  ### 0 for mono and 1 for dizygotic twins. We now formulate and AC model  zygbin <- d$zyg==\"DZ\"   n <- nrow(d)  ### random effects for each cluster  des.rv <- cbind(mm,(zygbin==1)*rep(c(1,0)),(zygbin==1)*rep(c(0,1)),1)  ### design making parameters half the variance for dizygotic components  pardes <- rbind(c(1,0), c(0.5,0),c(0.5,0), c(0.5,0), c(0,1))   outacem <-Grandom.cif(addm,data=d,cause1=1,cause2=1,     same.cens=TRUE,theta=c(0.35,0.15),             step=1.0,theta.des=pardes,random.design=des.rv)  summary(outacem) #> Random effect parameters for additive gamma random effects  #>  #> Cause 1 and cause 1 #>  #>  #>        Coef.     SE      z    P-val #> [1,]  0.4660 0.0787  5.920 3.14e-09 #> [2,] -0.0313 0.0815 -0.384 7.01e-01 #> $estimate #>        Coef.     SE      z    P-val #> [1,]  0.4660 0.0787  5.920 3.14e-09 #> [2,] -0.0313 0.0815 -0.384 7.01e-01 #>  #> $h #>    Estimate Std.Err    2.5%  97.5%   P-value #> p1  1.07202  0.1921  0.6956 1.4485 2.383e-08 #> p2 -0.07202  0.1921 -0.4485 0.3044 7.077e-01 #>"},{"path":"http://kkholst.github.io/mets/reference/LinSpline.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple linear spline — LinSpline","title":"Simple linear spline — LinSpline","text":"Simple linear spline","code":""},{"path":"http://kkholst.github.io/mets/reference/LinSpline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple linear spline — LinSpline","text":"","code":"LinSpline(x, knots, num = TRUE, name = \"Spline\")"},{"path":"http://kkholst.github.io/mets/reference/LinSpline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple linear spline — LinSpline","text":"x variable make spline knots cut points num give names x1 x2 forth name name spline expansion name.1 name.2 forth","code":""},{"path":"http://kkholst.github.io/mets/reference/LinSpline.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simple linear spline — LinSpline","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/TRACE.html","id":null,"dir":"Reference","previous_headings":"","what":"The TRACE study group of myocardial infarction — TRACE","title":"The TRACE study group of myocardial infarction — TRACE","text":"TRACE data frame contains 1877 patients subset data set consisting approximately 6000 patients.  contains data relating survival patients myocardial infarction various risk factors.","code":""},{"path":"http://kkholst.github.io/mets/reference/TRACE.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The TRACE study group of myocardial infarction — TRACE","text":"data frame contains following columns: id numeric vector. Patient code. status numeric vector code. Survival status. 9: dead myocardial infarction, 0: alive, 7: dead causes. time numeric vector. Survival time years. chf numeric vector code. Clinical heart pump failure, 1: present, 0: absent. diabetes numeric vector code. Diabetes, 1: present, 0: absent. vf numeric vector code. Ventricular fibrillation, 1: present, 0: absent. wmi numeric vector. Measure heart pumping effect based ultrasound measurements 2 normal 0 worst. sex numeric vector code. 1: female, 0: male. age numeric vector code. Age patient.","code":""},{"path":"http://kkholst.github.io/mets/reference/TRACE.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The TRACE study group of myocardial infarction — TRACE","text":"TRACE study group. Jensen, G.V., Torp-Pedersen, C., Hildebrandt, P., Kober, L., F. E. Nielsen, Melchior, T., Joen, T. P. K. Andersen (1997), -hospital ventricular fibrillation affect prognosis myocardial infarction?, European Heart Journal 18, 919–924.","code":""},{"path":"http://kkholst.github.io/mets/reference/TRACE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The TRACE study group of myocardial infarction — TRACE","text":"sTRACE subsample consisting 300 patients. tTRACE subsample consisting 1000 patients.","code":""},{"path":"http://kkholst.github.io/mets/reference/TRACE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The TRACE study group of myocardial infarction — TRACE","text":"","code":"data(TRACE) names(TRACE) #> [1] \"id\"       \"wmi\"      \"status\"   \"chf\"      \"age\"      \"sex\"      \"diabetes\" #> [8] \"time\"     \"vf\""},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":null,"dir":"Reference","previous_headings":"","what":"While-Alive estimands for recurrent events — WA_recurrent","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"Considers ratio means $$E(N(min(D,t)))/E(min(D,t))$$ mean events per time unit $$E(N(min(D,t))/min(D,t))$$ based IPCW etimation. RMST estimator equivalent Kaplan-Meier based estimator.","code":""},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"","code":"WA_recurrent(   formula,   data,   time = NULL,   cens.code = 0,   cause = 1,   death.code = 2,   trans = NULL,   cens.formula = NULL,   augmentR = NULL,   augmentC = NULL,   type = NULL,   marks = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"formula Event formula first covariate rhs must factor giving treatment data data frame time estimation cens.code censorings cause events death.code terminal events trans possible power mean events per time-unit cens.formula censoring model, default use strata(treatment) augmentR covariates model mean ratio augmentC covariates censoring augmentation type augmentation call binreg, augmentC given default \"\" otherwise \"II\" marks possible marks composite outcome situation model counts marks ... arguments binregATE","code":""},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"Nonparametric estimation Patient Weighted -Alive Estimand arXiv preprint . Ragni, T. Martinussen, T. Scheike Mao, L. (2023). Nonparametric inference general -alive estimands recurrent events. Biometrics, 79(3):1749–1760. Schmidli, H., Roger, J. H., Akacha, M. (2023). Estimands recurrent event endpoints presence terminal event. Statistics Biopharmaceutical Research, 15(2):238–248.","code":""},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/WA_recurrent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"While-Alive estimands for recurrent events — WA_recurrent","text":"","code":"library(mets) data(hfactioncpx12)  dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124  #>  dd <- WA_recurrent(Event(entry,time,status)~treatment+cluster(id),hfactioncpx12,time=2,death.code=2) summary(dd) #> While-Alive summaries:   #>  #> RMST,  E(min(D,t))  #>            Estimate Std.Err  2.5% 97.5% P-value #> treatment0    1.859 0.02108 1.817 1.900       0 #> treatment1    1.924 0.01502 1.894 1.953       0 #>   #>                           Estimate Std.Err    2.5%    97.5% P-value #> [treatment0] - [treat.... -0.06517 0.02588 -0.1159 -0.01444  0.0118 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> mean events, E(N(min(D,t))):  #>            Estimate Std.Err  2.5% 97.5%   P-value #> treatment0    1.572 0.09573 1.384 1.759 1.375e-60 #> treatment1    1.453 0.10315 1.251 1.656 4.376e-45 #>   #>                           Estimate Std.Err    2.5%  97.5% P-value #> [treatment0] - [treat....   0.1185  0.1407 -0.1574 0.3943     0.4 #>  #>  Null Hypothesis:  #>   [treatment0] - [treatment1] = 0  #> _______________________________________________________  #> Ratio of means E(N(min(D,t)))/E(min(D,t))  #>    Estimate Std.Err   2.5%  97.5%   P-value #> p1   0.8457 0.05264 0.7425 0.9488 4.411e-58 #> p2   0.7555 0.05433 0.6490 0.8619 5.963e-44 #>   #>             Estimate Std.Err     2.5%  97.5% P-value #> [p1] - [p2]  0.09022 0.07565 -0.05805 0.2385   0.233 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0  #> _______________________________________________________  #> Mean of Events per time-unit E(N(min(D,t))/min(D,t))  #>        Estimate Std.Err   2.5%  97.5%   P-value #> treat0   1.0725  0.1222 0.8331 1.3119 1.645e-18 #> treat1   0.7552  0.0643 0.6291 0.8812 7.508e-32 #>   #>                     Estimate Std.Err    2.5%  97.5% P-value #> [treat0] - [treat1]   0.3173  0.1381 0.04675 0.5879 0.02153 #>  #>  Null Hypothesis:  #>   [treat0] - [treat1] = 0"},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast additive hazards model with robust standard errors — aalenMets","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"Fast Lin-Ying additive hazards model possibly stratified baseline. Robust variance default variance summary.","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"","code":"aalenMets(formula, data = data, no.baseline = FALSE, ...)"},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"formula formula 'Surv' outcome (see coxph) data data frame .baseline fit model without baseline hazard ... Additional arguments phreg","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"influence functions (iid) follow numerical order given cluster variable ordering $id give iid order data-set.","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenMets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast additive hazards model with robust standard errors — aalenMets","text":"","code":"data(bmt); bmt$time <- bmt$time+runif(408)*0.001 out <- aalenMets(Surv(time,cause==1)~tcell+platelet+age,data=bmt) summary(out) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>            Estimate       S.E.    dU^-1/2 P-value #> tcell    -0.0129601  0.0041293  0.2303828  0.0017 #> platelet -0.0087422  0.0028056  0.1664323  0.0018 #> age       0.0066203  0.0013880  0.0789265  0.0000 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> tcell     0.98712 0.97917 0.9951 #> platelet  0.99130 0.98586 0.9968 #> age       1.00664 1.00391 1.0094 #>   ## out2 <- timereg::aalen(Surv(time,cause==1)~const(tcell)+const(platelet)+const(age),data=bmt) ## summary(out2)"},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Aalen frailty model — aalenfrailty","title":"Aalen frailty model — aalenfrailty","text":"Additive hazards model (gamma) frailty","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aalen frailty model — aalenfrailty","text":"","code":"aalenfrailty(time, status, X, id, theta, B = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aalen frailty model — aalenfrailty","text":"time Time variable status Status variable (0,1) X Covariate design matrix id cluster variable theta list thetas (returns score evaluated ), starting point optimization (defaults magic number 0.1) B (optional) Cumulative coefficients (update theta fixing B) ... Additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aalen frailty model — aalenfrailty","text":"Parameter estimates","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Aalen frailty model — aalenfrailty","text":"Aalen frailty model","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Aalen frailty model — aalenfrailty","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/aalenfrailty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Aalen frailty model — aalenfrailty","text":"","code":"library(\"timereg\") #> Loading required package: survival #>  #> Attaching package: ‘timereg’ #> The following objects are masked from ‘package:mets’: #>  #>     Event, event.split, kmplot, plotConfregion dd <- simAalenFrailty(5000) f <- ~1##+x X <- model.matrix(f,dd) ## design matrix for non-parametric terms system.time(out<-timereg::aalen(update(f,Surv(time,status)~.),dd,n.sim=0,robust=0)) #>    user  system elapsed  #>    0.01    0.00    0.01  dix <- which(dd$status==1) t1 <- system.time(bb <- .Call(\"Bhat\",as.integer(dd$status),                               X,0.2,as.integer(dd$id),NULL,NULL,                               PACKAGE=\"mets\")) spec <- 1 ##plot(out,spec=spec) ## plot(dd$time[dix],bb$B2[,spec],col=\"red\",type=\"s\", ##      ylim=c(0,max(dd$time)*c(beta0,beta)[spec])) ## abline(a=0,b=c(beta0,beta)[spec]) ##'  if (FALSE) { # \\dontrun{ thetas <- seq(0.1,2,length.out=10) Us <- unlist(aalenfrailty(dd$time,dd$status,X,dd$id,as.list(thetas))) ##plot(thetas,Us,type=\"l\",ylim=c(-.5,1)); abline(h=0,lty=2); abline(v=theta,lty=2) op <- aalenfrailty(dd$time,dd$status,X,dd$id) op } # }"},{"path":"http://kkholst.github.io/mets/reference/back2timereg.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to timereg object — back2timereg","title":"Convert to timereg object — back2timereg","text":"convert timereg object","code":""},{"path":"http://kkholst.github.io/mets/reference/back2timereg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to timereg object — back2timereg","text":"","code":"back2timereg(obj)"},{"path":"http://kkholst.github.io/mets/reference/back2timereg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to timereg object — back2timereg","text":"obj use","code":""},{"path":"http://kkholst.github.io/mets/reference/back2timereg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Convert to timereg object — back2timereg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/basehazplot.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting the baselines of stratified Cox — basehazplot.phreg","title":"Plotting the baselines of stratified Cox — basehazplot.phreg","text":"Plotting baselines stratified Cox","code":""},{"path":"http://kkholst.github.io/mets/reference/basehazplot.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting the baselines of stratified Cox — basehazplot.phreg","text":"","code":"basehazplot.phreg(   x,   se = FALSE,   time = NULL,   add = FALSE,   ylim = NULL,   xlim = NULL,   lty = NULL,   col = NULL,   lwd = NULL,   legend = TRUE,   ylab = NULL,   xlab = NULL,   polygon = TRUE,   level = 0.95,   stratas = NULL,   robust = FALSE,   conf.type = c(\"plain\", \"log\"),   ... )"},{"path":"http://kkholst.github.io/mets/reference/basehazplot.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting the baselines of stratified Cox — basehazplot.phreg","text":"x phreg object se include standard errors time plot specific time variables add add previous plot ylim give ylim xlim give xlim lty specify lty components col specify col components lwd specify lwd components legend specify col components ylab specify ylab xlab specify xlab polygon get standard error shaded form level standard errors stratas wich strata plot robust use robust standard errors possible conf.type \"plain\" \"log\" transformed ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/basehazplot.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plotting the baselines of stratified Cox — basehazplot.phreg","text":"Klaus K. Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/basehazplot.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plotting the baselines of stratified Cox — basehazplot.phreg","text":"","code":"data(TRACE) dcut(TRACE) <- ~. out1 <- phreg(Surv(time,status==9)~vf+chf+strata(wmicat.4),data=TRACE)  par(mfrow=c(2,2)) plot(out1) plot(out1,stratas=c(0,3)) plot(out1,stratas=c(0,3),col=2:3,lty=1:2,se=TRUE) plot(out1,stratas=c(0),col=2,lty=2,se=TRUE,polygon=FALSE)  plot(out1,stratas=c(0),col=matrix(c(2,1,3),1,3),lty=matrix(c(1,2,3),1,3),se=TRUE,polygon=FALSE)"},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimation of concordance in bivariate competing risks data — bicomprisk","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"Estimation concordance bivariate competing risks data","code":""},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"","code":"bicomprisk(   formula,   data,   cause = c(1, 1),   cens = 0,   causes,   indiv,   strata = NULL,   id,   num,   max.clust = 1000,   marg = NULL,   se.clusters = NULL,   wname = NULL,   prodlim = FALSE,   messages = TRUE,   model,   return.data = 0,   uniform = 0,   conservative = 1,   resample.iid = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"formula Formula left-hand-side Event object (see example ) left-hand-side specying covariate structure data Data frame cause Causes (default (1,1)) estimate bivariate cumulative incidence cens censoring code causes causes indiv indiv strata Strata id Clustering variable num num max.clust max number clusters timereg::comp.risk call iid decompostion, max.clust=NULL uses clusters otherwise rougher grouping. marg marginal cumulative incidence make stanard errors clusters subsequent use casewise.test() se.clusters specify clusters standard errors. Either vector cluster indices column name data. Defaults id variable. wname name additonal weight used paired competing risks data. prodlim prodlim use prodlim estimator (Aalen-Johansen) rather IPCW weighted estimator based comp.risk function.equivalent case covariates. esimators case stratified fitting. messages Control amount output model Type competing risk model (default Fine-Gray model \"fg\", see comp.risk). return.data data returned (skipping modeling) uniform compute uniform standard errors concordance estimates based resampling. conservative conservative standard errors, recommended larger data-sets. resample.iid return iid residual processes computations tests. ... Additional arguments timereg::comp.risk function","code":""},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"Scheike, T. H.; Holst, K. K. & Hjelmborg, J. B. Estimating twin concordance bivariate competing risks twin data Statistics Medicine, Wiley Online Library, 2014 , 33 , 1193-204","code":""},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"Thomas Scheike, Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/bicomprisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimation of concordance in bivariate competing risks data — bicomprisk","text":"","code":"library(\"timereg\")  ## Simulated data example prt <- simnordic.random(2000,delayed=TRUE,ptrunc=0.7,         cordz=0.5,cormz=2,lam0=0.3) ## Bivariate competing risk, concordance estimates p11 <- bicomprisk(Event(time,cause)~strata(zyg)+id(id),data=prt,cause=c(1,1)) #> Strata 'MZ' #> Strata 'DZ'  p11mz <- p11$model$\"MZ\" p11dz <- p11$model$\"DZ\" par(mfrow=c(1,2)) ## Concordance plot(p11mz,ylim=c(0,0.1)); plot(p11dz,ylim=c(0,0.1));  ## entry time, truncation weighting ### other weighting procedure prtl <-  prt[!prt$truncated,] prt2 <- ipw2(prtl,cluster=\"id\",same.cens=TRUE,      time=\"time\",cause=\"cause\",entrytime=\"entry\",      pairs=TRUE,strata=\"zyg\",obs.only=TRUE)  prt22 <- fast.reshape(prt2,id=\"id\")  prt22$event <- (prt22$cause1==1)*(prt22$cause2==1)*1 prt22$timel <- pmax(prt22$time1,prt22$time2) ipwc <- timereg::comp.risk(Event(timel,event)~-1+factor(zyg1),   data=prt22,cause=1,n.sim=0,model=\"rcif2\",times=50:90,   weights=prt22$weights1,cens.weights=rep(1,nrow(prt22)))  p11wmz <- ipwc$cum[,2] p11wdz <- ipwc$cum[,3] lines(ipwc$cum[,1],p11wmz,col=3) lines(ipwc$cum[,1],p11wdz,col=3)"},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"pairwise pairwise odds ratio model provides alternative alternating logistic regression (ALR).","code":""},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"","code":"binomial.twostage(   margbin,   data = parent.frame(),   method = \"nr\",   detail = 0,   clusters = NULL,   silent = 1,   weights = NULL,   theta = NULL,   theta.des = NULL,   var.link = 0,   var.par = 1,   var.func = NULL,   iid = 1,   notaylor = 1,   model = \"plackett\",   marginal.p = NULL,   beta.iid = NULL,   Dbeta.iid = NULL,   strata = NULL,   max.clust = NULL,   se.clusters = NULL,   numDeriv = 0,   random.design = NULL,   pairs = NULL,   dim.theta = NULL,   additive.gamma.sum = NULL,   pair.ascertained = 0,   case.control = 0,   no.opt = FALSE,   twostage = 1,   beta = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"margbin Marginal binomial model data data frame method Scoring method \"nr\", lava NR optimizer detail Detail clusters Cluster variable silent Debug information weights Weights log-likelihood, can used type outcome 2x2 tables. theta Starting values variance components theta.des design dependence parameters, pairs given indeces theta-design pair, given pairs column 5 var.link Link function variance var.par parametrization var.func alternative parametrizations used function can specify paramters related \\(\\lambda_j\\)'s. iid Calculate ..d. decomposition iid>=1, iid=2 avoids adding uncertainty marginal paramters additive gamma model (default). notaylor Taylor expansion model model marginal.p vector marginal probabilities beta.iid iid decomposition marginal probability  estimates subject, based GLM model computed. Dbeta.iid derivatives marginal model wrt marginal parameters, based GLM model computed. strata strata fitting: considers pairs strata max.clust max clusters se.clusters clusters iid decomposition roubst standard errors numDeriv uses Fisher scoring aprox second derivative 0, otherwise numerical derivatives random.design random effect design additive gamma model, pairs given indeces pairs random.design rows given columns 3:4 pairs matrix rows indeces (two-columns) pairs considered pairwise composite score, useful case-control sampling marginal known. dim.theta dimension theta pairs pairs specific design given. pairs 6 columns. additive.gamma.sum specification lamtot models via matrix multiplied onto parameters theta (dimensions=(number random effects x number theta parameters), null sums parameters. Default matrix 1's pair.ascertained pairs sampled events pair .e. Y1+Y2>=1. case.control data case control data pair call, 2nd column pairs probands (cases controls) .opt optimizing twostage default twostage=1, fit MLE use twostage=0 beta starting value beta MLE version ... NR lava","code":""},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"reported standard errors based cluster corrected score equations pairwise likelihoods assuming marginals known. gives correct standard errors case Odds-Ratio model (Plackett distribution) dependence, incorrect standard errors Clayton-Oakes types model (also called \"gamma\"-frailty). additive gamma version standard errors adjusted uncertainty marginal models via iid deomposition using iid() function lava. clayton oakes model speicifed via random effects can fixed subsequently using iid influence functions marginal model, typically change much. Clayton-Oakes version model, given gamma distributed random effects assumed probabilities indpendent, marginal survival functions logistic form $$ logit(P(Y=1|X)) = \\alpha + x^T \\beta $$ therefore conditional random effect probability event $$ logit(P(Y=1|X,Z)) = exp( -Z \\cdot Laplace^{-1}(lamtot,lamtot,P(Y=1|x)) ) $$ Can also fit structured additive gamma random effects model, ACE, ADE model survival data: Now random.design specificies random effects subject within cluster. matrix 1's 0's dimension n x d.  d random effects. cluster two subjects, let random.design rows  \\(v_1\\) \\(v_2\\). random effects subject 1 $$v_1^T (Z_1,...,Z_d)$$, d random effects. random effect associated parameter \\((\\lambda_1,...,\\lambda_d)\\). construction subjects 1's random effect Gamma distributed mean \\(\\lambda_j/v_1^T \\lambda\\) variance \\(\\lambda_j/(v_1^T \\lambda)^2\\). Note random effect \\(v_1^T (Z_1,...,Z_d)\\) mean 1 variance \\(1/(v_1^T \\lambda)\\). asssumed  \\(lamtot=v_1^T \\lambda\\) fixed clusters ACE model . DEFAULT parametrization uses variances random effecs (var.par=1) $$ \\theta_j  = \\lambda_j/(v_1^T \\lambda)^2 $$ alternative parametrizations (var.par=0) one can specify parameters relate \\(\\lambda_j\\) function Based parameters relative contribution (heritability, h) equivalent  expected values random effects  \\(\\lambda_j/v_1^T \\lambda\\) Given random effects probabilities  independent form $$ logit(P(Y=1|X)) = exp( - Laplace^{-1}(lamtot,lamtot,P(Y=1|x)) ) $$ inverse laplace gamma distribution mean 1 variance lamtot. parameters \\((\\lambda_1,...,\\lambda_d)\\) related parameters model regression construction \\(pard\\) (d x k), links \\(d\\) \\(\\lambda\\) parameters (k) underlying \\(\\theta\\) parameters $$ \\lambda = theta.des  \\theta $$ using theta.des specify low-dimension association. Default diagonal matrix.","code":""},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"Two-stage binomial modelling","code":""},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binomial.twostage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fits Clayton-Oakes or bivariate Plackett (OR) models for binary data using marginals that are on logistic form. If clusters contain more than two times, the algoritm uses a compososite likelihood based on all pairwise bivariate models. — binomial.twostage","text":"","code":"data(twinstut) twinstut0 <- subset(twinstut, tvparnr<4000) twinstut <- twinstut0 twinstut$binstut <- (twinstut$stutter==\"yes\")*1 theta.des <- model.matrix( ~-1+factor(zyg),data=twinstut) margbin <- glm(binstut~factor(sex)+age,data=twinstut,family=binomial()) bin <- binomial.twostage(margbin,data=twinstut,var.link=1,          clusters=twinstut$tvparnr,theta.des=theta.des,detail=0) summary(bin) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                    theta        se #> factor(zyg)dz -0.2853738 0.9894082 #> factor(zyg)mz  3.3391390 0.5590195 #> factor(zyg)os  0.4920396 0.7634939 #>  #> $or #>               Estimate Std.Err   2.5%  97.5% P-value #> factor(zyg)dz   0.7517  0.7438 -0.706  2.209 0.31216 #> factor(zyg)mz  28.1948 15.7615 -2.697 59.087 0.07364 #> factor(zyg)os   1.6356  1.2488 -0.812  4.083 0.19027 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  twinstut$cage <- scale(twinstut$age) theta.des <- model.matrix( ~-1+factor(zyg)+cage,data=twinstut) bina <- binomial.twostage(margbin,data=twinstut,var.link=1,              clusters=twinstut$tvparnr,theta.des=theta.des) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                    theta        se #> factor(zyg)dz -0.2684851 0.9930894 #> factor(zyg)mz  3.4239727 0.5773886 #> factor(zyg)os  0.4778091 0.7628390 #> cage           0.2519096 0.4821619 #>  #> $or #>               Estimate Std.Err     2.5%  97.5% P-value #> factor(zyg)dz   0.7645  0.7593 -0.72357  2.253 0.31395 #> factor(zyg)mz  30.6911 17.7207 -4.04082 65.423 0.08328 #> factor(zyg)os   1.6125  1.2301 -0.79843  4.024 0.18989 #> cage            1.2865  0.6203  0.07073  2.502 0.03808 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  theta.des <- model.matrix( ~-1+factor(zyg)+factor(zyg)*cage,data=twinstut) bina <- binomial.twostage(margbin,data=twinstut,var.link=1,              clusters=twinstut$tvparnr,theta.des=theta.des) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                          theta        se #> factor(zyg)dz      -0.27701246 1.0713974 #> factor(zyg)mz       3.52484148 0.5951743 #> factor(zyg)os       0.48859941 0.7644050 #> cage                0.06420907 3.6225641 #> factor(zyg)mz:cage  0.49441325 3.6865646 #> factor(zyg)os:cage -0.12312666 3.6921748 #>  #> $or #>                    Estimate Std.Err     2.5%  97.5% P-value #> factor(zyg)dz        0.7580  0.8122  -0.8338  2.350 0.35063 #> factor(zyg)mz       33.9484 20.2052  -5.6531 73.550 0.09292 #> factor(zyg)os        1.6300  1.2460  -0.8121  4.072 0.19080 #> cage                 1.0663  3.8628  -6.5046  8.637 0.78251 #> factor(zyg)mz:cage   1.6395  6.0443 -10.2070 13.486 0.78619 #> factor(zyg)os:cage   0.8842  3.2644  -5.5140  7.282 0.78651 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"   ## Reduce Ex.Timings ## refers to zygosity of first subject in eash pair : zyg1 ## could also use zyg2 (since zyg2=zyg1 within twinpair's)) out <- easy.binomial.twostage(stutter~factor(sex)+age,data=twinstut,                           response=\"binstut\",id=\"tvparnr\",var.link=1,                        theta.formula=~-1+factor(zyg1)) summary(out) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                     theta        se #> factor(zyg1)dz -0.2853738 0.9894082 #> factor(zyg1)mz  3.3391390 0.5590195 #> factor(zyg1)os  0.4920396 0.7634939 #>  #> $or #>                Estimate Std.Err   2.5%  97.5% P-value #> factor(zyg1)dz   0.7517  0.7438 -0.706  2.209 0.31216 #> factor(zyg1)mz  28.1948 15.7615 -2.697 59.087 0.07364 #> factor(zyg1)os   1.6356  1.2488 -0.812  4.083 0.19027 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ## refers to zygosity of first subject in eash pair : zyg1 ## could also use zyg2 (since zyg2=zyg1 within twinpair's)) desfs<-function(x,num1=\"zyg1\",num2=\"zyg2\")     c(x[num1]==\"dz\",x[num1]==\"mz\",x[num1]==\"os\")*1  out3 <- easy.binomial.twostage(binstut~factor(sex)+age,       data=twinstut,response=\"binstut\",id=\"tvparnr\",var.link=1,       theta.formula=desfs,desnames=c(\"mz\",\"dz\",\"os\")) summary(out3) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>         theta        se #> mz -0.2853738 0.9894082 #> dz  3.3391390 0.5590195 #> os  0.4920396 0.7634939 #>  #> $or #>    Estimate Std.Err   2.5%  97.5% P-value #> mz   0.7517  0.7438 -0.706  2.209 0.31216 #> dz  28.1948 15.7615 -2.697 59.087 0.07364 #> os   1.6356  1.2488 -0.812  4.083 0.19027 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"   ### use of clayton oakes binomial additive gamma model ###########################################################  ## Reduce Ex.Timings data <- simbinClaytonOakes.family.ace(10000,2,1,beta=NULL,alpha=NULL) margbin <- glm(ybin~x,data=data,family=binomial()) margbin #>  #> Call:  glm(formula = ybin ~ x, family = binomial(), data = data) #>  #> Coefficients: #> (Intercept)            x   #>      0.5228       0.3024   #>  #> Degrees of Freedom: 39999 Total (i.e. Null);  39998 Residual #> Null Deviance:\t    51190  #> Residual Deviance: 50980 \tAIC: 50990  head(data) #>   ybin x   type cluster #> 1    1 1 mother       1 #> 2    1 1 father       1 #> 3    1 1  child       1 #> 4    1 0  child       1 #> 5    1 1 mother       2 #> 6    0 0 father       2 data$number <- c(1,2,3,4) data$child <- 1*(data$number==3)  ### make ace random effects design out <- ace.family.design(data,member=\"type\",id=\"cluster\") out$pardes #>       [,1] [,2] #>  [1,] 0.25    0 #>  [2,] 0.25    0 #>  [3,] 0.25    0 #>  [4,] 0.25    0 #>  [5,] 0.25    0 #>  [6,] 0.25    0 #>  [7,] 0.25    0 #>  [8,] 0.25    0 #>  [9,] 0.00    1 head(out$des.rv) #>      m1 m2 m3 m4 f1 f2 f3 f4 env #> [1,]  1  1  1  1  0  0  0  0   1 #> [2,]  0  0  0  0  1  1  1  1   1 #> [3,]  1  1  0  0  1  1  0  0   1 #> [4,]  1  0  1  0  1  0  1  0   1 #> [5,]  1  1  1  1  0  0  0  0   1 #> [6,]  0  0  0  0  1  1  1  1   1  bints <- binomial.twostage(margbin,data=data,      clusters=data$cluster,detail=0,var.par=1,      theta=c(2,1),var.link=0,      random.design=out$des.rv,theta.des=out$pardes) summary(bints) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                theta         se #> dependence1 1.755909 0.14712541 #> dependence2 1.044354 0.05333728 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err   2.5%  97.5%    P-value #> dependence1   0.6271 0.02648 0.5751 0.6790 5.886e-124 #> dependence2   0.3729 0.02648 0.3210 0.4249  4.786e-45 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1      2.8  0.1365 2.533 3.068 1.638e-93 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  data <- simbinClaytonOakes.twin.ace(10000,2,1,beta=NULL,alpha=NULL) out  <- twin.polygen.design(data,id=\"cluster\",zygname=\"zygosity\") out$pardes #>      [,1] [,2] #> [1,]  1.0    0 #> [2,]  0.5    0 #> [3,]  0.5    0 #> [4,]  0.5    0 #> [5,]  0.0    1 head(out$des.rv) #>   MZ DZ DZns1 DZns2 env #> 1  1  0     0     0   1 #> 2  1  0     0     0   1 #> 3  1  0     0     0   1 #> 4  1  0     0     0   1 #> 5  1  0     0     0   1 #> 6  1  0     0     0   1 margbin <- glm(ybin~x,data=data,family=binomial())  bintwin <- binomial.twostage(margbin,data=data,      clusters=data$cluster,var.par=1,      theta=c(2,1),random.design=out$des.rv,theta.des=out$pardes) summary(bintwin) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                 theta        se #> dependence1 2.2419359 0.2480324 #> dependence2 0.8173951 0.1674785 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>             Estimate Std.Err   2.5%  97.5%   P-value #> dependence1   0.7328  0.0592 0.6168 0.8489 3.439e-35 #> dependence2   0.2672  0.0592 0.1511 0.3832 6.394e-06 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    3.059  0.1462 2.773 3.346 3.347e-97 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" concordanceTwinACE(bintwin) #> $MZ #>                      Estimate  Std.Err   2.5%  97.5% P-value #> concordance            0.5209 0.005515 0.5101 0.5317       0 #> casewise concordance   0.8312 0.004793 0.8218 0.8406       0 #> marginal               0.6267 0.005315 0.6163 0.6371       0 #>  #> $DZ #>                      Estimate  Std.Err   2.5%  97.5% P-value #> concordance            0.4697 0.006438 0.4571 0.4823       0 #> casewise concordance   0.7495 0.006024 0.7377 0.7613       0 #> marginal               0.6267 0.005315 0.6163 0.6371       0 #>"},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Binomial Regression for censored competing risks data — binreg","title":"Binomial Regression for censored competing risks data — binreg","text":"Simple version comp.risk function timereg just one time-point thus fitting model $$P(T \\leq t, \\epsilon=1 | X ) = expit( X^T beta) $$","code":""},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binomial Regression for censored competing risks data — binreg","text":"","code":"binreg(   formula,   data,   cause = 1,   time = NULL,   beta = NULL,   type = c(\"II\", \"I\"),   offset = NULL,   weights = NULL,   cens.weights = NULL,   cens.model = ~+1,   se = TRUE,   kaplan.meier = TRUE,   cens.code = 0,   no.opt = FALSE,   method = \"nr\",   augmentation = NULL,   outcome = c(\"cif\", \"rmst\", \"rmtl\"),   model = c(\"default\", \"logit\", \"exp\", \"lin\"),   Ydirect = NULL,   monotone = TRUE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binomial Regression for censored competing risks data — binreg","text":"formula formula outcome (see coxph) data data frame cause cause interest (numeric variable) time time interest beta starting values type \"II\" adds augmentation term, \"\" classic binomial regression offset offsets partial likelihood weights score equations cens.weights censoring weights cens.model stratified cox model without covariates se compute se's  based IPCW kaplan.meier uses Kaplan-Meier IPCW contrast exp(-Baseline) cens.code gives censoring code .opt optimize method optimization augmentation augment binomial regression outcome can CIF regression \"cif\"=F(t|X), \"rmst\"=E( min(T, t) | X) , years-lost \"rmtl\"=E( (epsilon==cause) ( t - mint(T,t)) ) | X) model link functions used, defaults logit cif, exp rmst rmtl, can logit, exp lin (identity link) Ydirect use Y instead outcome constructed inside program (e.g. (T< t, epsilon=1)), uses IPCW vesion Y, set outcome \"rmst\" fit using model specified model monotone true uses del link functions used, defaults logit cif, exp rmst rmtl, can logit, exp lin (identity link) ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binomial Regression for censored competing risks data — binreg","text":"Based binomial regresion IPCW response estimating equation: $$ X ( \\Delta^{ipcw}(t) (T \\leq t, \\epsilon=1 ) - expit( X^T beta)) = 0 $$ $$\\Delta^{ipcw}(t) = ((min(t,T)< C)/G_c(min(t,T)-)$$ IPCW adjustment response $$Y(t)= (T \\leq t, \\epsilon=1 )$$. (type=\"\") sovlves estimating equation using stratified Kaplan-Meier censoring distribution. (type=\"II\") default additional censoring augmentation term $$X \\int E(Y(t)| T>s)/G_c(s) d \\hat M_c$$ added. logitIPCW instead considers $$ X  (min(T_i,t) < G_i)/G_c(min(T_i ,t)) ( (T \\leq t, \\epsilon=1 ) - expit( X^T beta)) = 0 $$ standard logistic regression weights adjust IPCW. monotone FALSE solved equation binreg equivalent minmizing least squares problem thus becomes $$ D_\\beta h(\\beta)  ( \\Delta^{ipcw}(t) (T \\leq t, \\epsilon=1 ) - h( X^T beta)) = 0 $$. variance based squared influence functions also returned iid component. naive.var variance known censoring model. Censoring model may depend strata (cens.model=~strata(gX)).","code":""},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Binomial Regression for censored competing risks data — binreg","text":"Blanche PF, Holt , Scheike T (2022). “logistic regression right censored data, without competing risks, use estimating treatment effects.” Lifetime data analysis, 29, 441–482. Scheike TH, Zhang MJ, Gerds TA (2008). “Predicting cumulative incidence probability direct binomial regression.” Biometrika, 95(1), 205–220.","code":""},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Binomial Regression for censored competing risks data — binreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binomial Regression for censored competing risks data — binreg","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(408)*0.001 # logistic regresion with IPCW binomial regression  out <- binreg(Event(time,cause)~tcell+platelet,bmt,time=50) summary(out) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.180332  0.126755 -0.428766  0.068103  0.1548 #> tcell       -0.418194  0.345422 -1.095208  0.258820  0.2260 #> platelet    -0.437667  0.240973 -0.909965  0.034630  0.0693 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.83499 0.65131 1.0705 #> tcell        0.65823 0.33447 1.2954 #> platelet     0.64554 0.40254 1.0352 #>  #>  head(iid(out)) #>              [,1]        [,2]        [,3] #> [1,] -0.006946408 0.004004252 0.006177039 #> [2,] -0.006946408 0.004004252 0.006177039 #> [3,] -0.006946408 0.004004252 0.006177039 #> [4,] -0.006946408 0.004004252 0.006177039 #> [5,] -0.006946408 0.004004252 0.006177039 #> [6,] -0.006946408 0.004004252 0.006177039  predict(out,data.frame(tcell=c(0,1),platelet=c(1,1)),se=TRUE) #>        pred         se     lower     upper #> 1 0.3502366 0.04847385 0.2552279 0.4452454 #> 2 0.2618851 0.06969063 0.1252915 0.3984788  outs <- binreg(Event(time,cause)~tcell+platelet,bmt,time=50,cens.model=~strata(tcell,platelet)) summary(outs) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.180703  0.127413 -0.430429  0.069022  0.1561 #> tcell       -0.365924  0.350632 -1.053150  0.321302  0.2967 #> platelet    -0.433487  0.240270 -0.904408  0.037433  0.0712 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.83468 0.65023 1.0715 #> tcell        0.69356 0.34884 1.3789 #> platelet     0.64824 0.40478 1.0381 #>  #>   ## glm with IPCW weights  outl <- logitIPCW(Event(time,cause)~tcell+platelet,bmt,time=50) summary(outl) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.241521  0.131457 -0.499171  0.016129  0.0662 #> tcell       -0.344491  0.368376 -1.066494  0.377513  0.3497 #> platelet    -0.292933  0.262665 -0.807747  0.221881  0.2647 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.78543 0.60703 1.0163 #> tcell        0.70858 0.34421 1.4587 #> platelet     0.74607 0.44586 1.2484 #>  #>   ########################################## ### risk-ratio of different causes ####### ########################################## data(bmt) bmt$id <- 1:nrow(bmt) bmt$status <- bmt$cause bmt$strata <- 1 bmtdob <- bmt bmtdob$strata <-2 bmtdob <- dtransform(bmtdob,status=1,cause==2) bmtdob <- dtransform(bmtdob,status=2,cause==1) ### bmtdob <- rbind(bmt,bmtdob) dtable(bmtdob,cause+status~strata) #> strata: 1 #>  #>       status   0   1   2 #> cause                    #> 0            160   0   0 #> 1              0 161   0 #> 2              0   0  87 #> ------------------------------------------------------------  #> strata: 2 #>  #>       status   0   1   2 #> cause                    #> 0            160   0   0 #> 1              0   0 161 #> 2              0  87   0  cif1 <- cif(Event(time,cause)~+1,bmt,cause=1) cif2 <- cif(Event(time,cause)~+1,bmt,cause=2) plot(cif1) plot(cif2,add=TRUE,col=2)   cifs1 <- binreg(Event(time,cause)~tcell+platelet+age,bmt,cause=1,time=50) cifs2 <- binreg(Event(time,cause)~tcell+platelet+age,bmt,cause=2,time=50) summary(cifs1) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.198947  0.130987 -0.455677  0.057782  0.1288 #> tcell       -0.636947  0.356604 -1.335878  0.061983  0.0741 #> platelet    -0.344912  0.246013 -0.827089  0.137265  0.1609 #> age          0.437244  0.107266  0.227007  0.647481  0.0000 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.81959 0.63402 1.0595 #> tcell        0.52890 0.26293 1.0639 #> platelet     0.70828 0.43732 1.1471 #> age          1.54843 1.25484 1.9107 #>  #>  summary(cifs2) #>    n events #>  408     85 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -1.322081  0.157783 -1.631329 -1.012832  0.0000 #> tcell        0.746834  0.352260  0.056416  1.437252  0.0340 #> platelet    -0.019142  0.276984 -0.562021  0.523738  0.9449 #> age         -0.072139  0.141687 -0.349842  0.205563  0.6107 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.26658 0.19567 0.3632 #> tcell        2.11031 1.05804 4.2091 #> platelet     0.98104 0.57006 1.6883 #> age          0.93040 0.70480 1.2282 #>  #>   cifdob <- binreg(Event(time,status)~-1+factor(strata)+    tcell*factor(strata)+platelet*factor(strata)+age*factor(strata)    +cluster(id),bmtdob,cause=1,time=50,cens.model=~strata(strata)) summary(cifdob) #>    n events #>  816    245 #>  #>  408 clusters #> coeffients: #>                           Estimate   Std.Err      2.5%     97.5% P-value #> factor(strata)1          -0.198947  0.130987 -0.455677  0.057782  0.1288 #> factor(strata)2          -1.322081  0.157783 -1.631329 -1.012832  0.0000 #> tcell                    -0.636947  0.356604 -1.335878  0.061983  0.0741 #> platelet                 -0.344912  0.246013 -0.827089  0.137265  0.1609 #> age                       0.437244  0.107266  0.227007  0.647481  0.0000 #> factor(strata)2:tcell     1.383781  0.600927  0.205986  2.561577  0.0213 #> factor(strata)2:platelet  0.325770  0.432013 -0.520959  1.172500  0.4508 #> factor(strata)2:age      -0.509383  0.208101 -0.917254 -0.101513  0.0144 #>  #> exp(coeffients): #>                          Estimate    2.5%   97.5% #> factor(strata)1           0.81959 0.63402  1.0595 #> factor(strata)2           0.26658 0.19567  0.3632 #> tcell                     0.52890 0.26293  1.0639 #> platelet                  0.70828 0.43732  1.1471 #> age                       1.54843 1.25484  1.9107 #> factor(strata)2:tcell     3.98996 1.22874 12.9562 #> factor(strata)2:platelet  1.38510 0.59395  3.2301 #> factor(strata)2:age       0.60087 0.39961  0.9035 #>  #>  head(iid(cifdob))  #>           [,1]       [,2]        [,3]        [,4]          [,5]        [,6] #> 1 -0.007447571 0.01776726 0.004626980 0.006532086 -0.0006994667 -0.01601858 #> 2 -0.007988246 0.01802853 0.006444080 0.006743734 -0.0035442243 -0.02069211 #> 3 -0.008864645 0.01851042 0.010424440 0.006903340 -0.0101076390 -0.03003705 #> 4 -0.008835416 0.01849245 0.010262329 0.006903222 -0.0098333568 -0.02967265 #> 5 -0.007126560 0.01761890 0.003707153 0.006378236  0.0006895580 -0.01349320 #> 6 -0.009148217 0.01869572 0.012152107 0.006877028 -0.0130608526 -0.03386034 #>          [,7]          [,8] #> 1 -0.02078114  0.0033123348 #> 2 -0.01975211  0.0112638920 #> 3 -0.01757153  0.0274275718 #> 4 -0.01765988  0.0267910710 #> 5 -0.02132293 -0.0009455222 #> 6 -0.01662766  0.0341341628  newdata <- data.frame(tcell=1,platelet=1,age=0,strata=1:2,id=1) riskratio <- function(p) {   cifdob$coef <- p   p <- predict(cifdob,newdata,se=0)   return(p[1]/p[2]) } lava::estimate(cifdob,f=riskratio) #>    Estimate Std.Err   2.5% 97.5% P-value #> p1   0.6605  0.2738 0.1239 1.197 0.01585  predict(cifdob,newdata) #>        pred         se     lower     upper #> 1 0.2349072 0.06592758 0.1056892 0.3641253 #> 2 0.3556286 0.07421505 0.2101671 0.5010901 (p1 <- predict(cifs1,newdata)) #>        pred         se     lower     upper #> 1 0.2349072 0.06592758 0.1056892 0.3641253 #> 2 0.2349072 0.06592758 0.1056892 0.3641253 (p2 <- predict(cifs2,newdata)) #>        pred         se     lower     upper #> 1 0.3556286 0.07421505 0.2101671 0.5010901 #> 2 0.3556286 0.07421505 0.2101671 0.5010901 p1[1,1]/p2[1,1] #> [1] 0.6605409"},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":null,"dir":"Reference","previous_headings":"","what":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"standard causal assumptions  can estimate average treatment effect E(Y(1) - Y(0)). need Consistency, ignorability ( Y(1), Y(0) indep given X), positivity.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"","code":"binregATE(   formula,   data,   cause = 1,   time = NULL,   beta = NULL,   treat.model = ~+1,   cens.model = ~+1,   offset = NULL,   weights = NULL,   cens.weights = NULL,   se = TRUE,   type = c(\"II\", \"I\"),   kaplan.meier = TRUE,   cens.code = 0,   no.opt = FALSE,   method = \"nr\",   augmentation = NULL,   outcome = c(\"cif\", \"rmst\", \"rmtl\"),   model = c(\"default\", \"logit\", \"exp\", \"lin\"),   Ydirect = NULL,   typeATE = \"II\",   ... )"},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"formula formula outcome (see coxph) data data frame cause cause interest time time interest beta starting values treat.model logistic treatment model given covariates cens.model stratified cox model without covariates offset offsets partial likelihood weights score equations cens.weights censoring weights se compute se's IPCW  adjustment, otherwise assumes IPCW weights known type \"II\" adds augmentation term, \"\" classic binomial regression kaplan.meier uses Kaplan-Meier IPCW contrast exp(-Baseline) cens.code gives censoring code .opt optimize method optimization augmentation augment binomial regression outcome can CIF regression \"cif\"=F(t|X), \"rmst\"=E( min(T, t) | X) , E( (epsilon==cause) ( t - mint(T,t)) ) | X) depending number number causes. model exp linear model E( min(T, t) | X)=exp(X^t beta), E( (epsilon==cause) ( t - mint(T,t)) ) | X)=exp(X^t beta) Ydirect use outcome Y IPCW vesion typeATE \"II\" censor augment  estimating equation ... Additional arguments lower level funtions (binreg fits outcome model)","code":""},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"first covariate specification competing risks regression model must treatment variable coded factor. factor two levels uses mlogit propensity score modelling. censorings ordinary logistic regression modelling. Estimates ATE using standard binary double robust estimating equations IPCW censoring adjusted. Rather binomial regression also consider IPCW weighted version standard logistic regression logitIPCWATE. typeATE=\"II\" augment estimating equation $$ (/\\pi(X)) \\int E( O(t) | T \\geq t, S(X))/ G_c(t,S(X)) d \\hat M_c(s) $$ estimating mean outcome treated.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"Blanche PF, Holt , Scheike T (2022). “logistic regression right censored data, without competing risks, use estimating treatment effects.” Lifetime data analysis, 29, 441–482.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binregATE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Average Treatment effect for censored competing risks data using Binomial Regression — binregATE","text":"","code":"library(mets); data(bmt) dfactor(bmt)  <-  ~.  brs <- binregATE(Event(time,cause)~tcell.f+platelet+age,bmt,time=50,cause=1,   treat.model=tcell.f~platelet+age) summary(brs) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.199112  0.130982 -0.455831  0.057607  0.1285 #> tcell.f1    -0.637221  0.356617 -1.336177  0.061735  0.0740 #> platelet    -0.344504  0.245974 -0.826604  0.137596  0.1613 #> age          0.437222  0.107263  0.226991  0.647454  0.0000 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.81946 0.63392 1.0593 #> tcell.f1     0.52876 0.26285 1.0637 #> platelet     0.70857 0.43753 1.1475 #> age          1.54840 1.25482 1.9107 #>  #> Average Treatment effects (G-formula) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.428748  0.027511  0.374828  0.482668  0.0000 #> treat1     0.289931  0.065905  0.160761  0.419102  0.0000 #> treat:1-0 -0.138817  0.071772 -0.279487  0.001854  0.0531 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.428159  0.027612  0.374040  0.482278  0.0000 #> treat1     0.250414  0.064791  0.123426  0.377402  0.0001 #> treat:1-0 -0.177745  0.070145 -0.315226 -0.040263  0.0113 #>  #>  head(brs$riskDR.iid) #>          iidriska      iidriska #> [1,] -0.001158914 -3.544289e-05 #> [2,] -0.001200978  7.591687e-05 #> [3,] -0.001326400  3.360021e-04 #> [4,] -0.001320260  3.247937e-04 #> [5,] -0.001140662 -9.113840e-05 #> [6,] -0.001398172  4.595460e-04 head(brs$riskG.iid) #>        riskGa.iid    riskGa.iid #> [1,] -0.001190622 -0.0001527653 #> [2,] -0.001242318  0.0001090009 #> [3,] -0.001355147  0.0006917410 #> [4,] -0.001350560  0.0006678242 #> [5,] -0.001164390 -0.0002837983 #> [6,] -0.001403991  0.0009473264  brsi <- binregATE(Event(time,cause)~tcell.f+tcell.f*platelet+tcell.f*age,bmt,time=50,cause=1,   treat.model=tcell.f~platelet+age) summary(brsi) #>    n events #>  408    160 #>  #>  408 clusters #> coeffients: #>                    Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)       -0.161478  0.133240 -0.422623  0.099667  0.2255 #> tcell.f1          -1.029704  0.513385 -2.035920 -0.023489  0.0449 #> platelet          -0.490119  0.270795 -1.020867  0.040630  0.0703 #> age                0.445946  0.112205  0.226028  0.665864  0.0001 #> tcell.f1:platelet  0.956586  0.694975 -0.405540  2.318713  0.1687 #> tcell.f1:age      -0.154714  0.427213 -0.992036  0.682609  0.7172 #>  #> exp(coeffients): #>                   Estimate    2.5%   97.5% #> (Intercept)        0.85089 0.65533  1.1048 #> tcell.f1           0.35711 0.13056  0.9768 #> platelet           0.61255 0.36028  1.0415 #> age                1.56197 1.25361  1.9462 #> tcell.f1:platelet  2.60280 0.66662 10.1626 #> tcell.f1:age       0.85666 0.37082  1.9790 #>  #> Average Treatment effects (G-formula) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.427718  0.027557  0.373706  0.481729  0.0000 #> treat1     0.265587  0.069738  0.128903  0.402272  0.0001 #> treat:1-0 -0.162130  0.074795 -0.308726 -0.015535  0.0302 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     0.428133  0.027614  0.374012  0.482255  0.0000 #> treat1     0.253093  0.066785  0.122196  0.383990  0.0002 #> treat:1-0 -0.175040  0.072033 -0.316223 -0.033857  0.0151 #>  #>  head(brs$riskDR.iid) #>          iidriska      iidriska #> [1,] -0.001158914 -3.544289e-05 #> [2,] -0.001200978  7.591687e-05 #> [3,] -0.001326400  3.360021e-04 #> [4,] -0.001320260  3.247937e-04 #> [5,] -0.001140662 -9.113840e-05 #> [6,] -0.001398172  4.595460e-04 head(brs$riskG.iid) #>        riskGa.iid    riskGa.iid #> [1,] -0.001190622 -0.0001527653 #> [2,] -0.001242318  0.0001090009 #> [3,] -0.001355147  0.0006917410 #> [4,] -0.001350560  0.0006678242 #> [5,] -0.001164390 -0.0002837983 #> [6,] -0.001403991  0.0009473264"},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"Estimates casewise concordance based Concordance marginal estimate using binreg","code":""},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"","code":"binregCasewise(concbreg, margbreg, zygs = c(\"DZ\", \"MZ\"), newdata = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"concbreg Concordance margbreg Marginal estimate zygs order zygosity estimation concordance casewise. newdata give instead zygs. ... pass estimate function","code":""},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"Uses cluster iid two binomial-regression estimates  standard errors better casewise often conservative.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binregCasewise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimates the casewise concordance based on Concordance and marginal estimate using binreg — binregCasewise","text":"","code":"library(mets) data(prt) prt <- force.same.cens(prt,cause=\"status\")  dd <- bicompriskData(Event(time, status)~strata(zyg)+id(id), data=prt, cause=c(2, 2)) newdata <- data.frame(zyg=c(\"DZ\",\"MZ\"),id=1)  ## concordance  bcif1 <- binreg(Event(time,status)~-1+factor(zyg)+cluster(id), data=dd,                 time=80, cause=1, cens.model=~strata(zyg)) pconc <- predict(bcif1,newdata)  ## marginal estimates  mbcif1 <- binreg(Event(time,status)~cluster(id), data=prt, time=80, cause=2) mc <- predict(mbcif1,newdata)  cse <- binregCasewise(bcif1,mbcif1) cse #> $coef #>     Estimate      2.5%     97.5% #> p1 0.1586277 0.1445496 0.1740770 #> p2 0.4041311 0.3682646 0.4434908 #>  #> $logcoef #>    Estimate Std.Err   2.5%   97.5%   P-value #> p1   -1.841 0.04742 -1.934 -1.7483 0.000e+00 #> p2   -0.906 0.04742 -0.999 -0.8131 2.208e-81 #>"},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":null,"dir":"Reference","previous_headings":"","what":"G-estimator for binomial regression model (Standardized estimates) — binregG","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"Computes G-estimator $$ \\hat F(t,=) = n^{-1} \\sum_i \\hat F(t,=,Z_i) $$. Assumes first covariate $$. Gives influence functions risk estimates SE's based . first covariate factor contrast computed, continuous considered covariate values given Avalues.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"","code":"binregG(x, data, Avalues = NULL, varname = NULL)"},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"x binreg object data data frame risk averaging Avalues values compare first covariate , assumes first variable factor take levels varname given averages variable, default first variable","code":""},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"Blanche PF, Holt , Scheike T (2022). “logistic regression right censored data, without competing risks, use estimating treatment effects.” Lifetime data analysis, 29, 441–482.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binregG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"G-estimator for binomial regression model (Standardized estimates) — binregG","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(408)*0.001 bmt$event <- (bmt$cause!=0)*1  b1 <- binreg(Event(time,cause)~age+tcell+platelet,bmt,cause=1,time=50) sb1 <- binregG(b1,bmt,Avalues=c(0,1,2)) summary(sb1) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.4058 0.02588 0.3551 0.4565 1.982e-55 #> risk1   0.5119 0.03706 0.4393 0.5846 2.057e-43 #> risk2   0.6168 0.05516 0.5087 0.7250 4.993e-29 #>  #> Average Treatment effect: difference (G-estimator) : #>      Estimate Std.Err    2.5%  97.5%   P-value #> pa     0.1061 0.02623 0.05471 0.1575 5.222e-05 #> pa.1   0.2110 0.04960 0.11381 0.3082 2.096e-05 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>       Estimate    Std.Err      2.5%     97.5%      P-value #> [pa] 0.2323087 0.05277448 0.1288726 0.3357448 1.073002e-05 #> [pa] 0.4187166 0.08402886 0.2540231 0.5834101 6.260295e-07 #> ratio:  #>      Estimate     2.5%    97.5% #> [pa] 1.261509 1.137545 1.398982 #> [pa] 1.520010 1.289202 1.792139 #>"},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":null,"dir":"Reference","previous_headings":"","what":"Percentage of years lost due to cause regression — binregRatio","title":"Percentage of years lost due to cause regression — binregRatio","text":"Estimates percentage years lost due cause covariates affects percentage ICPW regression.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Percentage of years lost due to cause regression — binregRatio","text":"","code":"binregRatio(   formula,   data,   cause = 1,   time = NULL,   beta = NULL,   type = c(\"II\", \"I\"),   offset = NULL,   weights = NULL,   cens.weights = NULL,   cens.model = ~+1,   se = TRUE,   kaplan.meier = TRUE,   cens.code = 0,   no.opt = FALSE,   method = \"nr\",   augmentation = NULL,   outcome = c(\"cif\", \"rmtl\"),   model = c(\"logit\", \"exp\", \"lin\"),   Ydirect = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Percentage of years lost due to cause regression — binregRatio","text":"formula formula outcome (see coxph) data data frame cause cause interest (numeric variable) time time interest beta starting values type \"II\" adds augmentation term, \"\" classical outcome IPCW regression offset offsets partial likelihood weights score equations cens.weights censoring weights cens.model stratified cox model without covariates se compute se's  based IPCW kaplan.meier uses Kaplan-Meier IPCW contrast exp(-Baseline) cens.code gives censoring code .opt optimize method optimization augmentation augment binomial regression outcome can CIF regression \"cif\"=F(t|X), \"rmtl\"=E( t- min(T, t) | X)\" model logit, exp lin(ear) Ydirect use Y instead outcome constructed inside program, matrix two column numerator denominator. ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Percentage of years lost due to cause regression — binregRatio","text":"Let years lost  $$Y1= t- min(T ,) $$ years lost due cause 1 $$Y2= (epsilon==1) ( t- min(T ,t) $$ , model ratio $$logit( E(Y2 | X)/E(Y1 | X))  = X^T \\beta $$. Estimation based binomial regresion IPCW response estimating equation: $$ X ( \\Delta^{ipcw}(t) Y2 expit(X^T \\beta) -  Y1 ) = 0 $$ $$\\Delta^{ipcw}(t) = ((min(t,T)< C)/G_c(min(t,T)-)$$ IPCW adjustment response $$Y(t)= (T \\leq t, \\epsilon=1 )$$. (type=\"\") sovlves estimating equation using stratified Kaplan-Meier censoring distribution. (type=\"II\") default additional censoring augmentation term $$X \\int E(Y(t)| T>s)/G_c(s) d \\hat M_c$$ added. variance based squared influence functions also returned iid component. naive.var variance known censoring model. Censoring model may depend strata (cens.model=~strata(gX)).","code":""},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Percentage of years lost due to cause regression — binregRatio","text":"Scheike & Tanaka (2025), Restricted mean time lost ratio regression: Percentage restricted mean time lost due specific cause, WIP","code":""},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Percentage of years lost due to cause regression — binregRatio","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binregRatio.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Percentage of years lost due to cause regression — binregRatio","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(408)*0.001  rmst30 <- rmstIPCW(Event(time,cause!=0)~platelet+tcell+age,bmt,time=30,cause=1) rmst301 <- rmstIPCW(Event(time,cause)~platelet+tcell+age,bmt,time=30,cause=1) rmst302 <- rmstIPCW(Event(time,cause)~platelet+tcell+age,bmt,time=30,cause=2)  estimate(rmst30) #>             Estimate Std.Err     2.5%    97.5%   P-value #> (Intercept)   2.6104 0.05744  2.49778  2.72294 0.000e+00 #> platelet      0.2450 0.08435  0.07970  0.41035 3.674e-03 #> tcell         0.1456 0.11877 -0.08719  0.37838 2.202e-01 #> age          -0.1728 0.03794 -0.24718 -0.09844 5.255e-06 estimate(rmst301) #>             Estimate Std.Err    2.5%      97.5%    P-value #> (Intercept)   2.4449 0.07291  2.3020  2.5878403 1.655e-246 #> platelet     -0.4519 0.16738 -0.7799 -0.1238183  6.940e-03 #> tcell        -0.4935 0.25140 -0.9862 -0.0007153  4.967e-02 #> age           0.2746 0.06538  0.1465  0.4027874  2.658e-05 estimate(rmst302) #>             Estimate Std.Err    2.5%  97.5%   P-value #> (Intercept)  1.45850  0.1362  1.1915 1.7255 9.617e-27 #> platelet    -0.01886  0.2224 -0.4548 0.4171 9.324e-01 #> tcell        0.40810  0.2648 -0.1108 0.9270 1.232e-01 #> age          0.04566  0.1191 -0.1878 0.2791 7.014e-01  ## percentage of total cumulative incidence due to cause 1 rmtlratioI <- rmtlRatio(Event(time,cause)~platelet+tcell+age,bmt,time=30,cause=1) summary(rmtlratioI) #>    n events #>  408    154 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  0.977101  0.178189  0.627856  1.326345  0.0000 #> platelet    -0.412142  0.323288 -1.045775  0.221490  0.2024 #> tcell       -0.855816  0.419188 -1.677408 -0.034223  0.0412 #> age          0.217651  0.168416 -0.112439  0.547741  0.1962 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  2.65674 1.87359 3.7672 #> platelet     0.66223 0.35142 1.2479 #> tcell        0.42494 0.18686 0.9664 #> age          1.24315 0.89365 1.7293 #>  #>   pp <- predict(rmtlratioI,bmt[1:5,]) pp #>        pred         se     lower     upper #> 1 0.7349115 0.03513428 0.6660483 0.8037747 #> 2 0.7529172 0.03812709 0.6781881 0.8276463 #> 3 0.7839582 0.05014699 0.6856701 0.8822463 #> 4 0.7828518 0.04965019 0.6855374 0.8801661 #> 5 0.7243286 0.03566672 0.6544218 0.7942353  newdata <- data.frame(platelet=1,tcell=1,age=1) ## percentage of total cumulative incidence due to cause 1 cifratio <- binregRatio(Event(time,cause)~platelet+tcell+age,bmt,time=30,cause=1) summary(cifratio) #>    n events #>  408    154 #>  #>  408 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)  0.91183  0.17554  0.56778  1.25589  0.0000 #> platelet    -0.46563  0.31922 -1.09128  0.16003  0.1447 #> tcell       -1.06111  0.41568 -1.87583 -0.24639  0.0107 #> age          0.21200  0.16115 -0.10384  0.52784  0.1883 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  2.48888 1.76434 3.5109 #> platelet     0.62774 0.33579 1.1735 #> tcell        0.34607 0.15323 0.7816 #> age          1.23615 0.90137 1.6953 #>  #>  pp <- predict(cifratio,newdata) pp #>        pred       se     lower    upper #> 1 0.4006137 0.106969 0.1909544 0.610273  rmtlratioI <- binregRatio(Event(time,cause)~platelet+tcell+age,bmt,                                time=30,cause=1,outcome=\"rmtl\") summary(rmtlratioI) #>    n events #>  408    154 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  0.977101  0.178189  0.627856  1.326345  0.0000 #> platelet    -0.412142  0.323288 -1.045775  0.221490  0.2024 #> tcell       -0.855816  0.419188 -1.677408 -0.034223  0.0412 #> age          0.217651  0.168416 -0.112439  0.547741  0.1962 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  2.65674 1.87359 3.7672 #> platelet     0.66223 0.35142 1.2479 #> tcell        0.42494 0.18686 0.9664 #> age          1.24315 0.89365 1.7293 #>  #>   pp <- predict(rmtlratioI,newdata) pp #>        pred        se    lower     upper #> 1 0.4817067 0.1092932 0.267492 0.6959214"},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":null,"dir":"Reference","previous_headings":"","what":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"two-stage randomization can estimate average treatment effect E(Y(,j)) treatment regime (,j). estimator can agumented different ways: using two randomizations dynamic censoring augmetatation. treatment's must given factors.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"","code":"binregTSR(   formula,   data,   cause = 1,   time = NULL,   cens.code = 0,   response.code = NULL,   augmentR0 = NULL,   treat.model0 = ~+1,   augmentR1 = NULL,   treat.model1 = ~+1,   augmentC = NULL,   cens.model = ~+1,   estpr = c(1, 1),   response.name = NULL,   offset = NULL,   weights = NULL,   cens.weights = NULL,   beta = NULL,   kaplan.meier = TRUE,   no.opt = FALSE,   method = \"nr\",   augmentation = NULL,   outcome = c(\"cif\", \"rmst\", \"rmst-cause\"),   model = \"exp\",   Ydirect = NULL,   return.dataw = 0,   pi0 = 0.5,   pi1 = 0.5,   cens.time.fixed = 1,   outcome.iid = 1,   meanCs = 0,   ... )"},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"formula formula outcome (see coxph) data data frame cause cause interest time time interest cens.code gives censoring code response.code code status survival data indicates response 2nd randomization performed augmentR0 augmentation model  1st randomization treat.model0 logistic treatment model 1st randomization augmentR1 augmentation model  2nd randomization treat.model1 logistic treatment model 2ndrandomization augmentC augmentation model censoring cens.model stratification censoring model based observed covariates estpr estimate randomization probabilities using model response.name can give name response variable, otherwise reads first variable treat.model1 offset implemented weights implemented cens.weights can given beta starting values kaplan.meier censoring weights, rather exp cumulative hazard .opt implemented method implemented augmentation implemented outcome can c(\"cif\",\"rmst\",\"rmst-cause\") model implemented, uses linear regression augmentation Ydirect use Y instead outcome constructed inside program (e.g. (T< t, epsilon=1)), see binreg return.dataw return weighted data treatment regimes pi0 set known randomization probabilities pi1 set known randomization probabilities cens.time.fixed use time-dependent weights censoring estimation using weights outcome.iid get iid contribution outcome model (linear regression working models). meanCs (0) indicates censoring augmentation centered CensAugment.times/n ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"solved estimating eqution $$  (  (min(T_i,t) < G_i)/G_c(min(T_i ,t)) (T \\leq t, \\epsilon=1 ) - AUG_0 - AUG_1 + AUG_C  -  p(,j)) = 0 $$  using covariates augmentR0 $$ AUG_0 = \\frac{A_0() - \\pi_0()}{ \\pi_0()} X_0 \\gamma_0$$  using covariates augmentR1 $$ AUG_1 = \\frac{A_0()}{\\pi_0()} \\frac{A_1(j) - \\pi_1(j)}{ \\pi_1(j)} X_1 \\gamma_1$$   censoring augmentation $$  AUG_C =  \\int_0^t \\gamma_c(s)^T (e(s) - \\bar e(s))  \\frac{1}{G_c(s) } dM_c(s) $$ $$ \\gamma_c(s)$$ chosen minimize variance given dynamic  covariates specified augmentC. observational case, can use propensity score modelling outcome modelling (using linear regression). Standard errors estimated using influence function  estimators tests differences can therefore computed subsequently.","code":""},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/binregTSR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2 Stage Randomization for Survival Data or competing Risks Data — binregTSR","text":"","code":"library(mets) ddf <- mets:::gsim(200,covs=1,null=0,cens=1,ce=2)  bb <- binregTSR(Event(entry,time,status)~+1+cluster(id),ddf$datat,time=2,cause=c(1),         cens.code=0,treat.model0=A0.f~+1,treat.model1=A1.f~A0.f,         augmentR1=~X11+X12+TR,augmentR0=~X01+X02,         augmentC=~A01+A02+X01+X02+A11t+A12t+X11+X12+TR,         response.code=2) summary(bb)  #> Simple estimator : #>                              coef            #> A0.f=1, response*A1.f=1 0.6513604 0.05894890 #> A0.f=1, response*A1.f=2 0.8454786 0.06940480 #> A0.f=2, response*A1.f=1 0.3381049 0.07717948 #> A0.f=2, response*A1.f=2 0.1536713 0.05861389 #>  #> First Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.6509532 0.05841946 #> A0.f=1, response*A1.f=2 0.8466905 0.06961735 #> A0.f=2, response*A1.f=1 0.3372760 0.07745806 #> A0.f=2, response*A1.f=2 0.1539456 0.05837829 #>  #> Second Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.6575295 0.05738919 #> A0.f=1, response*A1.f=2 0.8256192 0.06904964 #> A0.f=2, response*A1.f=1 0.3298011 0.07878161 #> A0.f=2, response*A1.f=2 0.1492153 0.05848652 #>  #> 1st and 2nd Randomization Augmentation : #>                              coef            #> A0.f=1, response*A1.f=1 0.6614013 0.05563921 #> A0.f=1, response*A1.f=2 0.8295814 0.06803427 #> A0.f=2, response*A1.f=1 0.3304605 0.07864288 #> A0.f=2, response*A1.f=2 0.1502968 0.05795214 #>"},{"path":"http://kkholst.github.io/mets/reference/biprobit.html","id":null,"dir":"Reference","previous_headings":"","what":"Bivariate Probit model — biprobit","title":"Bivariate Probit model — biprobit","text":"Bivariate Probit model","code":""},{"path":"http://kkholst.github.io/mets/reference/biprobit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bivariate Probit model — biprobit","text":"","code":"biprobit(   x,   data,   id,   rho = ~1,   num = NULL,   strata = NULL,   eqmarg = TRUE,   indep = FALSE,   weights = NULL,   weights.fun = function(x) ifelse(any(x <= 0), 0, max(x)),   randomeffect = FALSE,   vcov = \"robust\",   pairs.only = FALSE,   allmarg = !is.null(weights),   control = list(trace = 0),   messages = 1,   constrain = NULL,   table = pairs.only,   p = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/biprobit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bivariate Probit model — biprobit","text":"x formula (vector) data data.frame id name column dataset containing cluster id-variable. rho Formula specifying regression model dependence parameter num Optional name order variable strata Strata eqmarg TRUE marginals assumed (exchangeable) indep Independence weights Weights weights.fun Function defining bivariate weight cluster randomeffect TRUE random effect model used (otherwise correlation parameter estimated allowing negative positive dependence) vcov Type standard errors calculated pairs.Include complete pairs ? allmarg marginal terms included control Control argument parsed optimization routine. Starting values may parsed 'start'. messages Control amount messages shown constrain Vector parameter constraints (NA free). Use set offset. table Type estimation procedure p Parameter vector p evaluate log-Likelihood score function ... Optional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/biprobit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bivariate Probit model — biprobit","text":"","code":"data(prt) prt0 <- subset(prt,country==\"Denmark\") a <- biprobit(cancer~1+zyg, ~1+zyg, data=prt0, id=\"id\") b <- biprobit(cancer~1+zyg, ~1+zyg, data=prt0, id=\"id\",pairs.only=TRUE) predict(b,newdata=lava::Expand(prt,zyg=c(\"MZ\"))) #>           p11        p10        p01       p00         p1         p2       mu1 #> 1 0.005847975 0.01052632 0.01052632 0.9730994 0.01637429 0.01637429 -2.135152 #>         mu2       rho parameter zyg #> 1 -2.135152 0.7568562         1  MZ predict(b,newdata=lava::Expand(prt,zyg=c(\"MZ\",\"DZ\"))) #>           p11        p10        p01       p00         p1         p2       mu1 #> 1 0.005847975 0.01052632 0.01052632 0.9730994 0.01637429 0.01637429 -2.135152 #> 2 0.000655527 0.01425761 0.01425761 0.9708293 0.01491313 0.01491313 -2.172390 #>         mu2       rho parameter zyg #> 1 -2.135152 0.7568562         1  MZ #> 2 -2.172390 0.1960491         2  DZ   ## Reduce Ex.Timings n <- 2e3 x <- sort(runif(n, -1, 1)) y <- rmvn(n, c(0,0), rho=cbind(tanh(x)))>0 d <- data.frame(y1=y[,1], y2=y[,2], x=x) dd <- fast.reshape(d)  a <- biprobit(y~1+x,rho=~1+x,data=dd,id=\"id\") summary(a, mean.contrast=c(1,.5), cor.contrast=c(1,.5)) #>  #>                Estimate   Std.Err       Z p-value     #> (Intercept)   -0.027944  0.019673 -1.4204  0.1555     #> x             -0.029548  0.032538 -0.9081  0.3638     #> r:(Intercept) -0.014561  0.038417 -0.3790  0.7047     #> r:x            1.047861  0.072660 14.4215  <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> logLik: -2646.19  mean(score^2): 9.962e-06  #>     n pairs  #>  4000  2000  #>  #> Contrast: #> \tDependence    [(Intercept)] + 0.5[x]  #> \tMean          [(Intercept)] + 0.5[x]  #>  #>                         Estimate 2.5%    97.5%   #> Rel.Recur.Risk          1.33294  1.26421 1.40166 #> OR                      3.62347  2.80740 4.67675 #> Tetrachoric correlation 0.46945  0.38582 0.54543 #>                                                  #> Concordance             0.31091  0.28469 0.33840 #> Casewise Concordance    0.64376  0.60920 0.67689 #> Marginal                0.48296  0.45989 0.50611 with(predict(a,data.frame(x=seq(-1,1,by=.1))), plot(p00~x,type=\"l\"))   pp <- predict(a,data.frame(x=seq(-1,1,by=.1)),which=c(1)) plot(pp[,1]~pp$x, type=\"l\", xlab=\"x\", ylab=\"Concordance\", lwd=2, xaxs=\"i\") lava::confband(pp$x,pp[,2],pp[,3],polygon=TRUE,lty=0,col=lava::Col(1))   pp <- predict(a,data.frame(x=seq(-1,1,by=.1)),which=c(9)) ## rho plot(pp[,1]~pp$x, type=\"l\", xlab=\"x\", ylab=\"Correlation\", lwd=2, xaxs=\"i\") lava::confband(pp$x,pp[,2],pp[,3],polygon=TRUE,lty=0,col=lava::Col(1)) with(pp, lines(x,tanh(-x),lwd=2,lty=2))  xp <- seq(-1,1,length.out=6); delta <- mean(diff(xp)) a2 <- biprobit(y~1+x,rho=~1+I(cut(x,breaks=xp)),data=dd,id=\"id\") pp2 <- predict(a2,data.frame(x=xp[-1]-delta/2),which=c(9)) ## rho lava::confband(pp2$x,pp2[,2],pp2[,3],center=pp2[,1])      ## Time if (FALSE) { # \\dontrun{     a <- biprobit.time(cancer~1, rho=~1+zyg, id=\"id\", data=prt, eqmarg=TRUE,                        cens.formula=Surv(time,status==0)~1,                        breaks=seq(75,100,by=3),fix.censweights=TRUE)      a <- biprobit.time2(cancer~1+zyg, rho=~1+zyg, id=\"id\", data=prt0, eqmarg=TRUE,                        cens.formula=Surv(time,status==0)~zyg,                        breaks=100)      #a1 <- biprobit.time2(cancer~1, rho=~1, id=\"id\", data=subset(prt0,zyg==\"MZ\"), eqmarg=TRUE,     #                   cens.formula=Surv(time,status==0)~1,     #                   breaks=100,pairs.only=TRUE)      #a2 <- biprobit.time2(cancer~1, rho=~1, id=\"id\", data=subset(prt0,zyg==\"DZ\"), eqmarg=TRUE,     #                    cens.formula=Surv(time,status==0)~1,     #                    breaks=100,pairs.only=TRUE) } # }"},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":null,"dir":"Reference","previous_headings":"","what":"Block sampling — blocksample","title":"Block sampling — blocksample","text":"Sample blockwise clustered data","code":""},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Block sampling — blocksample","text":"","code":"blocksample(data, size, idvar = NULL, replace = TRUE, ...)"},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Block sampling — blocksample","text":"data Data frame size Size samples idvar Column defining clusters replace Logical indicating wether sample replacement ... additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Block sampling — blocksample","text":"data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Block sampling — blocksample","text":"Original id stored attribute 'id'","code":""},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Block sampling — blocksample","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/blocksample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Block sampling — blocksample","text":"","code":"d <- data.frame(x=rnorm(5), z=rnorm(5), id=c(4,10,10,5,5), v=rnorm(5)) (dd <- blocksample(d,size=20,~id)) #>               x            z id          v #> 1    -0.2098647 -0.966818043  1 -0.2770055 #> 1.1  -0.2098647 -0.966818043  2 -0.2770055 #> 4     0.5540451 -0.001732368  3 -1.3458555 #> 5    -0.5581936 -0.351628450  3 -0.2126497 #> 2    -0.7869657 -2.057152528  4  1.1694029 #> 3     1.3636367  0.305695423  4 -0.9603945 #> 4.1   0.5540451 -0.001732368  5 -1.3458555 #> 5.1  -0.5581936 -0.351628450  5 -0.2126497 #> 2.1  -0.7869657 -2.057152528  6  1.1694029 #> 3.1   1.3636367  0.305695423  6 -0.9603945 #> 1.2  -0.2098647 -0.966818043  7 -0.2770055 #> 2.2  -0.7869657 -2.057152528  8  1.1694029 #> 3.2   1.3636367  0.305695423  8 -0.9603945 #> 4.2   0.5540451 -0.001732368  9 -1.3458555 #> 5.2  -0.5581936 -0.351628450  9 -0.2126497 #> 4.3   0.5540451 -0.001732368 10 -1.3458555 #> 5.3  -0.5581936 -0.351628450 10 -0.2126497 #> 1.3  -0.2098647 -0.966818043 11 -0.2770055 #> 1.4  -0.2098647 -0.966818043 12 -0.2770055 #> 2.3  -0.7869657 -2.057152528 13  1.1694029 #> 3.3   1.3636367  0.305695423 13 -0.9603945 #> 1.5  -0.2098647 -0.966818043 14 -0.2770055 #> 4.4   0.5540451 -0.001732368 15 -1.3458555 #> 5.4  -0.5581936 -0.351628450 15 -0.2126497 #> 1.6  -0.2098647 -0.966818043 16 -0.2770055 #> 1.7  -0.2098647 -0.966818043 17 -0.2770055 #> 1.8  -0.2098647 -0.966818043 18 -0.2770055 #> 1.9  -0.2098647 -0.966818043 19 -0.2770055 #> 1.10 -0.2098647 -0.966818043 20 -0.2770055 attributes(dd)$id #>  [1]  4  4  5  5 10 10  5  5 10 10  4 10 10  5  5  5  5  4  4 10 10  4  5  5  4 #> [26]  4  4  4  4  if (FALSE) { # \\dontrun{ blocksample(data.table::data.table(d),1e6,~id) } # }   d <- data.frame(x=c(1,rnorm(9)),                z=rnorm(10),                id=c(4,10,10,5,5,4,4,5,10,5),                id2=c(1,1,2,1,2,1,1,1,1,2),                v=rnorm(10)) dsample(d,~id, size=2) #>             x          z id id2           v id.1 #> 1   1.0000000 -0.4024641  4   1  0.09811391    1 #> 6  -0.9481249  0.8533950  4   1 -1.53366661    1 #> 7   0.3808644  0.3585276  4   1 -0.71283679    1 #> 4  -0.7826530  0.5446049  5   1  0.35253922    2 #> 5   0.3506016 -0.2281963  5   2 -0.65197620    2 #> 8  -0.3301687 -0.2970404  5   1  0.74729117    2 #> 10 -0.3731517 -1.7023895  5   2  1.54708600    2 dsample(d,.~id+id2) #>               x          z           v id #> 3    -2.1508496 -0.1699301  0.38173893  1 #> 1     1.0000000 -0.4024641  0.09811391  2 #> 6    -0.9481249  0.8533950 -1.53366661  2 #> 7     0.3808644  0.3585276 -0.71283679  2 #> 3.1  -2.1508496 -0.1699301  0.38173893  3 #> 4    -0.7826530  0.5446049  0.35253922  4 #> 8    -0.3301687 -0.2970404  0.74729117  4 #> 5     0.3506016 -0.2281963 -0.65197620  5 #> 10   -0.3731517 -1.7023895  1.54708600  5 #> 1.1   1.0000000 -0.4024641  0.09811391  6 #> 6.1  -0.9481249  0.8533950 -1.53366661  6 #> 7.1   0.3808644  0.3585276 -0.71283679  6 #> 1.2   1.0000000 -0.4024641  0.09811391  7 #> 6.2  -0.9481249  0.8533950 -1.53366661  7 #> 7.2   0.3808644  0.3585276 -0.71283679  7 #> 2    -2.0273286  1.3327415  0.27081692  8 #> 9    -0.5036722  1.6237297  0.24582474  8 #> 5.1   0.3506016 -0.2281963 -0.65197620  9 #> 10.1 -0.3731517 -1.7023895  1.54708600  9 #> 3.2  -2.1508496 -0.1699301  0.38173893 10 dsample(d,x+z~id|x>0,size=5) #>             x          z id #> 5   0.3506016 -0.2281963  1 #> 1   1.0000000 -0.4024641  2 #> 7   0.3808644  0.3585276  2 #> 5.1 0.3506016 -0.2281963  3 #> 5.2 0.3506016 -0.2281963  4 #> 5.3 0.3506016 -0.2281963  5"},{"path":"http://kkholst.github.io/mets/reference/bmt.html","id":null,"dir":"Reference","previous_headings":"","what":"The Bone Marrow Transplant Data — bmt","title":"The Bone Marrow Transplant Data — bmt","text":"Bone marrow transplant data 408 rows 5 columns.","code":""},{"path":"http://kkholst.github.io/mets/reference/bmt.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The Bone Marrow Transplant Data — bmt","text":"data 408 rows 5 columns. cause numeric vector code.  Survival status. 1: dead treatment related causes, 2: relapse , 0: censored. time numeric vector. Survival time. platelet numeric vector code. Plalelet 1: 100 x \\(10^9\\) per L, 0: less. tcell numeric vector. T-cell depleted BMT 1:yes, 0:. age numeric vector code. Age patient, scaled centered ((age-35)/15).","code":""},{"path":"http://kkholst.github.io/mets/reference/bmt.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The Bone Marrow Transplant Data — bmt","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/bmt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"The Bone Marrow Transplant Data — bmt","text":"NN","code":""},{"path":"http://kkholst.github.io/mets/reference/bmt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Bone Marrow Transplant Data — bmt","text":"","code":"data(bmt) names(bmt) #> [1] \"time\"     \"cause\"    \"platelet\" \"age\"      \"tcell\""},{"path":"http://kkholst.github.io/mets/reference/bptwin.html","id":null,"dir":"Reference","previous_headings":"","what":"Liability model for twin data — bptwin","title":"Liability model for twin data — bptwin","text":"Liability-threshold model twin data","code":""},{"path":"http://kkholst.github.io/mets/reference/bptwin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Liability model for twin data — bptwin","text":"","code":"bptwin(   x,   data,   id,   zyg,   DZ,   group = NULL,   num = NULL,   weights = NULL,   weights.fun = function(x) ifelse(any(x <= 0), 0, max(x)),   strata = NULL,   messages = 1,   control = list(trace = 0),   type = \"ace\",   eqmean = TRUE,   pairs.only = FALSE,   samecens = TRUE,   allmarg = samecens & !is.null(weights),   stderr = TRUE,   robustvar = TRUE,   p,   indiv = FALSE,   constrain,   varlink,   ... )"},{"path":"http://kkholst.github.io/mets/reference/bptwin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Liability model for twin data — bptwin","text":"x Formula specifying effects covariates response. data data.frame one observation pr row. addition column zygosity (DZ MZ given factor) individual much specified well twin id variable giving unique pair numbers/factors twin pair. id name column dataset containing twin-id variable. zyg name column dataset containing zygosity variable. DZ Character defining level zyg variable corresponding dyzogitic twins. group Optional. Variable name defining group interaction analysis (e.g., gender) num Optional twin number variable weights Weight matrix needed chosen estimator (IPCW) weights.fun Function defining single weight individual/cluster strata Strata messages Control amount messages shown control Control argument parsed optimization routine. Starting values may parsed 'start'. type Character defining type analysis performed. subset \"acde\" (additive genetic factors, common environmental factors, dominant genetic factors, unique environmental factors). eqmean Equal means (type=\"cor\")? pairs.Include complete pairs ? samecens censoring allmarg marginal terms included stderr standard errors calculated? robustvar TRUE robust (sandwich) variance estimates variance used p Parameter vector p evaluate log-Likelihood score function indiv TRUE score log-Likelihood contribution twin-pair constrain Development argument varlink Link function variance parameters ... Additional arguments lower level functions","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/bptwin.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Liability model for twin data — bptwin","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/bptwin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Liability model for twin data — bptwin","text":"","code":"data(twinstut) b0 <- bptwin(stutter~sex,              data=droplevels(subset(twinstut,zyg%in%c(\"mz\",\"dz\"))),              id=\"tvparnr\",zyg=\"zyg\",DZ=\"dz\",type=\"ae\") #> Warning: setting environment(<primitive function>) is not possible and trying it is deprecated #> Warning: setting environment(<primitive function>) is not possible and trying it is deprecated #> Warning: setting environment(<primitive function>) is not possible and trying it is deprecated summary(b0) #>  #>             Estimate  Std.Err        Z   p-value     #> (Intercept) -3.70371  0.24449 -15.1485 < 2.2e-16 *** #> sexmale      0.83310  0.08255  10.0920 < 2.2e-16 *** #> log(var(A))  1.18278  0.17179   6.8851 5.774e-12 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #>  Total MZ/DZ Complete pairs MZ/DZ #>  8777/12511  3255/4058            #>  #>                    Estimate 2.5%    97.5%   #> A                  0.76545  0.70500 0.82590 #> E                  0.23455  0.17410 0.29500 #> MZ Tetrachoric Cor 0.76545  0.69793 0.81948 #> DZ Tetrachoric Cor 0.38272  0.35210 0.41253 #>  #> MZ: #>                      Estimate 2.5%     97.5%    #> Concordance           0.01560  0.01273  0.01912 #> Casewise Concordance  0.42830  0.36248  0.49677 #> Marginal              0.03643  0.03294  0.04027 #> Rel.Recur.Risk       11.75741  9.77237 13.74246 #> log(OR)               3.52382  3.13466  3.91298 #> DZ: #>                      Estimate 2.5%    97.5%   #> Concordance          0.00558  0.00465 0.00670 #> Casewise Concordance 0.15327  0.13749 0.17050 #> Marginal             0.03643  0.03294 0.04027 #> Rel.Recur.Risk       4.20744  3.78588 4.62900 #> log(OR)              1.69996  1.57262 1.82730 #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.76545  0.70500 0.82590 #>"},{"path":"http://kkholst.github.io/mets/reference/calgb8923.html","id":null,"dir":"Reference","previous_headings":"","what":"CALGB 8923, twostage randomization SMART design — calgb8923","title":"CALGB 8923, twostage randomization SMART design — calgb8923","text":"Data CALGB 8923","code":""},{"path":"http://kkholst.github.io/mets/reference/calgb8923.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"CALGB 8923, twostage randomization SMART design — calgb8923","text":"Data smart design id: id subject status : 1-death, 2-response second randomization, 0-censoring A0 : treament first randomization A1 : treament second randomization .f : treament given record (A0 A1) TR : time response sex : 0-males, 1-females consent: 1 agrees 2nd randomization, censored R: 1 response trt1: A0 trt2: A1","code":""},{"path":"http://kkholst.github.io/mets/reference/calgb8923.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"CALGB 8923, twostage randomization SMART design — calgb8923","text":"https://github.com/ycchao/code_Joint_model_SMART","code":""},{"path":"http://kkholst.github.io/mets/reference/calgb8923.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CALGB 8923, twostage randomization SMART design — calgb8923","text":"","code":"data(calgb8923)"},{"path":"http://kkholst.github.io/mets/reference/casewise.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","title":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","text":".. content description (empty lines) ..","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","text":"","code":"casewise(conc, marg, cause.marg)"},{"path":"http://kkholst.github.io/mets/reference/casewise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","text":"conc Concordance marg Marginal estimate cause.marg specififes cause used marginal cif based prodlim","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimates the casewise concordance based on Concordance and marginal estimate using prodlim but no testing — casewise","text":"","code":"## Reduce Ex.Timings library(prodlim) data(prt); prt <- force.same.cens(prt,cause=\"status\")  ### marginal cumulative incidence of prostate cancer##'  outm <- prodlim(Hist(time,status)~+1,data=prt)  times <- 60:100 cifmz <- predict(outm,cause=2,time=times,newdata=data.frame(zyg=\"MZ\")) ## cause is 2 (second cause)  cifdz <- predict(outm,cause=2,time=times,newdata=data.frame(zyg=\"DZ\"))  ### concordance for MZ and DZ twins cc <- bicomprisk(Event(time,status)~strata(zyg)+id(id),data=prt,cause=c(2,2),prodlim=TRUE) #> Strata 'DZ' #> Strata 'MZ' cdz <- cc$model$\"DZ\" cmz <- cc$model$\"MZ\"  cdz <- casewise(cdz,outm,cause.marg=2)  cmz <- casewise(cmz,outm,cause.marg=2)  plot(cmz,ci=NULL,ylim=c(0,0.5),xlim=c(60,100),legend=TRUE,col=c(3,2,1)) par(new=TRUE) plot(cdz,ci=NULL,ylim=c(0,0.5),xlim=c(60,100),legend=TRUE)  summary(cdz) #> Casewise concordance and standard errors  #>        time casewise conc se casewise #>  [1,]  59.5        0.0866      0.0865 #>  [2,]  60.5        0.0659      0.0659 #>  [3,]  61.6        0.0593      0.0593 #>  [4,]  62.7        0.0483      0.0483 #>  [5,]  63.7        0.0358      0.0358 #>  [6,]  64.8        0.0279      0.0279 #>  [7,]  65.8        0.0223      0.0223 #>  [8,]  66.9        0.0197      0.0197 #>  [9,]  68.0        0.0415      0.0297 #> [10,]  69.0        0.0335      0.0240 #> [11,]  70.1        0.0452      0.0264 #> [12,]  71.1        0.0855      0.0352 #> [13,]  72.2        0.0728      0.0300 #> [14,]  73.2        0.0888      0.0317 #> [15,]  74.3        0.1010      0.0321 #> [16,]  75.4        0.1020      0.0310 #> [17,]  76.4        0.1130      0.0318 #> [18,]  77.5        0.1230      0.0320 #> [19,]  78.5        0.1400      0.0334 #> [20,]  79.6        0.1470      0.0332 #> [21,]  80.7        0.1530      0.0329 #> [22,]  81.7        0.1460      0.0307 #> [23,]  82.8        0.1470      0.0298 #> [24,]  83.8        0.1600      0.0307 #> [25,]  84.9        0.1470      0.0282 #> [26,]  86.0        0.1620      0.0297 #> [27,]  87.0        0.1680      0.0300 #> [28,]  88.1        0.1820      0.0311 #> [29,]  89.1        0.1760      0.0301 #> [30,]  90.2        0.1950      0.0323 #> [31,]  91.2        0.2040      0.0332 #> [32,]  92.3        0.1970      0.0321 #> [33,]  93.4        0.1940      0.0315 #> [34,]  94.4        0.1970      0.0318 #> [35,]  95.5        0.1940      0.0314 #> [36,]  96.5        0.1930      0.0312 #> [37,]  97.6        0.2040      0.0330 #> [38,]  98.7        0.2010      0.0325 #> [39,]  99.7        0.1990      0.0322 #> [40,] 101.0        0.1980      0.0321 #> [41,] 102.0        0.1950      0.0316 #> [42,] 103.0        0.1940      0.0314 #> [43,] 104.0        0.1940      0.0314 #> [44,] 105.0        0.1930      0.0312 #> [45,] 106.0        0.1920      0.0311 #> [46,] 107.0        0.1920      0.0311 #> [47,] 108.0        0.1910         NaN #>  #>  summary(cmz) #> Casewise concordance and standard errors  #>        time casewise conc se casewise #>  [1,]  60.6         0.519      0.2590 #>  [2,]  61.6         0.466      0.2330 #>  [3,]  62.7         0.380      0.1900 #>  [4,]  63.7         0.285      0.1420 #>  [5,]  64.8         0.286      0.1280 #>  [6,]  65.8         0.228      0.1020 #>  [7,]  66.9         0.295      0.1120 #>  [8,]  67.9         0.306      0.1090 #>  [9,]  68.9         0.327      0.1040 #> [10,]  70.0         0.338      0.0981 #> [11,]  71.0         0.345      0.0926 #> [12,]  72.1         0.399      0.0946 #> [13,]  73.1         0.414      0.0909 #> [14,]  74.2         0.426      0.0874 #> [15,]  75.2         0.388      0.0798 #> [16,]  76.3         0.391      0.0773 #> [17,]  77.3         0.410      0.0769 #> [18,]  78.4         0.392      0.0723 #> [19,]  79.4         0.410      0.0721 #> [20,]  80.5         0.423      0.0714 #> [21,]  81.5         0.400      0.0666 #> [22,]  82.6         0.442      0.0685 #> [23,]  83.6         0.446      0.0676 #> [24,]  84.7         0.433      0.0643 #> [25,]  85.7         0.413      0.0612 #> [26,]  86.8         0.389      0.0578 #> [27,]  87.8         0.396      0.0578 #> [28,]  88.9         0.396      0.0573 #> [29,]  89.9         0.399      0.0574 #> [30,]  91.0         0.386      0.0556 #> [31,]  92.0         0.400      0.0570 #> [32,]  93.1         0.393      0.0560 #> [33,]  94.1         0.415      0.0590 #> [34,]  95.2         0.477      0.0669 #> [35,]  96.2         0.493      0.0690 #> [36,]  97.3         0.511      0.0714 #> [37,]  98.3         0.507      0.0708 #> [38,]  99.4         0.500      0.0699 #> [39,] 100.0         0.525      0.0739 #> [40,] 101.0         0.520      0.0731 #> [41,] 103.0         0.514      0.0723 #> [42,] 104.0         0.541      0.0767 #> [43,] 105.0         0.541      0.0767 #>  #>"},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"Estimates casewise concordance based Concordance marginal estimate using timereg performs test independence","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"","code":"casewise.test(conc, marg, test = \"no-test\", p = 0.01)"},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"conc Concordance marg Marginal estimate test Type test independence assumption. \"conc\" makes test concordance scale \"case\" means test casewise concordance p check marginal probability greater point p","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"Uses cluster based conservative standard errors marginal sometimes uncertainty concordance estimates. works prettey well, alternatively one can use also          funcions Casewise specific time point","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/casewise.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimates the casewise concordance based on Concordance and marginal estimate using timereg and performs test for independence — casewise.test","text":"","code":"## Reduce Ex.Timings library(\"timereg\") data(\"prt\",package=\"mets\"); prt <- force.same.cens(prt,cause=\"status\")  prt <- prt[which(prt$id %in% sample(unique(prt$id),7500)),] ### marginal cumulative incidence of prostate cancer times <- seq(60,100,by=2) outm <- timereg::comp.risk(Event(time,status)~+1,data=prt,cause=2,times=times)  cifmz <- predict(outm,X=1,uniform=0,resample.iid=1) cifdz <- predict(outm,X=1,uniform=0,resample.iid=1)  ### concordance for MZ and DZ twins cc <- bicomprisk(Event(time,status)~strata(zyg)+id(id),                  data=prt,cause=c(2,2)) #> Strata 'DZ' #> Strata 'MZ' cdz <- cc$model$\"DZ\" cmz <- cc$model$\"MZ\"  ### To compute casewise cluster argument must be passed on, ###  here with a max of 100 to limit comp-time outm <- timereg::comp.risk(Event(time,status)~+1,data=prt,                  cause=2,times=times,max.clust=100) cifmz <- predict(outm,X=1,uniform=0,resample.iid=1) cc <- bicomprisk(Event(time,status)~strata(zyg)+id(id),data=prt,                 cause=c(2,2),se.clusters=outm$clusters) #> Strata 'DZ' #> Strata 'MZ' cdz <- cc$model$\"DZ\" cmz <- cc$model$\"MZ\"  cdz <- casewise.test(cdz,cifmz,test=\"case\") ## test based on casewise cmz <- casewise.test(cmz,cifmz,test=\"conc\") ## based on concordance  plot(cmz,ylim=c(0,0.7),xlim=c(60,100)) par(new=TRUE) plot(cdz,ylim=c(0,0.7),xlim=c(60,100))   slope.process(cdz$casewise[,1],cdz$casewise[,2],iid=cdz$casewise.iid) #> $intercept #> (Intercept)  #>   0.1162653  #>  #> $slope #>      ctime  #> 0.03274472  #>  #> $se.slope #> (Intercept)       ctime  #>  0.05514246  0.04437315  #>  #> $pval.slope #>    ctime  #> 0.460551  #>   slope.process(cmz$casewise[,1],cmz$casewise[,2],iid=cmz$casewise.iid) #> $intercept #> (Intercept)  #>   0.4266816  #>  #> $slope #>      ctime  #> 0.04093122  #>  #> $se.slope #> (Intercept)       ctime  #>  0.09322922  0.06828519  #>  #> $pval.slope #>     ctime  #> 0.5488956  #>"},{"path":"http://kkholst.github.io/mets/reference/cif.html","id":null,"dir":"Reference","previous_headings":"","what":"Cumulative incidence with robust standard errors — cif","title":"Cumulative incidence with robust standard errors — cif","text":"Cumulative incidence robust standard errors","code":""},{"path":"http://kkholst.github.io/mets/reference/cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cumulative incidence with robust standard errors — cif","text":"","code":"cif(formula, data = data, cause = 1, cens.code = 0, death.code = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cumulative incidence with robust standard errors — cif","text":"formula formula 'Event' outcome strata (!) data data frame cause NULL looks , otherwise specify cause consider cens.code censoring code \"0\" default, death cens.code!=0 death.code alternative cens.code give codes death ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cumulative incidence with robust standard errors — cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/cif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cumulative incidence with robust standard errors — cif","text":"","code":"library(mets) data(bmt) bmt$cluster <- sample(1:100,408,replace=TRUE) out1 <- cif(Event(time,cause)~+1,data=bmt,cause=1) out2 <- cif(Event(time,cause)~+1+cluster(cluster),data=bmt,cause=1)  par(mfrow=c(1,2)) plot(out1,se=TRUE) plot(out2,se=TRUE)"},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":null,"dir":"Reference","previous_headings":"","what":"CIF regression — cifreg","title":"CIF regression — cifreg","text":"CIF logistic-link propodds=1 default CIF Fine-Gray (cloglog) regression propodds=NULL. FG model can also called using cifregFG function propodds=NULL.","code":""},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CIF regression — cifreg","text":"","code":"cifreg(   formula,   data,   propodds = 1,   cause = 1,   cens.code = 0,   no.codes = NULL,   death.code = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CIF regression — cifreg","text":"formula formula 'Event' outcome data data frame propodds fit logit link model, propodds=NULL fit Fine-Gray model cause interest cens.code code censoring .codes certain event codes ignored finding competing causes, can used administrative censoring. death.code can also specify death.code (addition cause) overrule default takes remaining codes (minus cause,cens.code,.codes) ... Additional arguments recreg","code":""},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"CIF regression — cifreg","text":"FG model: $$ \\int (X - E ) Y_1(t) w(t) dM_1 $$ computed summed clusters returned multiplied inverse second derivative iid.naive. $$w(t) = G(t) ((T_i \\wedge t < C_i)/G_c(T_i \\wedge t))$$ $$E(t) = S_1(t)/S_0(t)$$ $$S_j(t) = \\sum X_i^j Y_{i1}(t) w_i(t) \\exp(X_i^T \\beta)$$. iid decomposition beta's, however, also censoring term also computed added (still scaled inverse second derivative) $$ \\int (X - E ) Y_1(t) w(t) dM_1 + \\int q(s)/p(s) dM_c $$ returned iid logistic link standard errors slightly small since uncertainty recursive baseline considered, smaller data-sets recommended use prop.odds.subdist timereg also efficient due use different weights estimating equations. Alternatively, one can also bootstrap standard errors.","code":""},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"CIF regression — cifreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/cifreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"CIF regression — cifreg","text":"","code":"## data with no ties library(mets) data(bmt,package=\"mets\") bmt$time <- bmt$time+runif(nrow(bmt))*0.01 bmt$id <- 1:nrow(bmt)  ## logistic link  OR interpretation or=cifreg(Event(time,cause)~tcell+platelet+age,data=bmt,cause=1) summary(or) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> tcell    -0.709569  0.331982  0.274926  0.0326 #> platelet -0.455401  0.236010  0.187919  0.0537 #> age       0.391196  0.098040  0.083671  0.0001 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> tcell     0.49186 0.25660 0.9428 #> platelet  0.63419 0.39933 1.0072 #> age       1.47875 1.22023 1.7920 #>  par(mfrow=c(1,2)) plot(or) nd <- data.frame(tcell=c(1,0),platelet=0,age=0) por <- predict(or,nd) plot(por)   ## approximate standard errors  por <-mets:::predict.phreg(or,nd) plot(por,se=1)  ## Fine-Gray model fg=cifregFG(Event(time,cause)~tcell+platelet+age,data=bmt,cause=1) summary(fg) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> tcell    -0.596588  0.270514  0.275786  0.0274 #> platelet -0.425558  0.180762  0.187724  0.0186 #> age       0.343748  0.080265  0.086283  0.0000 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> tcell     0.55069 0.32407 0.9358 #> platelet  0.65340 0.45848 0.9312 #> age       1.41022 1.20494 1.6505 #>  ##fg=recreg(Event(time,cause)~tcell+platelet+age,data=bmt,cause=1,death.code=2) ##summary(fg) plot(fg)  nd <- data.frame(tcell=c(1,0),platelet=0,age=0) pfg <- predict(fg,nd,se=1) plot(pfg,se=1)  ## bt <- iidBaseline(fg,time=30) ## bt <- IIDrecreg(fg$cox.prep,fg,time=30)  ## not run to avoid timing issues ## gofFG(Event(time,cause)~tcell+platelet+age,data=bmt,cause=1)  sfg <- cifregFG(Event(time,cause)~strata(tcell)+platelet+age,data=bmt,cause=1) summary(sfg) #>  #>    n events #>  408    161 #>  #>  408 clusters #> coeffients: #>           Estimate      S.E.   dU^-1/2 P-value #> platelet -0.424172  0.180849  0.187824   0.019 #> age       0.341941  0.079859  0.086283   0.000 #>  #> exp(coeffients): #>          Estimate    2.5%  97.5% #> platelet  0.65431 0.45903 0.9327 #> age       1.40768 1.20372 1.6462 #>  plot(sfg)   ### predictions with CI based on iid decomposition of baseline and beta ### these are used in the predict function above fg <- cifregFG(Event(time,cause)~tcell+platelet+age,data=bmt,cause=1) Biid <- iidBaseline(fg,time=20) pfg1 <- FGprediid(Biid,nd) pfg1 #>           pred    se-log     lower     upper #> [1,] 0.2693322 0.2276224 0.1723994 0.4207661 #> [2,] 0.4343766 0.0747748 0.3751613 0.5029385"},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":null,"dir":"Reference","previous_headings":"","what":"Finds subjects related to same cluster — cluster.index","title":"Finds subjects related to same cluster — cluster.index","text":"Finds subjects related cluster","code":""},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Finds subjects related to same cluster — cluster.index","text":"","code":"cluster.index(   clusters,   index.type = FALSE,   num = NULL,   Rindex = 0,   mat = NULL,   return.all = FALSE,   code.na = NA )"},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Finds subjects related to same cluster — cluster.index","text":"clusters list indeces index.type TRUE already list integers index.type num get numbering according num-type separate columns Rindex index starts 1, C 0 mat return matrix indeces return.return arguments code.na code missing values","code":""},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Finds subjects related to same cluster — cluster.index","text":"Cluster indeces","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Finds subjects related to same cluster — cluster.index","text":"Klaus Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/cluster.index.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Finds subjects related to same cluster — cluster.index","text":"","code":"i<-c(1,1,2,2,1,3) d<- cluster.index(i) print(d) #> $clusters #> [1] 0 0 1 1 0 2 #>  #> $maxclust #> [1] 3 #>  #> $idclustmat #>      [,1] [,2] [,3] #> [1,]    0    1    4 #> [2,]    2    3   NA #> [3,]    5   NA   NA #>  #> $cluster.size #> [1] 3 2 1 #>  #> $uniqueclust #> [1] 3 #>  #> $firstclustid #> [1] 0 2 5 #>   type<-c(\"m\",\"f\",\"m\",\"c\",\"c\",\"c\") d<- cluster.index(i,num=type,Rindex=1) print(d) #> $clusters #> [1] 0 0 1 1 0 2 #>  #> $maxclust #> [1] 3 #>  #> $idclustmat #>      [,1] [,2] [,3] #> [1,]    4    1    0 #> [2,]    3   NA    2 #> [3,]    5   NA   NA #>  #> $cluster.size #> [1] 3 2 1 #>  #> $uniqueclust #> [1] 3 #>  #> $firstclustid #> [1] 1 3 6 #>  #> $idclust #>      [,1] [,2] [,3] #> [1,]    5    2    1 #> [2,]    4   NA    3 #> [3,]    6   NA   NA #>"},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":null,"dir":"Reference","previous_headings":"","what":"Concordance Computes concordance and casewise concordance — concordanceCor","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"Concordance Twins","code":""},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"","code":"concordanceCor(   object,   cif1,   cif2 = NULL,   messages = TRUE,   model = NULL,   coefs = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"object Output cor.cif, rr.cif .cif function cif1 Marginal cumulative incidence cif2 Marginal cumulative incidence cause (cause2)  different cause1 messages print messages model Specfifies wich model considered object given. coefs Specfifies  dependence parameters object given. ... Extra arguments, used.","code":""},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"concordance probability twins experienced event interest defined $$  cor(t) = P(T_1 \\leq t, \\epsilon_1 =1 , T_2 \\leq t, \\epsilon_2=1)  $$ Similarly, casewise concordance $$  casewise(t) = \\frac{cor(t)}{P(T_1 \\leq t, \\epsilon_1=1) } $$ probability twin \"2\" event given twins \"1\" .","code":""},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"Estimating twin concordance bivariate competing risks twin data Thomas H. Scheike, Klaus K. Holst Jacob B. Hjelmborg, Statistics Medicine 2014, 1193-1204 Estimating Twin Pair Concordance Age Onset. Thomas H. Scheike, Jacob V B Hjelmborg, Klaus K. Holst, 2015 Behavior genetics DOI:10.1007/s10519-015-9729-3","code":""},{"path":"http://kkholst.github.io/mets/reference/concordanceCor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Concordance Computes concordance and casewise concordance — concordanceCor","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"Fits parametric model log-cross-odds-ratio predictive effect cumulative incidence curves \\(T_1\\) experiencing cause given \\(T_2\\) experienced cause k : $$ \\log(COR(|k))  =  h(\\theta,z_1,,z_2,k,t)=_{default}  \\theta^T z = $$ log cross odds ratio $$ COR(|k) = \\frac{O(T_1 \\leq t,cause_1=| T_2 \\leq t,cause_2=k)}{ O(T_1 \\leq t,cause_1=)} $$ conditional odds divided unconditional odds, odds , respectively $$ O(T_1 \\leq t,cause_1=| T_2 \\leq t,cause_1=k) = \\frac{ P_x(T_1 \\leq t,cause_1=| T_2 \\leq t,cause_2=k)}{ P_x((T_1 \\leq t,cause_1=)^c | T_2 \\leq t,cause_2=k)} $$ $$ O(T_1 \\leq t,cause_1=) = \\frac{P_x(T_1 \\leq t,cause_1=)}{P_x((T_1 \\leq t,cause_1=)^c )}. $$ \\(B^c\\) complement event \\(B\\), \\(P_x\\) distribution given covariates (\\(x\\) subject specific \\(z\\) cluster specific covariates), \\(h()\\) function simple identity \\(\\theta^T z\\) default.","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"","code":"cor.cif(   cif,   data,   cause = NULL,   times = NULL,   cause1 = 1,   cause2 = 1,   cens.code = NULL,   cens.model = \"KM\",   Nit = 40,   detail = 0,   clusters = NULL,   theta = NULL,   theta.des = NULL,   step = 1,   sym = 0,   weights = NULL,   par.func = NULL,   dpar.func = NULL,   dimpar = NULL,   score.method = \"nlminb\",   same.cens = FALSE,   censoring.weights = NULL,   silent = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"cif model object timereg::comp.risk function marginal cumulative incidence cause1, .e., event interest, whose odds comparision compared conditional odds given cause2 data data.frame variables. cause specifies causes  related death times, value cens.code censoring value. missing comes marginal cif. times time-vector specifies times used estimating euqations cross-odds-ratio estimation. cause1 specificies cause considered. cause2 specificies cause conditioned . cens.code specificies code censoring NULL uses one marginal cif model. cens.model specified model use ICPW, KM Kaplan-Meier alternatively may \"cox\" Nit number iterations Newton-Raphson algorithm. detail 0 details printed iterations, 1 details given. clusters specifies cluster structure. theta specifies starting values cross-odds-ratio parameters model. theta.des specifies regression design cross-odds-ratio parameters. step specifies step size Newton-Raphson algorithm. sym specifies symmetry used model. weights weights estimating equations. par.func parfunc dpar.func dparfunc dimpar dimpar score.method \"nlminb\", can also use \"nr\". .cens true censoring within clusters assumed variable, default independent censoring. censoring.weights probabilities used bivariate censoring dist. silent 1 suppress output convergence related issues. ... used.","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"returns object type 'cor'. following arguments: theta estimate proportional odds parameters model. var.theta variance gamma. hess derivative used score. score scores final stage. score scores final stage. theta.iid matrix iid decomposition parametric effects.","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"dependence measure given $$ (,k) = \\log ( \\frac{O(T_1 \\leq t,cause_1=| T_2 \\leq t,cause_2=k)}{ O(T_1 \\leq t,cause_1=) | T_2 \\leq t,cause_2=k)} $$ measure numerically stabile COR measure, symetric ,k. RR dependence measure given $$ RR(,k) = \\log ( \\frac{P(T_1 \\leq t,cause_1=, T_2 \\leq t,cause_2=k)}{ P(T_1 \\leq t,cause_1=) P(T_2 \\leq t,cause_2=k)} $$ measure numerically stabile COR measure, symetric ,k. model fitted symmetry (sym=1), .e., assumed \\(T_1\\) \\(T_2\\) can interchanged leads cross-odd-ratio (.e. \\(COR(|k) = COR(k|))\\), expected twins without symmetry might case mothers daughters (sym=0). \\(h()\\) may specified R-function parameters, see example , default simply \\(\\theta^T z\\).","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"Cross odds ratio Modelling dependence Multivariate Competing Risks Data, Scheike Sun (2012), Biostatistics. Semiparametric Random Effects Model Multivariate Competing Risks Data, Scheike, Zhang, Sun, Jensen (2010), Biometrika.","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/cor.cif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-odds-ratio, OR or RR risk regression for competing risks — cor.cif","text":"","code":"## Reduce Ex.Timings library(\"timereg\") data(multcif); multcif$cause[multcif$cause==0] <- 2 zyg <- rep(rbinom(200,1,0.5),each=2) theta.des <- model.matrix(~-1+factor(zyg))  times=seq(0.05,1,by=0.05) # to speed up computations use only these time-points add <- timereg::comp.risk(Event(time,cause)~+1+cluster(id),data=multcif,cause=1,                n.sim=0,times=times,model=\"fg\",max.clust=NULL) add2 <- timereg::comp.risk(Event(time,cause)~+1+cluster(id),data=multcif,cause=2,                n.sim=0,times=times,model=\"fg\",max.clust=NULL)  out1 <- cor.cif(add,data=multcif,cause1=1,cause2=1) summary(out1) #> Cross odds ratio dependence for competing risks #>  #> Effect of cause1=1 on cause2=1 under symmetry=0 #>  #>           log-Coef.   SE     z P-val Cross odds ratio   SE #> intercept      1.04 9.33 0.111 0.911             2.82 26.4 #>   out2 <- cor.cif(add,data=multcif,cause1=1,cause2=1,theta.des=theta.des) summary(out2) #> Cross odds ratio dependence for competing risks #>  #> Effect of cause1=1 on cause2=1 under symmetry=0 #>  #>              log-Coef.    SE      z P-val Cross odds ratio   SE #> factor(zyg)0     0.882  8.96 0.0984 0.922             2.42 21.6 #> factor(zyg)1     1.230 21.80 0.0562 0.955             3.41 74.4 #>   ##out3 <- cor.cif(add,data=multcif,cause1=1,cause2=2,cif2=add2) ##summary(out3) ########################################################### # investigating further models using parfunc and dparfunc ########################################################### set.seed(100) prt<-simnordic.random(2000,cordz=2,cormz=5) prt$status <-prt$cause table(prt$status) #>  #>    0    1    2  #> 2102  597 5301   times <- seq(40,100,by=10) cifmod <- timereg::comp.risk(Event(time,cause)~+1+cluster(id),data=prt,                     cause=1,n.sim=0,                     times=times,conservative=1,max.clust=NULL,model=\"fg\") theta.des <- model.matrix(~-1+factor(zyg),data=prt)  parfunc <- function(par,t,pardes) { par <- pardes %*% c(par[1],par[2]) +        pardes %*% c( par[3]*(t-60)/12,par[4]*(t-60)/12) par } head(parfunc(c(0.1,1,0.1,1),50,theta.des)) #>        [,1] #> 1 0.1666667 #> 2 0.1666667 #> 3 0.1666667 #> 4 0.1666667 #> 5 0.1666667 #> 6 0.1666667  dparfunc <- function(par,t,pardes) { dpar <- cbind(pardes, t(t(pardes) * c( (t-60)/12,(t-60)/12)) ) dpar } head(dparfunc(c(0.1,1,0.1,1),50,theta.des)) #>   factor(zyg)MZ factor(zyg)DZ factor(zyg)MZ factor(zyg)DZ #> 1             0             1             0    -0.8333333 #> 2             0             1             0    -0.8333333 #> 3             0             1             0    -0.8333333 #> 4             0             1             0    -0.8333333 #> 5             0             1             0    -0.8333333 #> 6             0             1             0    -0.8333333  names(prt) #>  [1] \"time\"      \"cause\"     \"x\"         \"country\"   \"id\"        \"cens\"      #>  [7] \"stime\"     \"entry\"     \"truncated\" \"zyg\"       \"status\"    or1 <- or.cif(cifmod,data=prt,cause1=1,cause2=1,theta.des=theta.des,               same.cens=TRUE,theta=c(0.6,1.1,0.1,0.1),               par.func=parfunc,dpar.func=dparfunc,dimpar=4,               score.method=\"nr\",detail=1) #> [1] \"Fisher-Scoring ===================: it= 1\" #> theta:[1] 0.6 1.1 0.1 0.1 #> score:[1]  4.4038139 -0.5697823 11.6772976 -1.6949449 #> hess:          [,1]      [,2]       [,3]       [,4] #> [1,] -2.380951  0.000000  -6.184041   0.000000 #> [2,]  0.000000 -3.262648   0.000000  -8.285153 #> [3,] -6.184041  0.000000 -17.409502   0.000000 #> [4,]  0.000000 -8.285153   0.000000 -23.052048 #> [1] \"Fisher-Scoring ===================: it= 2\" #> theta:[1]  1.98846481  1.23830300  0.27754528 -0.02323445 #> score:[1] -1.25603529 -0.01146765 -2.79344759 -0.04626781 #> hess:          [,1]      [,2]       [,3]       [,4] #> [1,] -3.955147  0.000000  -9.393342   0.000000 #> [2,]  0.000000 -3.009163   0.000000  -7.530575 #> [3,] -9.393342  0.000000 -25.237970   0.000000 #> [4,]  0.000000 -7.530575   0.000000 -20.789841 #> [1] \"Fisher-Scoring ===================: it= 3\" #> theta:[1]  1.51716906  1.25710704  0.34227293 -0.03227122 #> score:[1]  0.0084015577  0.0001720138  0.0266070641 -0.0006890471 #> hess:          [,1]      [,2]       [,3]       [,4] #> [1,] -3.976728  0.000000  -9.711857   0.000000 #> [2,]  0.000000 -3.004696   0.000000  -7.507249 #> [3,] -9.711857  0.000000 -26.396637   0.000000 #> [4,]  0.000000 -7.507249   0.000000 -20.707731 #> [1] \"Fisher-Scoring ===================: it= 4\" #> theta:[1]  1.51373019  1.25859721  0.34454613 -0.03284474 #> score:[1]  1.000615e-04  1.638158e-05  2.676517e-04 -3.827292e-05 #> hess:          [,1]      [,2]      [,3]       [,4] #> [1,] -3.974979  0.000000  -9.70733   0.000000 #> [2,]  0.000000 -3.004947   0.00000  -7.507026 #> [3,] -9.707330  0.000000 -26.38239   0.000000 #> [4,]  0.000000 -7.507026   0.00000 -20.705898 #> [1] \"Fisher-Scoring ===================: it= 5\" #> theta:[1]  1.51373411  1.25870404  0.34455483 -0.03288531 #> score:[1]  7.061280e-08  1.183464e-06  1.291720e-06 -2.693329e-06 #> hess:          [,1]      [,2]       [,3]       [,4] #> [1,] -3.974972  0.000000  -9.707299   0.000000 #> [2,]  0.000000 -3.004967   0.000000  -7.507016 #> [3,] -9.707299  0.000000 -26.382278   0.000000 #> [4,]  0.000000 -7.507016   0.000000 -20.705785 #> [1] \"Fisher-Scoring ===================: it= 6\" #> theta:[1]  1.51373311  1.25871167  0.34455525 -0.03288821 #> score:[1]  3.249126e-08  8.427829e-08  5.920297e-08 -1.934684e-07 #> hess:          [,1]      [,2]       [,3]       [,4] #> [1,] -3.974972  0.000000  -9.707298   0.000000 #> [2,]  0.000000 -3.004969   0.000000  -7.507015 #> [3,] -9.707298  0.000000 -26.382277   0.000000 #> [4,]  0.000000 -7.507015   0.000000 -20.705777 summary(or1) #> OR for dependence for competing risks #>  #> OR of cumulative incidence for cause1= 1  and cause2= 1 #>        log-ratio Coef.    SE      z    P-val Ratio    SE #> R-func          1.5100 0.348  4.350 1.39e-05 4.540 1.580 #> R-func          1.2600 0.408  3.090 2.03e-03 3.520 1.440 #> R-func          0.3450 0.129  2.680 7.43e-03 1.410 0.182 #> R-func         -0.0329 0.111 -0.296 7.67e-01 0.968 0.107 #>    cor1 <- cor.cif(cifmod,data=prt,cause1=1,cause2=1,theta.des=theta.des,                  same.cens=TRUE,theta=c(0.5,1.0,0.1,0.1),                  par.func=parfunc,dpar.func=dparfunc,dimpar=4,                  control=list(trace=TRUE),detail=1) #>   0:     764.39429: 0.500000  1.00000 0.100000 0.100000 #>   1:     760.41547: 0.638110 0.960514 0.467772 -0.0132205 #>   2:     760.40387: 0.653394 0.974160 0.451035 0.0181984 #>   3:     760.38389: 0.690757 0.972239 0.448591 0.00142152 #>   4:     760.37606: 0.718635 0.978903 0.420006 0.00837761 #>   5:     760.25923:  1.06302  1.01086 0.300325 -0.0208226 #>   6:     760.25074:  1.17117  1.03359 0.291034 0.00480685 #>   7:     760.21950:  1.27004  1.04554 0.238147 -0.0108041 #>   8:     760.20998:  1.37486  1.06362 0.202831 -0.0306896 #>   9:     760.20957:  1.45490  1.09158 0.167676 -0.0343349 #>  10:     760.20850:  1.41961  1.09023 0.182575 -0.0388101 #>  11:     760.20845:  1.41809  1.09720 0.183351 -0.0412561 #>  12:     760.20842:  1.41724  1.11066 0.183663 -0.0460526 #>  13:     760.20842:  1.41733  1.11052 0.183624 -0.0459859 #> $par #> [1]  1.4173313  1.1105171  0.1836238 -0.0459859 #>  #> $objective #> [1] 760.2084 #>  #> $convergence #> [1] 0 #>  #> $iterations #> [1] 13 #>  #> $evaluations #> function gradient  #>       19       71  #>  #> $message #> [1] \"relative convergence (4)\" #>  #> iid decomposition summary(cor1) #> Cross odds ratio dependence for competing risks #>  #> Effect of cause1=1 on cause2=1 under symmetry=0 #>  #>        log-Coef.   SE        z P-val Cross odds ratio  SE #> R-func     1.420 66.5  0.02130 0.983            4.130 274 #> R-func     1.110 71.3  0.01560 0.988            3.040 217 #> R-func     0.184 23.3  0.00789 0.994            1.200  28 #> R-func    -0.046 22.0 -0.00209 0.998            0.955  21 #>   ### piecewise contant OR model gparfunc <- function(par,t,pardes) {   cuts <- c(0,80,90,120)   grop <- diff(t<cuts) paru  <- (pardes[,1]==1) * sum(grop*par[1:3]) +     (pardes[,2]==1) * sum(grop*par[4:6]) paru }  dgparfunc <- function(par,t,pardes) {   cuts <- c(0,80,90,120)   grop <- diff(t<cuts) par1 <- matrix(c(grop),nrow(pardes),length(grop),byrow=TRUE) parmz <- par1* (pardes[,1]==1) pardz <- (pardes[,2]==1) * par1 dpar <- cbind( parmz,pardz) dpar } head(dgparfunc(rep(0.1,6),50,theta.des)) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    0    0    0    1    0    0 #> [2,]    0    0    0    1    0    0 #> [3,]    0    0    0    1    0    0 #> [4,]    0    0    0    1    0    0 #> [5,]    0    0    0    1    0    0 #> [6,]    0    0    0    1    0    0 head(gparfunc(rep(0.1,6),50,theta.des)) #>   1   2   3   4   5   6  #> 0.1 0.1 0.1 0.1 0.1 0.1   or1g <- or.cif(cifmod,data=prt,cause1=1,cause2=1,                theta.des=theta.des, same.cens=TRUE,                par.func=gparfunc,dpar.func=dgparfunc,                dimpar=6,score.method=\"nr\",detail=1) #> [1] \"Fisher-Scoring ===================: it= 1\" #> theta:[1] 0.1 0.1 0.1 0.1 0.1 0.1 #> score:[1] 0.1272695 0.6721568 3.2995329 0.1193734 0.3414362 1.1416387 #> hess:            [,1]       [,2]       [,3]        [,4]       [,5]       [,6] #> [1,] -0.05264815  0.0000000  0.0000000  0.00000000  0.0000000  0.0000000 #> [2,]  0.00000000 -0.2202109  0.0000000  0.00000000  0.0000000  0.0000000 #> [3,]  0.00000000  0.0000000 -0.8603209  0.00000000  0.0000000  0.0000000 #> [4,]  0.00000000  0.0000000  0.0000000 -0.05264815  0.0000000  0.0000000 #> [5,]  0.00000000  0.0000000  0.0000000  0.00000000 -0.2202109  0.0000000 #> [6,]  0.00000000  0.0000000  0.0000000  0.00000000  0.0000000 -0.8603209 #> [1] \"Fisher-Scoring ===================: it= 2\" #> theta:[1] 2.517359 3.152333 3.935235 2.367380 1.650497 1.426992 #> score:[1] -0.4338385 -1.0147952 -2.5460332 -0.3722739 -0.3313216 -0.7014266 #> hess:           [,1]       [,2]      [,3]       [,4]       [,5]      [,6] #> [1,] -0.4976024  0.0000000  0.000000  0.0000000  0.0000000  0.000000 #> [2,]  0.0000000 -0.8549109  0.000000  0.0000000  0.0000000  0.000000 #> [3,]  0.0000000  0.0000000 -1.600519  0.0000000  0.0000000  0.000000 #> [4,]  0.0000000  0.0000000  0.000000 -0.4751796  0.0000000  0.000000 #> [5,]  0.0000000  0.0000000  0.000000  0.0000000 -0.8248922  0.000000 #> [6,]  0.0000000  0.0000000  0.000000  0.0000000  0.0000000 -2.408282 #> [1] \"Fisher-Scoring ===================: it= 3\" #> theta:[1] 1.645501 1.965314 2.344480 1.583942 1.248842 1.135736 #> score:[1] -0.02982302  0.04411753  0.63062461 -0.02738202 -0.01381585 -0.01990217 #> hess:           [,1]       [,2]      [,3]      [,4]       [,5]      [,6] #> [1,] -0.3202401  0.0000000  0.000000  0.000000  0.0000000  0.000000 #> [2,]  0.0000000 -0.9097494  0.000000  0.000000  0.0000000  0.000000 #> [3,]  0.0000000  0.0000000 -2.766973  0.000000  0.0000000  0.000000 #> [4,]  0.0000000  0.0000000  0.000000 -0.305418  0.0000000  0.000000 #> [5,]  0.0000000  0.0000000  0.000000  0.000000 -0.6707065  0.000000 #> [6,]  0.0000000  0.0000000  0.000000  0.000000  0.0000000 -2.098342 #> [1] \"Fisher-Scoring ===================: it= 4\" #> theta:[1] 1.552374 2.013808 2.572392 1.494288 1.228243 1.126251 #> score:[1] -4.297262e-04 -1.168380e-04  3.731020e-03 -5.158185e-04 -4.497370e-05 #> [6]  2.252054e-06 #> hess:           [,1]       [,2]      [,3]       [,4]       [,5]      [,6] #> [1,] -0.2978324  0.0000000  0.000000  0.0000000  0.0000000  0.000000 #> [2,]  0.0000000 -0.9191812  0.000000  0.0000000  0.0000000  0.000000 #> [3,]  0.0000000  0.0000000 -2.695765  0.0000000  0.0000000  0.000000 #> [4,]  0.0000000  0.0000000  0.000000 -0.2839372  0.0000000  0.000000 #> [5,]  0.0000000  0.0000000  0.000000  0.0000000 -0.6619257  0.000000 #> [6,]  0.0000000  0.0000000  0.000000  0.0000000  0.0000000 -2.087228 #> [1] \"Fisher-Scoring ===================: it= 5\" #> theta:[1] 1.550931 2.013681 2.573776 1.492471 1.228175 1.126252 #> score:[1]  1.115720e-06 -7.362229e-10  1.590683e-06 -1.109555e-06 -4.887707e-10 #> [6] -2.862365e-09 #> hess:           [,1]       [,2]      [,3]       [,4]       [,5]     [,6] #> [1,] -0.2974862  0.0000000  0.000000  0.0000000  0.0000000  0.00000 #> [2,]  0.0000000 -0.9191579  0.000000  0.0000000  0.0000000  0.00000 #> [3,]  0.0000000  0.0000000 -2.695165  0.0000000  0.0000000  0.00000 #> [4,]  0.0000000  0.0000000  0.000000 -0.2835044  0.0000000  0.00000 #> [5,]  0.0000000  0.0000000  0.000000  0.0000000 -0.6618966  0.00000 #> [6,]  0.0000000  0.0000000  0.000000  0.0000000  0.0000000 -2.08723 #> [1] \"Fisher-Scoring ===================: it= 6\" #> theta:[1] 1.550935 2.013681 2.573776 1.492467 1.228175 1.126252 #> score:[1] -3.212719e-09 -6.107094e-15  8.052016e-10 -1.957429e-09 -2.185535e-15 #> [6]  7.569806e-11 #> hess:           [,1]       [,2]      [,3]       [,4]       [,5]     [,6] #> [1,] -0.2974871  0.0000000  0.000000  0.0000000  0.0000000  0.00000 #> [2,]  0.0000000 -0.9191578  0.000000  0.0000000  0.0000000  0.00000 #> [3,]  0.0000000  0.0000000 -2.695165  0.0000000  0.0000000  0.00000 #> [4,]  0.0000000  0.0000000  0.000000 -0.2835035  0.0000000  0.00000 #> [5,]  0.0000000  0.0000000  0.000000  0.0000000 -0.6618966  0.00000 #> [6,]  0.0000000  0.0000000  0.000000  0.0000000  0.0000000 -2.08723 summary(or1g) #> OR for dependence for competing risks #>  #> OR of cumulative incidence for cause1= 1  and cause2= 1 #>        log-ratio Coef.    SE    z    P-val Ratio    SE #> R-func            1.55 0.362 4.28 1.87e-05  4.72 1.710 #> R-func            2.01 0.325 6.20 5.64e-10  7.49 2.430 #> R-func            2.57 0.352 7.31 2.71e-13 13.10 4.620 #> R-func            1.49 0.369 4.05 5.12e-05  4.45 1.640 #> R-func            1.23 0.307 3.99 6.49e-05  3.41 1.050 #> R-func            1.13 0.274 4.11 4.01e-05  3.08 0.846 #>  names(or1g) #>  [1] \"theta\"      \"score\"      \"hess\"       \"hessi\"      \"var.theta\"  #>  [6] \"theta.iid\"  \"score1\"     \"thetanames\" \"brierscore\" \"p11\"        #> [11] \"call\"       head(or1g$theta.iid) #>      [,1] [,2] [,3]         [,4]         [,5]         [,6] #> [1,]    0    0    0 0.0007341431 0.0008025713 0.0008395676 #> [2,]    0    0    0 0.0007341431 0.0008025713 0.0008395676 #> [3,]    0    0    0 0.0007341431 0.0008025713 0.0008395676 #> [4,]    0    0    0 0.0007341431 0.0008025713 0.0008395676 #> [5,]    0    0    0 0.0007341431 0.0008025713 0.0008395676 #> [6,]    0    0    0 0.0007341431 0.0008025713 0.0008395676"},{"path":"http://kkholst.github.io/mets/reference/count.history.html","id":null,"dir":"Reference","previous_headings":"","what":"Counts the number of previous events of two types for recurrent events processes — count.history","title":"Counts the number of previous events of two types for recurrent events processes — count.history","text":"Counts number previous events two types recurrent events processes","code":""},{"path":"http://kkholst.github.io/mets/reference/count.history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Counts the number of previous events of two types for recurrent events processes — count.history","text":"","code":"count.history(   data,   status = \"status\",   id = \"id\",   types = 1,   names.count = \"Count\",   lag = TRUE,   multitype = FALSE,   marks = NULL )"},{"path":"http://kkholst.github.io/mets/reference/count.history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Counts the number of previous events of two types for recurrent events processes — count.history","text":"data data-frame status name status id id types types events (code) related status (multiple values possible) names.count name Counts, example Count1 Count2 types=c(1,2) lag true counts previously observed, lag=FALSE counts know multitype, multitype true counts status \"\" types, otherwise counts value type, types=c(1,2) marks values related status (\"\" types), counts marks types, multitype=TRUE","code":""},{"path":"http://kkholst.github.io/mets/reference/count.history.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Counts the number of previous events of two types for recurrent events processes — count.history","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/count.history.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Counts the number of previous events of two types for recurrent events processes — count.history","text":"","code":"data(hfactioncpx12) hf <- hfactioncpx12 dtable(hf,~status) #>  #> status #>    0    1    2  #>  617 1391  124  #>  rr <-  count.history(hf,types=1:2,id=\"id\",status=\"status\") dtable(rr,~\"Count*\"+status,level=1) #>  #> Count1 #>   0   1   2   3   4   5   6   7  #> 741 507 319 209 146  97  67  46  #>  #> Count2 #>    0  #> 2132  #>  #> status #>    0    1    2  #>  617 1391  124  #>"},{"path":"http://kkholst.github.io/mets/reference/daggregate.html","id":null,"dir":"Reference","previous_headings":"","what":"aggregating for for data frames — daggregate","title":"aggregating for for data frames — daggregate","text":"aggregating data frames","code":""},{"path":"http://kkholst.github.io/mets/reference/daggregate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"aggregating for for data frames — daggregate","text":"","code":"daggregate(   data,   y = NULL,   x = NULL,   subset,   ...,   fun = \"summary\",   regex = mets.options()$regex,   missing = FALSE,   remove.empty = FALSE,   matrix = FALSE,   silent = FALSE,   na.action = na.pass,   convert = NULL )"},{"path":"http://kkholst.github.io/mets/reference/daggregate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"aggregating for for data frames — daggregate","text":"data data.frame y name variable, formula, names variables data frame. x name variable, formula, names variables data frame. subset subset expression ... additional arguments lower level functions fun function defining aggregation regex interpret x,y regular expressions missing Missing used groups (x) remove.empty remove empty groups output matrix TRUE matrix returned instead array silent suppress messages na.action model.frame deals 'NA's convert TRUE try coerce result matrix. Can also user-defined function","code":""},{"path":"http://kkholst.github.io/mets/reference/daggregate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"aggregating for for data frames — daggregate","text":"","code":"data(\"sTRACE\") daggregate(iris, \"^.e.al\", x=\"Species\", fun=cor, regex=TRUE) #> Species: setosa #>              Sepal.Length Sepal.Width Petal.Length Petal.Width #> Sepal.Length    1.0000000   0.7425467    0.2671758   0.2780984 #> Sepal.Width     0.7425467   1.0000000    0.1777000   0.2327520 #> Petal.Length    0.2671758   0.1777000    1.0000000   0.3316300 #> Petal.Width     0.2780984   0.2327520    0.3316300   1.0000000 #> ------------------------------------------------------------  #> Species: versicolor #>              Sepal.Length Sepal.Width Petal.Length Petal.Width #> Sepal.Length    1.0000000   0.5259107    0.7540490   0.5464611 #> Sepal.Width     0.5259107   1.0000000    0.5605221   0.6639987 #> Petal.Length    0.7540490   0.5605221    1.0000000   0.7866681 #> Petal.Width     0.5464611   0.6639987    0.7866681   1.0000000 #> ------------------------------------------------------------  #> Species: virginica #>              Sepal.Length Sepal.Width Petal.Length Petal.Width #> Sepal.Length    1.0000000   0.4572278    0.8642247   0.2811077 #> Sepal.Width     0.4572278   1.0000000    0.4010446   0.5377280 #> Petal.Length    0.8642247   0.4010446    1.0000000   0.3221082 #> Petal.Width     0.2811077   0.5377280    0.3221082   1.0000000 daggregate(iris, Sepal.Length+Petal.Length ~Species, fun=summary) #> Species: setosa #>   Sepal.Length    Petal.Length   #>  Min.   :4.300   Min.   :1.000   #>  1st Qu.:4.800   1st Qu.:1.400   #>  Median :5.000   Median :1.500   #>  Mean   :5.006   Mean   :1.462   #>  3rd Qu.:5.200   3rd Qu.:1.575   #>  Max.   :5.800   Max.   :1.900   #> ------------------------------------------------------------  #> Species: versicolor #>   Sepal.Length    Petal.Length  #>  Min.   :4.900   Min.   :3.00   #>  1st Qu.:5.600   1st Qu.:4.00   #>  Median :5.900   Median :4.35   #>  Mean   :5.936   Mean   :4.26   #>  3rd Qu.:6.300   3rd Qu.:4.60   #>  Max.   :7.000   Max.   :5.10   #> ------------------------------------------------------------  #> Species: virginica #>   Sepal.Length    Petal.Length   #>  Min.   :4.900   Min.   :4.500   #>  1st Qu.:6.225   1st Qu.:5.100   #>  Median :6.500   Median :5.550   #>  Mean   :6.588   Mean   :5.552   #>  3rd Qu.:6.900   3rd Qu.:5.875   #>  Max.   :7.900   Max.   :6.900   daggregate(iris, log(Sepal.Length)+I(Petal.Length>1.5) ~ Species,                  fun=summary) #> Species: setosa #>  log(Sepal.Length) I(Petal.Length > 1.5) #>  Min.   :1.459     Mode :logical         #>  1st Qu.:1.569     FALSE:37              #>  Median :1.609     TRUE :13              #>  Mean   :1.608                           #>  3rd Qu.:1.649                           #>  Max.   :1.758                           #> ------------------------------------------------------------  #> Species: versicolor #>  log(Sepal.Length) I(Petal.Length > 1.5) #>  Min.   :1.589     Mode:logical          #>  1st Qu.:1.723     TRUE:50               #>  Median :1.775                           #>  Mean   :1.777                           #>  3rd Qu.:1.841                           #>  Max.   :1.946                           #> ------------------------------------------------------------  #> Species: virginica #>  log(Sepal.Length) I(Petal.Length > 1.5) #>  Min.   :1.589     Mode:logical          #>  1st Qu.:1.829     TRUE:50               #>  Median :1.872                           #>  Mean   :1.881                           #>  3rd Qu.:1.932                           #>  Max.   :2.067                           daggregate(iris, \"*Length*\", x=\"Species\", fun=head) #> Species: setosa #>   Sepal.Length Petal.Length #> 1          5.1          1.4 #> 2          4.9          1.4 #> 3          4.7          1.3 #> 4          4.6          1.5 #> 5          5.0          1.4 #> 6          5.4          1.7 #> ------------------------------------------------------------  #> Species: versicolor #>    Sepal.Length Petal.Length #> 51          7.0          4.7 #> 52          6.4          4.5 #> 53          6.9          4.9 #> 54          5.5          4.0 #> 55          6.5          4.6 #> 56          5.7          4.5 #> ------------------------------------------------------------  #> Species: virginica #>     Sepal.Length Petal.Length #> 101          6.3          6.0 #> 102          5.8          5.1 #> 103          7.1          5.9 #> 104          6.3          5.6 #> 105          6.5          5.8 #> 106          7.6          6.6 daggregate(iris, \"^.e.al\", x=\"Species\", fun=tail, regex=TRUE) #> Species: setosa #>    Sepal.Length Sepal.Width Petal.Length Petal.Width #> 45          5.1         3.8          1.9         0.4 #> 46          4.8         3.0          1.4         0.3 #> 47          5.1         3.8          1.6         0.2 #> 48          4.6         3.2          1.4         0.2 #> 49          5.3         3.7          1.5         0.2 #> 50          5.0         3.3          1.4         0.2 #> ------------------------------------------------------------  #> Species: versicolor #>     Sepal.Length Sepal.Width Petal.Length Petal.Width #> 95           5.6         2.7          4.2         1.3 #> 96           5.7         3.0          4.2         1.2 #> 97           5.7         2.9          4.2         1.3 #> 98           6.2         2.9          4.3         1.3 #> 99           5.1         2.5          3.0         1.1 #> 100          5.7         2.8          4.1         1.3 #> ------------------------------------------------------------  #> Species: virginica #>     Sepal.Length Sepal.Width Petal.Length Petal.Width #> 145          6.7         3.3          5.7         2.5 #> 146          6.7         3.0          5.2         2.3 #> 147          6.3         2.5          5.0         1.9 #> 148          6.5         3.0          5.2         2.0 #> 149          6.2         3.4          5.4         2.3 #> 150          5.9         3.0          5.1         1.8 daggregate(sTRACE, status~ diabetes, fun=table) #> diabetes: 0 #> status #>   0   7   9  #> 220   5 228  #> ------------------------------------------------------------  #> diabetes: 1 #> status #>  0  9  #> 16 31  daggregate(sTRACE, status~ diabetes+sex, fun=table) #> diabetes: 0 #> sex: 0 #> status #>  0  9  #> 63 80  #> ------------------------------------------------------------  #> diabetes: 1 #> sex: 0 #> status #>  0  9  #>  6 13  #> ------------------------------------------------------------  #> diabetes: 0 #> sex: 1 #> status #>   0   7   9  #> 157   5 148  #> ------------------------------------------------------------  #> diabetes: 1 #> sex: 1 #> status #>  0  9  #> 10 18  daggregate(sTRACE, status + diabetes+sex ~ vf+I(wmi>1.4), fun=table) #> vf: 0 #> I(wmi > 1.4): FALSE #> , , sex = 0 #>  #>       diabetes #> status  0  1 #>      0 21  3 #>      7  0  0 #>      9 39  8 #>  #> , , sex = 1 #>  #>       diabetes #> status  0  1 #>      0 48  6 #>      7  1  0 #>      9 94 14 #>  #> ------------------------------------------------------------  #> vf: 1 #> I(wmi > 1.4): FALSE #> , , sex = 0 #>  #>       diabetes #> status 0 1 #>      0 2 0 #>      9 5 1 #>  #> , , sex = 1 #>  #>       diabetes #> status 0 1 #>      0 4 0 #>      9 8 0 #>  #> ------------------------------------------------------------  #> vf: 0 #> I(wmi > 1.4): TRUE #> , , sex = 0 #>  #>       diabetes #> status   0   1 #>      0  38   3 #>      7   0   0 #>      9  34   4 #>  #> , , sex = 1 #>  #>       diabetes #> status   0   1 #>      0 102   4 #>      7   4   0 #>      9  44   4 #>  #> ------------------------------------------------------------  #> vf: 1 #> I(wmi > 1.4): TRUE #> , , sex = 0 #>  #>       diabetes #> status 0 #>      0 2 #>      9 2 #>  #> , , sex = 1 #>  #>       diabetes #> status 0 #>      0 3 #>      9 2 #>  daggregate(iris, \"^.e.al\", x=\"Species\",regex=TRUE) #> Species: setosa #>   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    #>  Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100   #>  1st Qu.:4.800   1st Qu.:3.200   1st Qu.:1.400   1st Qu.:0.200   #>  Median :5.000   Median :3.400   Median :1.500   Median :0.200   #>  Mean   :5.006   Mean   :3.428   Mean   :1.462   Mean   :0.246   #>  3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300   #>  Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600   #> ------------------------------------------------------------  #> Species: versicolor #>   Sepal.Length    Sepal.Width     Petal.Length   Petal.Width    #>  Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000   #>  1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200   #>  Median :5.900   Median :2.800   Median :4.35   Median :1.300   #>  Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326   #>  3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500   #>  Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800   #> ------------------------------------------------------------  #> Species: virginica #>   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width    #>  Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400   #>  1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800   #>  Median :6.500   Median :3.000   Median :5.550   Median :2.000   #>  Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026   #>  3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300   #>  Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500   dlist(iris,Petal.Length+Sepal.Length ~ Species |Petal.Length>1.3 & Sepal.Length>5,             n=list(1:3,-(3:1))) #> Species: setosa #>     Petal.Length Sepal.Length #> 1   1.4          5.1          #> 6   1.7          5.4          #> 11  1.5          5.4          #> ---                           #> 45  1.9          5.1          #> 47  1.6          5.1          #> 49  1.5          5.3          #> ------------------------------------------------------------  #> Species: versicolor #>     Petal.Length Sepal.Length #> 51  4.7          7.0          #> 52  4.5          6.4          #> 53  4.9          6.9          #> ---                           #> 98  4.3          6.2          #> 99  3.0          5.1          #> 100 4.1          5.7          #> ------------------------------------------------------------  #> Species: virginica #>     Petal.Length Sepal.Length #> 101 6.0          6.3          #> 102 5.1          5.8          #> 103 5.9          7.1          #> ---                           #> 148 5.2          6.5          #> 149 5.4          6.2          #> 150 5.1          5.9          daggregate(iris, I(Sepal.Length>7)~Species | I(Petal.Length>1.5)) #> Species: setosa #>  I(Sepal.Length > 7) #>  Mode :logical       #>  FALSE:13            #> ------------------------------------------------------------  #> Species: versicolor #>  I(Sepal.Length > 7) #>  Mode :logical       #>  FALSE:50            #> ------------------------------------------------------------  #> Species: virginica #>  I(Sepal.Length > 7) #>  Mode :logical       #>  FALSE:38            #>  TRUE :12            daggregate(iris, I(Sepal.Length>7)~Species | I(Petal.Length>1.5),                  fun=table) #> Species: setosa #> I(Sepal.Length > 7) #> FALSE  #>    13  #> ------------------------------------------------------------  #> Species: versicolor #> I(Sepal.Length > 7) #> FALSE  #>    50  #> ------------------------------------------------------------  #> Species: virginica #> I(Sepal.Length > 7) #> FALSE  TRUE  #>    38    12   dsum(iris, .~Species, matrix=TRUE, missing=TRUE) #>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1     setosa        250.3       171.4         73.1        12.3 #> 2 versicolor        296.8       138.5        213.0        66.3 #> 3  virginica        329.4       148.7        277.6       101.3  par(mfrow=c(1,2)) data(iris) drename(iris) <- ~. daggregate(iris,'sepal*'~species|species!=\"virginica\",fun=plot)  #> species: setosa #> NULL #> ------------------------------------------------------------  #> species: versicolor #> NULL #> ------------------------------------------------------------  #> species: virginica #> NULL daggregate(iris,'sepal*'~I(as.numeric(species))|I(as.numeric(species))!=1,fun=summary) #> I(as.numeric(species)): 2 #>   sepal.length    sepal.width    #>  Min.   :4.900   Min.   :2.000   #>  1st Qu.:5.600   1st Qu.:2.525   #>  Median :5.900   Median :2.800   #>  Mean   :5.936   Mean   :2.770   #>  3rd Qu.:6.300   3rd Qu.:3.000   #>  Max.   :7.000   Max.   :3.400   #> ------------------------------------------------------------  #> I(as.numeric(species)): 3 #>   sepal.length    sepal.width    #>  Min.   :4.900   Min.   :2.200   #>  1st Qu.:6.225   1st Qu.:2.800   #>  Median :6.500   Median :3.000   #>  Mean   :6.588   Mean   :2.974   #>  3rd Qu.:6.900   3rd Qu.:3.175   #>  Max.   :7.900   Max.   :3.800    dnumeric(iris) <- ~species daggregate(iris,'sepal*'~species.n|species.n!=1,fun=summary) #> species.n: 2 #>   sepal.length    sepal.width    #>  Min.   :4.900   Min.   :2.000   #>  1st Qu.:5.600   1st Qu.:2.525   #>  Median :5.900   Median :2.800   #>  Mean   :5.936   Mean   :2.770   #>  3rd Qu.:6.300   3rd Qu.:3.000   #>  Max.   :7.000   Max.   :3.400   #> ------------------------------------------------------------  #> species.n: 3 #>   sepal.length    sepal.width    #>  Min.   :4.900   Min.   :2.200   #>  1st Qu.:6.225   1st Qu.:2.800   #>  Median :6.500   Median :3.000   #>  Mean   :6.588   Mean   :2.974   #>  3rd Qu.:6.900   3rd Qu.:3.175   #>  Max.   :7.900   Max.   :3.800"},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate summary statistics grouped by — dby","title":"Calculate summary statistics grouped by — dby","text":"Calculate summary statistics grouped variable","code":""},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate summary statistics grouped by — dby","text":"","code":"dby(   data,   INPUT,   ...,   ID = NULL,   ORDER = NULL,   SUBSET = NULL,   SORT = 0,   COMBINE = !REDUCE,   NOCHECK = FALSE,   ARGS = NULL,   NAMES,   COLUMN = FALSE,   REDUCE = FALSE,   REGEX = mets.options()$regex,   ALL = TRUE )"},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate summary statistics grouped by — dby","text":"data Data.frame INPUT Input variables (character formula) ... functions ID id variable ORDER (optional) order variable SUBSET (optional) subset expression SORT sort order (id+order variable) COMBINE TRUE result appended data NOCHECK sorting check missing data ARGS Optional list arguments functions (...) NAMES Optional vector column names COLUMN TRUE calculations column REDUCE Reduce number redundant rows REGEX Allow regular expressions FALSE subset returned","code":""},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate summary statistics grouped by — dby","text":"Calculate summary statistics grouped dby2 column-wise calculations","code":""},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate summary statistics grouped by — dby","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dby.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate summary statistics grouped by — dby","text":"","code":"n <- 4 k <- c(3,rbinom(n-1,3,0.5)+1) N <- sum(k) d <- data.frame(y=rnorm(N),x=rnorm(N),    id=rep(seq(n),k),num=unlist(sapply(k,seq)) ) d2 <- d[sample(nrow(d)),]  dby(d2, y~id, mean) #>            y          x id num       mean #> 3  2.6190994  0.7257837  1   3  0.3065174 #> 1 -1.2150233  0.3308853  1   1  0.3065174 #> 2 -0.4845239 -1.1121771  1   2  0.3065174 #> 5 -1.9064389  0.8263667  2   2 -0.3917604 #> 4 -0.3123679 -0.5940380  2   1 -0.3917604 #> 6  1.0435257 -1.2821343  2   3 -0.3917604 #> 7 -0.8944838 -0.7110099  3   1 -0.8944838 #> 9  1.2689582 -0.1237129  4   2  0.3647540 #> 8 -0.5394503 -1.9165302  4   1  0.3647540 dby(d2, y~id + order(num), cumsum) #>            y          x id num     cumsum #> 1 -1.2150233  0.3308853  1   1 -1.2150233 #> 2 -0.4845239 -1.1121771  1   2 -1.6995472 #> 3  2.6190994  0.7257837  1   3  0.9195522 #> 4 -0.3123679 -0.5940380  2   1 -0.3123679 #> 5 -1.9064389  0.8263667  2   2 -2.2188069 #> 6  1.0435257 -1.2821343  2   3 -1.1752811 #> 7 -0.8944838 -0.7110099  3   1 -0.8944838 #> 8 -0.5394503 -1.9165302  4   1 -0.5394503 #> 9  1.2689582 -0.1237129  4   2  0.7295079  dby(d,y ~ id + order(num), dlag) #>            y          x id num       dlag #> 1 -1.2150233  0.3308853  1   1         NA #> 2 -0.4845239 -1.1121771  1   2 -1.2150233 #> 3  2.6190994  0.7257837  1   3 -0.4845239 #> 4 -0.3123679 -0.5940380  2   1         NA #> 5 -1.9064389  0.8263667  2   2 -0.3123679 #> 6  1.0435257 -1.2821343  2   3 -1.9064389 #> 7 -0.8944838 -0.7110099  3   1         NA #> 8 -0.5394503 -1.9165302  4   1         NA #> 9  1.2689582 -0.1237129  4   2 -0.5394503 dby(d,y ~ id + order(num), dlag, ARGS=list(k=1:2)) #>            y          x id num      dlag1      dlag2 #> 1 -1.2150233  0.3308853  1   1         NA         NA #> 2 -0.4845239 -1.1121771  1   2 -1.2150233         NA #> 3  2.6190994  0.7257837  1   3 -0.4845239 -1.2150233 #> 4 -0.3123679 -0.5940380  2   1         NA         NA #> 5 -1.9064389  0.8263667  2   2 -0.3123679         NA #> 6  1.0435257 -1.2821343  2   3 -1.9064389 -0.3123679 #> 7 -0.8944838 -0.7110099  3   1         NA         NA #> 8 -0.5394503 -1.9165302  4   1         NA         NA #> 9  1.2689582 -0.1237129  4   2 -0.5394503         NA dby(d,y ~ id + order(num), dlag, ARGS=list(k=1:2), NAMES=c(\"l1\",\"l2\")) #>            y          x id num         l1         l2 #> 1 -1.2150233  0.3308853  1   1         NA         NA #> 2 -0.4845239 -1.1121771  1   2 -1.2150233         NA #> 3  2.6190994  0.7257837  1   3 -0.4845239 -1.2150233 #> 4 -0.3123679 -0.5940380  2   1         NA         NA #> 5 -1.9064389  0.8263667  2   2 -0.3123679         NA #> 6  1.0435257 -1.2821343  2   3 -1.9064389 -0.3123679 #> 7 -0.8944838 -0.7110099  3   1         NA         NA #> 8 -0.5394503 -1.9165302  4   1         NA         NA #> 9  1.2689582 -0.1237129  4   2 -0.5394503         NA  dby(d, y~id + order(num), mean=mean, csum=cumsum, n=length) #>            y          x id num       mean       csum n #> 1 -1.2150233  0.3308853  1   1  0.3065174 -1.2150233 3 #> 2 -0.4845239 -1.1121771  1   2  0.3065174 -1.6995472 3 #> 3  2.6190994  0.7257837  1   3  0.3065174  0.9195522 3 #> 4 -0.3123679 -0.5940380  2   1 -0.3917604 -0.3123679 3 #> 5 -1.9064389  0.8263667  2   2 -0.3917604 -2.2188069 3 #> 6  1.0435257 -1.2821343  2   3 -0.3917604 -1.1752811 3 #> 7 -0.8944838 -0.7110099  3   1 -0.8944838 -0.8944838 1 #> 8 -0.5394503 -1.9165302  4   1  0.3647540 -0.5394503 2 #> 9  1.2689582 -0.1237129  4   2  0.3647540  0.7295079 2 dby(d2, y~id + order(num), a=cumsum, b=mean, N=length,   l1=function(x) c(NA,x)[-length(x)] ) #>            y          x id num          a          b N         l1 #> 1 -1.2150233  0.3308853  1   1 -1.2150233  0.3065174 3         NA #> 2 -0.4845239 -1.1121771  1   2 -1.6995472  0.3065174 3 -1.2150233 #> 3  2.6190994  0.7257837  1   3  0.9195522  0.3065174 3  2.6190994 #> 4 -0.3123679 -0.5940380  2   1 -0.3123679 -0.3917604 3         NA #> 5 -1.9064389  0.8263667  2   2 -2.2188069 -0.3917604 3 -0.3123679 #> 6  1.0435257 -1.2821343  2   3 -1.1752811 -0.3917604 3  1.0435257 #> 7 -0.8944838 -0.7110099  3   1 -0.8944838 -0.8944838 1 -0.8944838 #> 8 -0.5394503 -1.9165302  4   1 -0.5394503  0.3647540 2         NA #> 9  1.2689582 -0.1237129  4   2  0.7295079  0.3647540 2  1.2689582  dby(d, y~id + order(num), nn=seq_along, n=length) #>            y          x id num nn n #> 1 -1.2150233  0.3308853  1   1  1 3 #> 2 -0.4845239 -1.1121771  1   2  2 3 #> 3  2.6190994  0.7257837  1   3  3 3 #> 4 -0.3123679 -0.5940380  2   1  1 3 #> 5 -1.9064389  0.8263667  2   2  2 3 #> 6  1.0435257 -1.2821343  2   3  3 3 #> 7 -0.8944838 -0.7110099  3   1  1 1 #> 8 -0.5394503 -1.9165302  4   1  1 2 #> 9  1.2689582 -0.1237129  4   2  2 2 dby(d, y~id + order(num), nn=seq_along, n=length) #>            y          x id num nn n #> 1 -1.2150233  0.3308853  1   1  1 3 #> 2 -0.4845239 -1.1121771  1   2  2 3 #> 3  2.6190994  0.7257837  1   3  3 3 #> 4 -0.3123679 -0.5940380  2   1  1 3 #> 5 -1.9064389  0.8263667  2   2  2 3 #> 6  1.0435257 -1.2821343  2   3  3 3 #> 7 -0.8944838 -0.7110099  3   1  1 1 #> 8 -0.5394503 -1.9165302  4   1  1 2 #> 9  1.2689582 -0.1237129  4   2  2 2  d <- d[,1:4] dby(d, x<0) <- list(z=mean) d <- dby(d, is.na(z), z=1)  f <- function(x) apply(x,1,min) dby(d, y+x~id, min=f) #> Error: object 'f' not found  dby(d,y+x~id+order(num), function(x) x) #>            y          x id num         z        _11        _12 #> 1 -1.2150233  0.3308853  1   1 1.0000000 -1.2150233  0.3308853 #> 2 -0.4845239 -1.1121771  1   2 0.8475856 -0.4845239 -1.1121771 #> 3  2.6190994  0.7257837  1   3 1.0000000  2.6190994  0.7257837 #> 4 -0.3123679 -0.5940380  2   1 0.8475856 -0.3123679 -0.5940380 #> 5 -1.9064389  0.8263667  2   2 1.0000000 -1.9064389  0.8263667 #> 6  1.0435257 -1.2821343  2   3 0.8475856  1.0435257 -1.2821343 #> 7 -0.8944838 -0.7110099  3   1 0.8475856 -0.8944838 -0.7110099 #> 8 -0.5394503 -1.9165302  4   1 0.8475856 -0.5394503 -1.9165302 #> 9  1.2689582 -0.1237129  4   2 0.8475856  1.2689582 -0.1237129  f <- function(x) { cbind(cumsum(x[,1]),cumsum(x[,2]))/sum(x)} dby(d, y+x~id, f) #> Error: object 'f' not found  ## column-wise a <- d dby2(a, mean, median, REGEX=TRUE) <- '^[y|x]'~id a #>            y          x id num         z     mean.y      mean.x   median.y #> 1 -1.2150233  0.3308853  1   1 1.0000000  0.3065174 -0.01850271 -0.4845239 #> 2 -0.4845239 -1.1121771  1   2 0.8475856  0.3065174 -0.01850271 -0.4845239 #> 3  2.6190994  0.7257837  1   3 1.0000000  0.3065174 -0.01850271 -0.4845239 #> 4 -0.3123679 -0.5940380  2   1 0.8475856 -0.3917604 -0.34993519 -0.3123679 #> 5 -1.9064389  0.8263667  2   2 1.0000000 -0.3917604 -0.34993519 -0.3123679 #> 6  1.0435257 -1.2821343  2   3 0.8475856 -0.3917604 -0.34993519 -0.3123679 #> 7 -0.8944838 -0.7110099  3   1 0.8475856 -0.8944838 -0.71100994 -0.8944838 #> 8 -0.5394503 -1.9165302  4   1 0.8475856  0.3647540 -1.02012154  0.3647540 #> 9  1.2689582 -0.1237129  4   2 0.8475856  0.3647540 -1.02012154  0.3647540 #>     median.x #> 1  0.3308853 #> 2  0.3308853 #> 3  0.3308853 #> 4 -0.5940380 #> 5 -0.5940380 #> 6 -0.5940380 #> 7 -0.7110099 #> 8 -1.0201215 #> 9 -1.0201215 ## wildcards  dby2(a,'y*'+'x*'~id,mean)  #>            y          x id num         z   median.y   median.x     mean.y #> 1 -1.2150233  0.3308853  1   1 1.0000000 -0.4845239  0.3308853  0.3065174 #> 2 -0.4845239 -1.1121771  1   2 0.8475856 -0.4845239  0.3308853  0.3065174 #> 3  2.6190994  0.7257837  1   3 1.0000000 -0.4845239  0.3308853  0.3065174 #> 4 -0.3123679 -0.5940380  2   1 0.8475856 -0.3123679 -0.5940380 -0.3917604 #> 5 -1.9064389  0.8263667  2   2 1.0000000 -0.3123679 -0.5940380 -0.3917604 #> 6  1.0435257 -1.2821343  2   3 0.8475856 -0.3123679 -0.5940380 -0.3917604 #> 7 -0.8944838 -0.7110099  3   1 0.8475856 -0.8944838 -0.7110099 -0.8944838 #> 8 -0.5394503 -1.9165302  4   1 0.8475856  0.3647540 -1.0201215  0.3647540 #> 9  1.2689582 -0.1237129  4   2 0.8475856  0.3647540 -1.0201215  0.3647540 #>        mean.x #> 1 -0.01850271 #> 2 -0.01850271 #> 3 -0.01850271 #> 4 -0.34993519 #> 5 -0.34993519 #> 6 -0.34993519 #> 7 -0.71100994 #> 8 -1.02012154 #> 9 -1.02012154   ## subset dby(d, x<0) <- list(z=NA) d #>            y          x id num  z #> 1 -1.2150233  0.3308853  1   1  1 #> 2 -0.4845239 -1.1121771  1   2 NA #> 3  2.6190994  0.7257837  1   3  1 #> 4 -0.3123679 -0.5940380  2   1 NA #> 5 -1.9064389  0.8263667  2   2  1 #> 6  1.0435257 -1.2821343  2   3 NA #> 7 -0.8944838 -0.7110099  3   1 NA #> 8 -0.5394503 -1.9165302  4   1 NA #> 9  1.2689582 -0.1237129  4   2 NA dby(d, y~id|x>-1, v=mean,z=1) #>            y          x id num          v  z #> 1 -1.2150233  0.3308853  1   1  0.7020380  1 #> 2 -0.4845239 -1.1121771  1   2         NA NA #> 3  2.6190994  0.7257837  1   3  0.7020380  1 #> 4 -0.3123679 -0.5940380  2   1 -1.1094034  1 #> 5 -1.9064389  0.8263667  2   2 -1.1094034  1 #> 6  1.0435257 -1.2821343  2   3         NA NA #> 7 -0.8944838 -0.7110099  3   1 -0.8944838  1 #> 8 -0.5394503 -1.9165302  4   1         NA NA #> 9  1.2689582 -0.1237129  4   2  1.2689582  1 dby(d, y+x~id|x>-1, mean, median, COLUMN=TRUE) #>            y          x id num  z     mean.y     mean.x   median.y   median.x #> 1 -1.2150233  0.3308853  1   1  1  0.7020380  0.5283345  0.7020380  0.5283345 #> 2 -0.4845239 -1.1121771  1   2 NA         NA         NA         NA         NA #> 3  2.6190994  0.7257837  1   3  1  0.7020380  0.5283345  0.7020380  0.5283345 #> 4 -0.3123679 -0.5940380  2   1 NA -1.1094034  0.1161644 -1.1094034  0.1161644 #> 5 -1.9064389  0.8263667  2   2  1 -1.1094034  0.1161644 -1.1094034  0.1161644 #> 6  1.0435257 -1.2821343  2   3 NA         NA         NA         NA         NA #> 7 -0.8944838 -0.7110099  3   1 NA -0.8944838 -0.7110099 -0.8944838 -0.7110099 #> 8 -0.5394503 -1.9165302  4   1 NA         NA         NA         NA         NA #> 9  1.2689582 -0.1237129  4   2 NA  1.2689582 -0.1237129  1.2689582 -0.1237129  dby2(d, y+x~id|x>0, mean, REDUCE=TRUE) #>   id    mean.y    mean.x #> 1  1  0.702038 0.5283345 #> 2  2 -1.906439 0.8263667  dby(d,y~id|x<0,mean,ALL=FALSE) #>            y          x id num  z       mean #> 2 -0.4845239 -1.1121771  1   2 NA -0.4845239 #> 4 -0.3123679 -0.5940380  2   1 NA  0.3655789 #> 6  1.0435257 -1.2821343  2   3 NA  0.3655789 #> 7 -0.8944838 -0.7110099  3   1 NA -0.8944838 #> 8 -0.5394503 -1.9165302  4   1 NA  0.3647540 #> 9  1.2689582 -0.1237129  4   2 NA  0.3647540  a <- iris a <- dby(a,y=1) dby(a,Species==\"versicolor\") <- list(y=2)"},{"path":"http://kkholst.github.io/mets/reference/dcor.html","id":null,"dir":"Reference","previous_headings":"","what":"summary, tables, and correlations for data frames — dcor","title":"summary, tables, and correlations for data frames — dcor","text":"summary, tables, correlations data frames","code":""},{"path":"http://kkholst.github.io/mets/reference/dcor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary, tables, and correlations for data frames — dcor","text":"","code":"dcor(data, y = NULL, x = NULL, use = \"pairwise.complete.obs\", ...)"},{"path":"http://kkholst.github.io/mets/reference/dcor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary, tables, and correlations for data frames — dcor","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. x possible group variable use handle missing values ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/dcor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"summary, tables, and correlations for data frames — dcor","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dcor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"summary, tables, and correlations for data frames — dcor","text":"","code":"data(\"sTRACE\",package=\"timereg\") dt<- sTRACE dt$time2 <- dt$time^2 dt$wmi2 <- dt$wmi^2 head(dt) #>         no wmi status chf    age sex diabetes     time vf     time2 wmi2 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0  1.809025 2.25 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 47.748100 3.61 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0  0.038416 0.64 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 56.896849 1.69 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 50.779876 2.56 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 20.544520 1.00  dcor(dt) #>                     no          wmi        status         chf         age #> no        1.0000000000 -0.009437654  0.0002042854  0.03176893 -0.08317491 #> wmi      -0.0094376537  1.000000000 -0.3103788196 -0.37464791 -0.19288208 #> status    0.0002042854 -0.310378820  1.0000000000  0.34237066  0.43051203 #> chf       0.0317689348 -0.374647914  0.3423706564  1.00000000  0.34197944 #> age      -0.0831749071 -0.192882080  0.4305120263  0.34197944  1.00000000 #> sex      -0.1090494653 -0.023542809 -0.0671941443 -0.13205952 -0.27356308 #> diabetes  0.0103460848 -0.138602457  0.0866091232  0.08871325  0.04685823 #> time      0.0185007942  0.328582293 -0.8145783614 -0.34820213 -0.41435287 #> vf        0.0618025722 -0.089668054  0.0473290244  0.13467109 -0.03983927 #> time2     0.0091527320  0.313368613 -0.8526160479 -0.34533562 -0.41370693 #> wmi2     -0.0081611472  0.987512109 -0.2992334471 -0.36641743 -0.18629143 #>                  sex    diabetes        time          vf        time2 #> no       -0.10904947  0.01034608  0.01850079  0.06180257  0.009152732 #> wmi      -0.02354281 -0.13860246  0.32858229 -0.08966805  0.313368613 #> status   -0.06719414  0.08660912 -0.81457836  0.04732902 -0.852616048 #> chf      -0.13205952  0.08871325 -0.34820213  0.13467109 -0.345335622 #> age      -0.27356308  0.04685823 -0.41435287 -0.03983927 -0.413706928 #> sex       1.00000000 -0.05523671  0.03630103 -0.04760863  0.037530286 #> diabetes -0.05523671  1.00000000 -0.09049143 -0.05060615 -0.087337711 #> time      0.03630103 -0.09049143  1.00000000 -0.10704592  0.976099537 #> vf       -0.04760863 -0.05060615 -0.10704592  1.00000000 -0.076612883 #> time2     0.03753029 -0.08733771  0.97609954 -0.07661288  1.000000000 #> wmi2     -0.01543475 -0.12474865  0.31427694 -0.09003700  0.302021849 #>                  wmi2 #> no       -0.008161147 #> wmi       0.987512109 #> status   -0.299233447 #> chf      -0.366417431 #> age      -0.186291429 #> sex      -0.015434752 #> diabetes -0.124748650 #> time      0.314276938 #> vf       -0.090037003 #> time2     0.302021849 #> wmi2      1.000000000  dcor(dt,~time+wmi) #>           time       wmi #> time 1.0000000 0.3285823 #> wmi  0.3285823 1.0000000 dcor(dt,~time+wmi,~vf+chf) #> vf: 0 #> chf: 0 #>           time       wmi #> time 1.0000000 0.2443018 #> wmi  0.2443018 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 0 #>           time       wmi #> time 1.0000000 0.8466897 #> wmi  0.8466897 1.0000000 #> ------------------------------------------------------------  #> vf: 0 #> chf: 1 #>           time       wmi #> time 1.0000000 0.1903859 #> wmi  0.1903859 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 1 #>          time      wmi #> time 1.000000 0.354452 #> wmi  0.354452 1.000000 dcor(dt,time+wmi~vf+chf) #> vf: 0 #> chf: 0 #>           time       wmi #> time 1.0000000 0.2443018 #> wmi  0.2443018 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 0 #>           time       wmi #> time 1.0000000 0.8466897 #> wmi  0.8466897 1.0000000 #> ------------------------------------------------------------  #> vf: 0 #> chf: 1 #>           time       wmi #> time 1.0000000 0.1903859 #> wmi  0.1903859 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 1 #>          time      wmi #> time 1.000000 0.354452 #> wmi  0.354452 1.000000  dcor(dt,c(\"time*\",\"wmi*\"),~vf+chf) #> vf: 0 #> chf: 0 #>            time     time2       wmi      wmi2 #> time  1.0000000 0.9744619 0.2443018 0.2286891 #> time2 0.9744619 1.0000000 0.2345873 0.2233235 #> wmi   0.2443018 0.2345873 1.0000000 0.9913077 #> wmi2  0.2286891 0.2233235 0.9913077 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 0 #>            time     time2       wmi      wmi2 #> time  1.0000000 0.9942844 0.8466897 0.8244614 #> time2 0.9942844 1.0000000 0.8793376 0.8612097 #> wmi   0.8466897 0.8793376 1.0000000 0.9968521 #> wmi2  0.8244614 0.8612097 0.9968521 1.0000000 #> ------------------------------------------------------------  #> vf: 0 #> chf: 1 #>            time     time2       wmi      wmi2 #> time  1.0000000 0.9731838 0.1903859 0.1826388 #> time2 0.9731838 1.0000000 0.1762251 0.1697576 #> wmi   0.1903859 0.1762251 1.0000000 0.9833844 #> wmi2  0.1826388 0.1697576 0.9833844 1.0000000 #> ------------------------------------------------------------  #> vf: 1 #> chf: 1 #>            time     time2       wmi      wmi2 #> time  1.0000000 0.9828671 0.3544520 0.3026710 #> time2 0.9828671 1.0000000 0.2827122 0.2298121 #> wmi   0.3544520 0.2827122 1.0000000 0.9842457 #> wmi2  0.3026710 0.2298121 0.9842457 1.0000000"},{"path":"http://kkholst.github.io/mets/reference/dcut.html","id":null,"dir":"Reference","previous_headings":"","what":"Cutting, sorting, rm (removing), rename for data frames — dcut","title":"Cutting, sorting, rm (removing), rename for data frames — dcut","text":"Cut variables, breaks given used, otherwise cuts using group size given probs, equispace groups range. Default equally sized groups possible","code":""},{"path":"http://kkholst.github.io/mets/reference/dcut.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cutting, sorting, rm (removing), rename for data frames — dcut","text":"","code":"dcut(   data,   y = NULL,   x = NULL,   breaks = 4,   probs = NULL,   equi = FALSE,   regex = mets.options()$regex,   sep = NULL,   na.rm = TRUE,   labels = NULL,   all = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/dcut.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cutting, sorting, rm (removing), rename for data frames — dcut","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. x name variable, fomula, names variables data frame. breaks number breaks, variables vector break points, probs groups defined quantiles equi equi-spaced breaks regex regular expressions. sep seperator naming cut names. na.rm remove NA grouping variables. labels use cut groups variables, even breaks unique ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/dcut.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cutting, sorting, rm (removing), rename for data frames — dcut","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dcut.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cutting, sorting, rm (removing), rename for data frames — dcut","text":"","code":"data(\"sTRACE\",package=\"timereg\") sTRACE$age2 <- sTRACE$age^2 sTRACE$age3 <- sTRACE$age^3  mm <- dcut(sTRACE,~age+wmi) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4 #> X1944 (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]  mm <- dcut(sTRACE,catage4+wmi4~age+wmi) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>         catage4      wmi4 #> X1944 (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]  mm <- dcut(sTRACE,~age+wmi,breaks=c(2,4)) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>          agecat.2  wmicat.4 #> X1944 (68.7,92.1] (1.4,2.7] #> X5783 (68.7,92.1] (1.4,2.7] #> X784  (68.7,92.1] [0.4,1.4] #> X3763 [28.1,68.7] [0.4,1.4] #> X2927 [28.1,68.7] (1.4,2.7] #> X4511 [28.1,68.7] [0.4,1.4]  mm <- dcut(sTRACE,c(\"age\",\"wmi\")) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4 #> X1944 (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]  mm <- dcut(sTRACE,~.) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>                   nocat.4  wmicat.4  agecat.4       timecat.4 #> X1944 (1.51e+03,3.44e+03] (1.4,1.8] (76,92.1] [0.000687,1.47] #> X5783 (5.18e+03,6.64e+03] (1.8,2.7] (68.7,76]     (6.81,8.16] #> X784        [22,1.51e+03] [0.4,1.1] (76,92.1] [0.000687,1.47] #> X3763 (3.44e+03,5.18e+03] (1.1,1.4] [28.1,60]     (6.81,8.16] #> X2927 (1.51e+03,3.44e+03] (1.4,1.8] (60,68.7]     (6.81,8.16] #> X4511 (3.44e+03,5.18e+03] [0.4,1.1] (60,68.7]     (1.47,6.05] #>                 age2cat.4           age3cat.4 #> X1944 (5.77e+03,8.48e+03] (4.39e+05,7.81e+05] #> X5783 (4.71e+03,5.77e+03] (3.24e+05,4.39e+05] #> X784  (5.77e+03,8.48e+03] (4.39e+05,7.81e+05] #> X3763       [788,3.6e+03] [2.21e+04,2.16e+05] #> X2927  (3.6e+03,4.71e+03] (2.16e+05,3.24e+05] #> X4511  (3.6e+03,4.71e+03] (2.16e+05,3.24e+05]  mm <- dcut(sTRACE,c(\"age\",\"wmi\"),breaks=c(2,4)) head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>          agecat.2  wmicat.4 #> X1944 (68.7,92.1] (1.4,2.7] #> X5783 (68.7,92.1] (1.4,2.7] #> X784  (68.7,92.1] [0.4,1.4] #> X3763 [28.1,68.7] [0.4,1.4] #> X2927 [28.1,68.7] (1.4,2.7] #> X4511 [28.1,68.7] [0.4,1.4]  gx <- dcut(sTRACE$age) head(gx) #> [1] (76,92.1] (68.7,76] (76,92.1] [28.1,60] (60,68.7] (60,68.7] #> Levels: [28.1,60] (60,68.7] (68.7,76] (76,92.1]   ## Removes all cuts variables with these names wildcards mm1 <- drm(mm,c(\"*.2\",\"*.4\")) head(mm1) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4  ## wildcards, for age, age2, age4 and wmi head(dcut(mm,c(\"a*\",\"?m*\"))) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>          agecat.2  wmicat.4  agecat.4           age2cat.4           age3cat.4 #> X1944 (68.7,92.1] (1.4,1.8] (76,92.1] (5.77e+03,8.48e+03] (4.39e+05,7.81e+05] #> X5783 (68.7,92.1] (1.8,2.7] (68.7,76] (4.71e+03,5.77e+03] (3.24e+05,4.39e+05] #> X784  (68.7,92.1] [0.4,1.1] (76,92.1] (5.77e+03,8.48e+03] (4.39e+05,7.81e+05] #> X3763 [28.1,68.7] (1.1,1.4] [28.1,60]       [788,3.6e+03] [2.21e+04,2.16e+05] #> X2927 [28.1,68.7] (1.4,1.8] (60,68.7]  (3.6e+03,4.71e+03] (2.16e+05,3.24e+05] #> X4511 [28.1,68.7] [0.4,1.1] (60,68.7]  (3.6e+03,4.71e+03] (2.16e+05,3.24e+05]  ## with direct asignment drm(mm) <- c(\"*.2\",\"*.4\") head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4  dcut(mm) <- c(\"age\",\"*m*\") dcut(mm) <- ageg1+wmig1~age+wmi head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1]  ############################ ## renaming ############################  head(mm) #>         no wmi status chf    age sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1] drename(mm, ~Age+Wmi) <- c(\"wmi\",\"age\") head(mm) #>         no Age status chf    Wmi sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1] mm1 <- mm  ## all names to lower drename(mm1) <- ~. head(mm1) #>         no age status chf    wmi sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1]  ## A* to lower mm2 <-  drename(mm,c(\"A*\",\"W*\")) head(mm2) #>         no age status chf    wmi sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1] drename(mm) <- \"A*\" head(mm) #>         no age status chf    Wmi sex diabetes     time vf     age2     age3 #> X1944 1944 1.5      9   0 84.924   1        1 1.345000  0 7212.086 612479.2 #> X5783 5783 1.9      0   1 74.193   0        0 6.910000  0 5504.601 408402.9 #> X784   784 0.8      9   0 78.081   0        1 0.196000  0 6096.643 476031.9 #> X3763 3763 1.3      0   0 55.479   1        0 7.543000  0 3077.919 170759.9 #> X2927 2927 1.6      0   1 62.997   0        0 7.126000  0 3968.622 250011.3 #> X4511 4511 1.0      9   1 67.644   1        0 4.532606  0 4575.711 309519.4 #>        agecat.4  wmicat.4       timecat.4     ageg1     wmig1 #> X1944 (76,92.1] (1.4,1.8] [0.000687,1.47] (76,92.1] (1.4,1.8] #> X5783 (68.7,76] (1.8,2.7]     (6.81,8.16] (68.7,76] (1.8,2.7] #> X784  (76,92.1] [0.4,1.1] [0.000687,1.47] (76,92.1] [0.4,1.1] #> X3763 [28.1,60] (1.1,1.4]     (6.81,8.16] [28.1,60] (1.1,1.4] #> X2927 (60,68.7] (1.4,1.8]     (6.81,8.16] (60,68.7] (1.4,1.8] #> X4511 (60,68.7] [0.4,1.1]     (1.47,6.05] (60,68.7] [0.4,1.1]  dd <- data.frame(A_1=1:2,B_1=1:2) funn <- function(x) gsub(\"_\",\".\",x) drename(dd) <- ~. drename(dd,fun=funn) <- ~. names(dd) #> [1] \"a.1\" \"b.1\""},{"path":"http://kkholst.github.io/mets/reference/dermalridges.html","id":null,"dir":"Reference","previous_headings":"","what":"Dermal ridges data (families) — dermalridges","title":"Dermal ridges data (families) — dermalridges","text":"Data dermal ridge counts left right hand (nuclear) families","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridges.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dermal ridges data (families) — dermalridges","text":"Data 50 families ridge counts left right hand moter, father child. Family id 'family' gender child number 'sex' 'child'.","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridges.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Dermal ridges data (families) — dermalridges","text":"Sarah B. Holt (1952). Genetics dermal ridges: bilateral asymmetry finger ridge-counts.  Annals Eugenics 17 (1), pp.211–231. DOI: 10.1111/j.1469-1809.1952.tb02513.x","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridges.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dermal ridges data (families) — dermalridges","text":"","code":"data(dermalridges) fast.reshape(dermalridges,id=\"family\",varying=c(\"child.left\",\"child.right\",\"sex\")) #>     father.left father.right mother.left mother.right   sex1 child.left1 #> 1            51           63           5           18   male          66 #> 2             7            8          78           82 female          30 #> 3            30           35          95           91   male          54 #> 4            39           17          68           67   male          74 #> 5            76           78          72           76 female          69 #> 6           105          108          97          104   male          88 #> 7            35           42          64           91   male          54 #> 8           102           97          34           36 female          71 #> 9            93          113          62           47 female          67 #> 10           76           78          64           71 female          75 #> 11           61           70          13           15   male          76 #> 12           91           98          74           83 female          84 #> 13           69           84          73           71   male          61 #> 14           59           57          87          110   male          94 #> 15           84           81          84           86 female          78 #> 16           77           82          30           30 female          72 #> 17           94          113          49           60   male          82 #> 18           42           58          61           81   male          45 #> 20           65           43          96           92 female          69 #> 22           25           40          67           53 female          44 #> 24           70           76          50           60   male          71 #> 26           93           95          29           31 female          72 #> 28           77           88          79           83   male          53 #> 30          114          112          94           87 female         103 #> 32           32           23          20           33 female          66 #> 34           61           37          59           62   male          64 #> 36           51           47          84           73   male          41 #> 38            6           23          67           72   male          38 #> 40           61           75          79           83   male          95 #> 42          113          116          91           91   male         106 #> 44           46           39          82           93   male          80 #> 46           82           90          17           25   male          52 #> 48           61           75          59           72 female          59 #> 50           69           68          46           69 female          33 #> 52           95          100          43           44   male         100 #> 54           93           96          46           57   male          76 #> 56           20           26          53           50 female          42 #> 59           28           38          37           37 female          12 #> 62           49           59          62           73 female          31 #> 65           32           34          15           20   male          35 #> 68           57           69          92           86   male          93 #> 71           95          106         115          115 female         116 #> 74           44           57          79           77   male          68 #> 77           52           50          77           98   male          76 #> 80           46           67          38           50   male          81 #> 83           87           85          11            7 female          19 #> 87          110          114          97           84 female         103 #> 91           90           98          20           24   male          87 #> 96           90           87          68           64   male         106 #> 101          66           62          15           35 female          42 #>     child.right1 family child note child.left2 child.right2   sex2 child.left3 #> 1             67      1     1 <NA>          NA           NA   <NA>          NA #> 2             26      2     1 <NA>          NA           NA   <NA>          NA #> 3             74      3     1 <NA>          NA           NA   <NA>          NA #> 4             77      4     1 <NA>          NA           NA   <NA>          NA #> 5             62      5     1 <NA>          NA           NA   <NA>          NA #> 6            101      6     1 <NA>          NA           NA   <NA>          NA #> 7             59      7     1 <NA>          NA           NA   <NA>          NA #> 8             78      8     1 <NA>          NA           NA   <NA>          NA #> 9             59      9     1 <NA>          NA           NA   <NA>          NA #> 10            76     10     1 <NA>          NA           NA   <NA>          NA #> 11            89     11     1 <NA>          NA           NA   <NA>          NA #> 12            97     12     1 <NA>          NA           NA   <NA>          NA #> 13            75     13     1 <NA>          NA           NA   <NA>          NA #> 14            88     14     1 <NA>          NA           NA   <NA>          NA #> 15            69     15     1 <NA>          NA           NA   <NA>          NA #> 16            79     16     1 <NA>          NA           NA   <NA>          NA #> 17            89     17     1 <NA>          NA           NA   <NA>          NA #> 18            44     18     1 <NA>          49           53 female          NA #> 20            69     19     1 <NA>          75           76   male          NA #> 22            45     20     1 <NA>          68           77 female          NA #> 24            77     21     1 <NA>          41           54   male          NA #> 26            75     22     1 <NA>          62           63 female          NA #> 28            66     23     1 <NA>          87           83   male          NA #> 30           101     24     1 <NA>         100           93 female          NA #> 32            63     25     1 <NA>          40           26 female          NA #> 34            81     26     1   dz          66           65   male          NA #> 36            37     27     1 <NA>          61           57 female          NA #> 38            39     28     1 <NA>          12           12   male          NA #> 40            88     29     1 <NA>          62           57   male          NA #> 42           106     30     1 <NA>          97           90   male          NA #> 44            81     31     1 <NA>          82           82 female          NA #> 46            51     32     1 <NA>          53           73   male          NA #> 48            78     33     1 <NA>          47           48 female          NA #> 50            43     34     1 <NA>          81           85   male          NA #> 52           108     35     1 <NA>          66           45   male          NA #> 54            80     36     1 <NA>          72           71 female          NA #> 56            39     37     1 <NA>          28           34 female          49 #> 59            12     38     1 <NA>           5            4 female          17 #> 62            51     39     1 <NA>          46           50 female          58 #> 65            44     40     1 <NA>          26           38   male          16 #> 68           100     41     1 <NA>          99          108 female          53 #> 71           124     42     1 <NA>          89          101 female          99 #> 74            74     43     1 <NA>          57           67 female          52 #> 77            97     44     1 <NA>          89           77   male          64 #> 80            65     45     1 <NA>          59           42   male          57 #> 83            45     46     1 <NA>          76           75   male           4 #> 87           104     47     1 <NA>          99          104 female          99 #> 91            90     48     1 <NA>          43           50 female          15 #> 96            93     49     1 <NA>          96          103   male          76 #> 101           65     50     1 <NA>           0            0 female           0 #>     child.right3   sex3 child.left4 child.right4   sex4 child.left5 #> 1             NA   <NA>          NA           NA   <NA>          NA #> 2             NA   <NA>          NA           NA   <NA>          NA #> 3             NA   <NA>          NA           NA   <NA>          NA #> 4             NA   <NA>          NA           NA   <NA>          NA #> 5             NA   <NA>          NA           NA   <NA>          NA #> 6             NA   <NA>          NA           NA   <NA>          NA #> 7             NA   <NA>          NA           NA   <NA>          NA #> 8             NA   <NA>          NA           NA   <NA>          NA #> 9             NA   <NA>          NA           NA   <NA>          NA #> 10            NA   <NA>          NA           NA   <NA>          NA #> 11            NA   <NA>          NA           NA   <NA>          NA #> 12            NA   <NA>          NA           NA   <NA>          NA #> 13            NA   <NA>          NA           NA   <NA>          NA #> 14            NA   <NA>          NA           NA   <NA>          NA #> 15            NA   <NA>          NA           NA   <NA>          NA #> 16            NA   <NA>          NA           NA   <NA>          NA #> 17            NA   <NA>          NA           NA   <NA>          NA #> 18            NA   <NA>          NA           NA   <NA>          NA #> 20            NA   <NA>          NA           NA   <NA>          NA #> 22            NA   <NA>          NA           NA   <NA>          NA #> 24            NA   <NA>          NA           NA   <NA>          NA #> 26            NA   <NA>          NA           NA   <NA>          NA #> 28            NA   <NA>          NA           NA   <NA>          NA #> 30            NA   <NA>          NA           NA   <NA>          NA #> 32            NA   <NA>          NA           NA   <NA>          NA #> 34            NA   <NA>          NA           NA   <NA>          NA #> 36            NA   <NA>          NA           NA   <NA>          NA #> 38            NA   <NA>          NA           NA   <NA>          NA #> 40            NA   <NA>          NA           NA   <NA>          NA #> 42            NA   <NA>          NA           NA   <NA>          NA #> 44            NA   <NA>          NA           NA   <NA>          NA #> 46            NA   <NA>          NA           NA   <NA>          NA #> 48            NA   <NA>          NA           NA   <NA>          NA #> 50            NA   <NA>          NA           NA   <NA>          NA #> 52            NA   <NA>          NA           NA   <NA>          NA #> 54            NA   <NA>          NA           NA   <NA>          NA #> 56            40   male          NA           NA   <NA>          NA #> 59            11   male          NA           NA   <NA>          NA #> 62            74 female          NA           NA   <NA>          NA #> 65            18   male          NA           NA   <NA>          NA #> 68            79 female          NA           NA   <NA>          NA #> 71            98   male          NA           NA   <NA>          NA #> 74            54 female          NA           NA   <NA>          NA #> 77            72 female          NA           NA   <NA>          NA #> 80            50   male          NA           NA   <NA>          NA #> 83             0 female          35           31 female          NA #> 87           102 female         101           93   male          NA #> 91             8   male          54           42 female          36 #> 96            82   male          83           85   male          76 #> 101            8 female          76           77 female          30 #>     child.right5   sex5 child.left6 child.right6   sex6 #> 1             NA   <NA>          NA           NA   <NA> #> 2             NA   <NA>          NA           NA   <NA> #> 3             NA   <NA>          NA           NA   <NA> #> 4             NA   <NA>          NA           NA   <NA> #> 5             NA   <NA>          NA           NA   <NA> #> 6             NA   <NA>          NA           NA   <NA> #> 7             NA   <NA>          NA           NA   <NA> #> 8             NA   <NA>          NA           NA   <NA> #> 9             NA   <NA>          NA           NA   <NA> #> 10            NA   <NA>          NA           NA   <NA> #> 11            NA   <NA>          NA           NA   <NA> #> 12            NA   <NA>          NA           NA   <NA> #> 13            NA   <NA>          NA           NA   <NA> #> 14            NA   <NA>          NA           NA   <NA> #> 15            NA   <NA>          NA           NA   <NA> #> 16            NA   <NA>          NA           NA   <NA> #> 17            NA   <NA>          NA           NA   <NA> #> 18            NA   <NA>          NA           NA   <NA> #> 20            NA   <NA>          NA           NA   <NA> #> 22            NA   <NA>          NA           NA   <NA> #> 24            NA   <NA>          NA           NA   <NA> #> 26            NA   <NA>          NA           NA   <NA> #> 28            NA   <NA>          NA           NA   <NA> #> 30            NA   <NA>          NA           NA   <NA> #> 32            NA   <NA>          NA           NA   <NA> #> 34            NA   <NA>          NA           NA   <NA> #> 36            NA   <NA>          NA           NA   <NA> #> 38            NA   <NA>          NA           NA   <NA> #> 40            NA   <NA>          NA           NA   <NA> #> 42            NA   <NA>          NA           NA   <NA> #> 44            NA   <NA>          NA           NA   <NA> #> 46            NA   <NA>          NA           NA   <NA> #> 48            NA   <NA>          NA           NA   <NA> #> 50            NA   <NA>          NA           NA   <NA> #> 52            NA   <NA>          NA           NA   <NA> #> 54            NA   <NA>          NA           NA   <NA> #> 56            NA   <NA>          NA           NA   <NA> #> 59            NA   <NA>          NA           NA   <NA> #> 62            NA   <NA>          NA           NA   <NA> #> 65            NA   <NA>          NA           NA   <NA> #> 68            NA   <NA>          NA           NA   <NA> #> 71            NA   <NA>          NA           NA   <NA> #> 74            NA   <NA>          NA           NA   <NA> #> 77            NA   <NA>          NA           NA   <NA> #> 80            NA   <NA>          NA           NA   <NA> #> 83            NA   <NA>          NA           NA   <NA> #> 87            NA   <NA>          NA           NA   <NA> #> 91            32   male          NA           NA   <NA> #> 96            74 female          NA           NA   <NA> #> 101           26 female          59           61 female"},{"path":"http://kkholst.github.io/mets/reference/dermalridgesMZ.html","id":null,"dir":"Reference","previous_headings":"","what":"Dermal ridges data (monozygotic twins) — dermalridgesMZ","title":"Dermal ridges data (monozygotic twins) — dermalridgesMZ","text":"Data dermal ridge counts left right hand (nuclear) families","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridgesMZ.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dermal ridges data (monozygotic twins) — dermalridgesMZ","text":"Data dermal ridge counts (left right hand) 18 monozygotic twin pairs.","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridgesMZ.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Dermal ridges data (monozygotic twins) — dermalridgesMZ","text":"Sarah B. Holt (1952). Genetics dermal ridges: bilateral asymmetry finger ridge-counts.  Annals Eugenics 17 (1), pp.211–231. DOI: 10.1111/j.1469-1809.1952.tb02513.x","code":""},{"path":"http://kkholst.github.io/mets/reference/dermalridgesMZ.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dermal ridges data (monozygotic twins) — dermalridgesMZ","text":"","code":"data(dermalridgesMZ) fast.reshape(dermalridgesMZ,id=\"id\",varying=c(\"left\",\"right\")) #>       sex left1 right1              group id left2 right2 #> 1  female    95     83 psychotic&neurotic  1    90     85 #> 3  female    93     90 psychotic&neurotic  2    80     80 #> 5    male    99     94 psychotic&neurotic  3    94     99 #> 7  female    55     61 psychotic&neurotic  4    48     54 #> 9  female    41     24 psychotic&neurotic  5    26     42 #> 11   male    39     38 psychotic&neurotic  6    49     38 #> 13 female    69     81 psychotic&neurotic  7    81     71 #> 15   male    78     72 psychotic&neurotic  8    77     77 #> 17   male    64     64 psychotic&neurotic  9    69     55 #> 19   male    74     83 psychotic&neurotic 10    78     87 #> 21 female    37     34 psychotic&neurotic 11    27     46 #> 23 female    39     45 psychotic&neurotic 12    45     53 #> 25 female    62     62             normal 13    69     66 #> 27   male    92     95             normal 14    90     89 #> 29 female    97     90             normal 15    92     99 #> 31 female    28     44             normal 16    44     48 #> 33 female    53     59             normal 17    51     53 #> 35   male    62     60             normal 18    56     51"},{"path":"http://kkholst.github.io/mets/reference/diabetes.html","id":null,"dir":"Reference","previous_headings":"","what":"The Diabetic Retinopathy Data — diabetes","title":"The Diabetic Retinopathy Data — diabetes","text":"data colleceted test laser treatment delaying blindness patients dibetic retinopathy. subset 197 patiens given Huster et al. (1989) used.","code":""},{"path":"http://kkholst.github.io/mets/reference/diabetes.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The Diabetic Retinopathy Data — diabetes","text":"data frame contains following columns: id numeric vector. Patient code. agedx numeric vector. Age patient diagnosis. time numeric vector. Survival time: time blindness censoring. status numeric vector code. Survival status. 1: blindness, 0: censored. trteye numeric vector code. Random eye selected treatment. 1: left eye 2: right eye. treat numeric vector. 1: treatment 0: untreated. adult numeric vector code. 1: younger 20, 2: older 20.","code":""},{"path":"http://kkholst.github.io/mets/reference/diabetes.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The Diabetic Retinopathy Data — diabetes","text":"Huster W.J. Brookmeyer, R. Self. S. (1989) MOdelling paired survival data covariates, Biometrics 45, 145-56.","code":""},{"path":"http://kkholst.github.io/mets/reference/diabetes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Diabetic Retinopathy Data — diabetes","text":"","code":"data(diabetes) names(diabetes) #> [1] \"id\"     \"time\"   \"status\" \"trteye\" \"treat\"  \"adult\"  \"agedx\""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a data set and run function — divide.conquer","title":"Split a data set and run function — divide.conquer","text":"Split data set run function","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a data set and run function — divide.conquer","text":"","code":"divide.conquer(func = NULL, data, size, splits, id = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a data set and run function — divide.conquer","text":"func called function data data-frame size size splits splits number splits (ignored size given) id optional cluster variable ... Additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Split a data set and run function — divide.conquer","text":"Thomas Scheike, Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split a data set and run function — divide.conquer","text":"","code":"## avoid dependency on timereg ## library(timereg) ## data(TRACE) ## res <- divide.conquer(prop.odds,TRACE, ##        formula=Event(time,status==9)~chf+vf+age,n.sim=0,size=200)"},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.timereg.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","title":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","text":"Split data set run function cox-aalen type aggregate results","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.timereg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","text":"","code":"divide.conquer.timereg(func = NULL, data, size, ...)"},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.timereg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","text":"func called function data data-frame size size splits ... Additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.timereg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","text":"Thomas Scheike, Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/divide.conquer.timereg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split a data set and run function from timereg and aggregate — divide.conquer.timereg","text":"","code":"## library(timereg) ## data(TRACE) ## a <- divide.conquer.timereg(prop.odds,TRACE, ##                            formula=Event(time,status==9)~chf+vf+age,n.sim=0,size=200) ## coef(a) ## a2 <- divide.conquer.timereg(prop.odds,TRACE, ##                              formula=Event(time,status==9)~chf+vf+age,n.sim=0,size=500) ## coef(a2) ##  ##if (interactive()) { ##par(mfrow=c(1,1)) ##plot(a,xlim=c(0,8),ylim=c(0,0.01)) ##par(new=TRUE) ##plot(a2,xlim=c(0,8),ylim=c(0,0.01)) ##}"},{"path":"http://kkholst.github.io/mets/reference/dlag.html","id":null,"dir":"Reference","previous_headings":"","what":"Lag operator — dlag","title":"Lag operator — dlag","text":"Lag operator","code":""},{"path":"http://kkholst.github.io/mets/reference/dlag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lag operator — dlag","text":"","code":"dlag(data, x, k = 1, combine = TRUE, simplify = TRUE, names, ...)"},{"path":"http://kkholst.github.io/mets/reference/dlag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lag operator — dlag","text":"data data.frame vector x optional column names formula k lag (vector integers) combine combine results original data.frame simplify Return vector possible names optional new column names ... additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/dlag.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lag operator — dlag","text":"","code":"d <- data.frame(y=1:10,x=c(10:1)) dlag(d,k=1:2) #>     y  x y.1 y.2 x.1 x.2 #> 1   1 10  NA  NA  NA  NA #> 2   2  9   1  NA  10  NA #> 3   3  8   2   1   9  10 #> 4   4  7   3   2   8   9 #> 5   5  6   4   3   7   8 #> 6   6  5   5   4   6   7 #> 7   7  4   6   5   5   6 #> 8   8  3   7   6   4   5 #> 9   9  2   8   7   3   4 #> 10 10  1   9   8   2   3 dlag(d,~x,k=0:1) #>     y  x x.0 x.1 #> 1   1 10  10  NA #> 2   2  9   9  10 #> 3   3  8   8   9 #> 4   4  7   7   8 #> 5   5  6   6   7 #> 6   6  5   5   6 #> 7   7  4   4   5 #> 8   8  3   3   4 #> 9   9  2   2   3 #> 10 10  1   1   2 dlag(d$x,k=1) #>  [1] NA 10  9  8  7  6  5  4  3  2 dlag(d$x,k=-1:2, names=letters[1:4]) #>        a  b  c  d #>  [1,]  9 10 NA NA #>  [2,]  8  9 10 NA #>  [3,]  7  8  9 10 #>  [4,]  6  7  8  9 #>  [5,]  5  6  7  8 #>  [6,]  4  5  6  7 #>  [7,]  3  4  5  6 #>  [8,]  2  3  4  5 #>  [9,]  1  2  3  4 #> [10,] NA  1  2  3"},{"path":"http://kkholst.github.io/mets/reference/doubleFGR.html","id":null,"dir":"Reference","previous_headings":"","what":"Double CIF Fine-Gray model with two causes — doubleFGR","title":"Double CIF Fine-Gray model with two causes — doubleFGR","text":"Estimation based derived hazards recursive estimating equations. fits two parametrizations 1)  $$ F_1(t,X) = 1 - \\exp( - \\exp( X^T \\beta ) \\Lambda_1(t)) $$  $$ F_2(t,X_2) = 1 - \\exp( -  \\exp( X_2^T \\beta_2 ) \\Lambda_2(t)) $$ restricted version 2)  $$ F_1(t,X) = 1 - \\exp( -  \\exp( X^T \\beta ) \\Lambda_1(t)) $$  $$ F_2(t,X_2,X) = ( 1 - \\exp(  - \\exp( X_2^T \\beta_2 ) \\Lambda_2(t)) ) (1 - F_1(\\infty,X)) $$","code":""},{"path":"http://kkholst.github.io/mets/reference/doubleFGR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Double CIF Fine-Gray model with two causes — doubleFGR","text":"","code":"doubleFGR(formula, data, offset = NULL, weights = NULL, X2 = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/doubleFGR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Double CIF Fine-Gray model with two causes — doubleFGR","text":"formula formula 'Event' data data frame offset offsets cox model weights weights Cox score equations X2 specifies regression design second CIF model ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/doubleFGR.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Double CIF Fine-Gray model with two causes — doubleFGR","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/doubleFGR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Double CIF Fine-Gray model with two causes — doubleFGR","text":"","code":"library(mets) res <- 0 data(bmt) bmt$age2 <- bmt$age newdata <- bmt[1:19,] ## if (interactive()) par(mfrow=c(5,3))  ## same X1 and X2 pr2 <- doubleFGR(Event(time,cause)~age+platelet,data=bmt,restrict=res) ##if (interactive()) { ##  bplotdFG(pr2,cause=1) ##  bplotdFG(pr2,cause=2,add=TRUE) ##} ##pp21 <- predictdFG(pr2,newdata=newdata) ##pp22 <- predictdFG(pr2,newdata=newdata,cause=2) ##if (interactive()) { ##  plot(pp21) ##  plot(pp22,add=TRUE,col=2) ##} ##pp21 <- predictdFG(pr2) ##pp22 <- predictdFG(pr2,cause=2) ##if (interactive()) {  ## plot(pp21)  ## plot(pp22,add=TRUE,col=2) ##}  pr2 <- doubleFGR(Event(time,cause)~strata(platelet),data=bmt,restrict=res)  ## different X1 and X2 pr2 <- doubleFGR(Event(time,cause)~age+platelet+age2,data=bmt,X2=3,restrict=res)  ### uden X1 pr2 <- doubleFGR(Event(time,cause)~age+platelet,data=bmt,X2=1:2,restrict=res)  ### without X2 pr2 <- doubleFGR(Event(time,cause)~age+platelet,data=bmt,X2=0,restrict=res)"},{"path":"http://kkholst.github.io/mets/reference/dprint.html","id":null,"dir":"Reference","previous_headings":"","what":"list, head, print, tail — dprint","title":"list, head, print, tail — dprint","text":"listing data frames","code":""},{"path":"http://kkholst.github.io/mets/reference/dprint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"list, head, print, tail — dprint","text":"","code":"dprint(data, y = NULL, n = 0, ..., x = NULL)"},{"path":"http://kkholst.github.io/mets/reference/dprint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"list, head, print, tail — dprint","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. n Index observations print (default c(1:nfirst, n-nlast:nlast) ... Optional additional arguments (nfirst,nlast, print options) x possible group variable","code":""},{"path":"http://kkholst.github.io/mets/reference/dprint.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"list, head, print, tail — dprint","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dprint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"list, head, print, tail — dprint","text":"","code":"m <- lava::lvm(letters) d <- lava::sim(m, 20)  dlist(d,~a+b+c) #>     a        b       c         #> 1   -0.57713  1.3864  0.2035   #> 2   -0.80592 -1.3047  0.8770   #> 3    1.22148  0.8083 -0.8739   #> 4    0.03472  0.1451 -0.2107   #> 5   -2.55785  0.8355  0.5970   #> ---                            #> 16   1.0592   0.1010 -0.004013 #> 17   1.0293  -0.1649  0.582503 #> 18  -0.3133  -0.7004  0.170292 #> 19  -0.9958   1.8680  0.055593 #> 20  -0.6279  -0.5425  1.141641 dlist(d,~a+b+c|a<0 & b>0) #>    a       b       c        #> 1  -0.5771 1.38640  0.20349 #> 5  -2.5578 0.83551  0.59703 #> 7  -1.4403 1.84722 -1.36392 #> 12 -0.6864 0.34647  1.12117 #> 13 -0.4690 0.03698  0.21409 #> 14 -0.8690 0.24112  0.85016 #> 19 -0.9958 1.86798  0.05559 ## listing all :  dlist(d,~a+b+c|a<0 & b>0,n=0) #>             a          b           c #> 1  -0.5771254 1.38639858  0.20349331 #> 5  -2.5578479 0.83551163  0.59702674 #> 7  -1.4402667 1.84721571 -1.36391844 #> 12 -0.6863567 0.34647417  1.12117388 #> 13 -0.4689727 0.03698312  0.21408719 #> 14 -0.8689645 0.24111853  0.85016454 #> 19 -0.9958388 1.86798026  0.05559288 dlist(d,a+b+c~I(d>0)|a<0 & b>0) #> I(d > 0): FALSE #>   a     b     c      #> 7 -1.44 1.847 -1.364 #> ------------------------------------------------------------  #> I(d > 0): TRUE #>    a       b       c       #> 1  -0.5771 1.38640 0.20349 #> 5  -2.5578 0.83551 0.59703 #> 12 -0.6864 0.34647 1.12117 #> 13 -0.4690 0.03698 0.21409 #> 14 -0.8690 0.24112 0.85016 #> 19 -0.9958 1.86798 0.05559 dlist(d,.~I(d>0)|a<0 & b>0) #> I(d > 0): FALSE #>   a     b     c      d       e      f      g      h       i      j      k     #> 7 -1.44 1.847 -1.364 -0.5632 -1.316 0.3615 0.5216 -0.1331 0.4985 -1.102 1.115 #>   l      m      n      o      p      q     r      s       t       u      v      #> 7 0.7383 -1.494 0.1968 0.3385 0.4008 0.491 0.6065 -0.3661 -0.4159 -1.657 -2.589 #>   w      x       y     z     #> 7 -1.496 -0.5205 -1.21 1.693 #> ------------------------------------------------------------  #> I(d > 0): TRUE #>    a       b       c       d      e        f       g         h       i        #> 1  -0.5771 1.38640 0.20349 1.4098  0.04847 -1.9705  0.658111  0.4043 -0.52642 #> 5  -2.5578 0.83551 0.59703 0.3855  0.51960 -0.1985  0.821824 -0.8275  1.08230 #> 12 -0.6864 0.34647 1.12117 0.5538  1.96098  1.5414  0.194344  0.1269 -0.04678 #> 13 -0.4690 0.03698 0.21409 0.1177 -0.46321  0.9789  0.936691 -0.3200 -0.94240 #> 14 -0.8690 0.24112 0.85016 0.9213 -0.86465 -0.3636 -0.253180  0.5867 -0.16376 #> 19 -0.9958 1.86798 0.05559 0.9732  0.44719 -1.1159  0.001865  0.2458  0.09711 #>    j       k        l        m       n         o        p       q       #> 1   0.3942  1.38818  0.17237 -0.5659 -0.007628  0.04385 -0.5305 -0.4197 #> 5   0.1518 -0.07623 -0.36970  1.1151 -0.299895 -0.57605  0.5141 -0.3771 #> 12 -1.2743 -0.05771 -0.68983 -1.2597  0.613626 -0.33646  0.8328 -0.9099 #> 13  0.7546  0.14783  0.04554 -0.4604  0.017346  0.87833  1.4594  1.3654 #> 14 -0.4069 -0.14879  0.82880 -1.3357 -0.700748  1.24798  1.5513 -0.5203 #> 19  0.5729 -0.45289 -0.34918 -0.1733 -0.341316 -0.20052  0.6914 -1.6135 #>    r         s       t       u         v       w        x       y        #> 1   1.231414  1.1565 -0.6686 -0.699676  0.6825  0.53973 2.29360  0.97959 #> 5  -1.090981 -0.7229  1.5300 -1.217620 -0.7953 -0.30082 0.78596  0.60130 #> 12 -0.237471 -1.4424 -0.3349  0.918788  0.2956 -0.19642 0.33206 -0.29644 #> 13 -0.009664 -0.5202  0.9787  0.338243  0.1452  0.45054 0.47294  0.03932 #> 14  1.532874 -0.4943  0.6186 -0.008522 -0.2978  1.73364 0.39648  2.58308 #> 19  2.351449  0.6613  0.7215  1.700028 -0.4123  0.02187 0.06938 -0.73137 #>    z        #> 1   0.65615 #> 5  -0.02632 #> 12 -0.13111 #> 13 -0.99994 #> 14 -0.06770 #> 19  1.57627 dlist(d,~a+b+c|a<0 & b>0, nlast=0) #>    a       b       c       #> 1  -0.5771 1.38640  0.2035 #> 5  -2.5578 0.83551  0.5970 #> 7  -1.4403 1.84722 -1.3639 #> 12 -0.6864 0.34647  1.1212 #> 13 -0.4690 0.03698  0.2141 dlist(d,~a+b+c|a<0 & b>0, nfirst=3, nlast=3) #>     a       b       c       #> 1   -0.5771 1.3864   0.2035 #> 5   -2.5578 0.8355   0.5970 #> 7   -1.4403 1.8472  -1.3639 #> ---                         #> 13  -0.4690 0.03698 0.21409 #> 14  -0.8690 0.24112 0.85016 #> 19  -0.9958 1.86798 0.05559 dlist(d,~a+b+c|a<0 & b>0, 1:5) #>    a       b       c       #> 1  -0.5771 1.38640  0.2035 #> 5  -2.5578 0.83551  0.5970 #> 7  -1.4403 1.84722 -1.3639 #> 12 -0.6864 0.34647  1.1212 #> 13 -0.4690 0.03698  0.2141 dlist(d,~a+b+c|a<0 & b>0, -(5:1)) #>    a       b       c        #> 7  -1.4403 1.84722 -1.36392 #> 12 -0.6864 0.34647  1.12117 #> 13 -0.4690 0.03698  0.21409 #> 14 -0.8690 0.24112  0.85016 #> 19 -0.9958 1.86798  0.05559 dlist(d,~a+b+c|a<0 & b>0, list(1:5,50:55,-(5:1))) #>     a       b       c        #> 1   -0.5771 1.38640  0.2035  #> 5   -2.5578 0.83551  0.5970  #> 7   -1.4403 1.84722 -1.3639  #> 12  -0.6864 0.34647  1.1212  #> 13  -0.4690 0.03698  0.2141  #> ---                          #> ---                          #> 7   -1.4403 1.84722 -1.36392 #> 12  -0.6864 0.34647  1.12117 #> 13  -0.4690 0.03698  0.21409 #> 14  -0.8690 0.24112  0.85016 #> 19  -0.9958 1.86798  0.05559 dprint(d,a+b+c ~ I(d>0) |a<0 & b>0, list(1:5,50:55,-(5:1))) #> I(d > 0): FALSE #>     a     b     c      #> 7   -1.44 1.847 -1.364 #> ---                    #> ---                    #> 7   -1.44 1.847 -1.364 #> ------------------------------------------------------------  #> I(d > 0): TRUE #>     a       b       c       #> 1   -0.5771 1.38640 0.2035  #> 5   -2.5578 0.83551 0.5970  #> 12  -0.6864 0.34647 1.1212  #> 13  -0.4690 0.03698 0.2141  #> 14  -0.8690 0.24112 0.8502  #> ---                         #> ---                         #> 5   -2.5578 0.83551 0.59703 #> 12  -0.6864 0.34647 1.12117 #> 13  -0.4690 0.03698 0.21409 #> 14  -0.8690 0.24112 0.85016 #> 19  -0.9958 1.86798 0.05559"},{"path":"http://kkholst.github.io/mets/reference/dreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression for data frames with dutility call — dreg","title":"Regression for data frames with dutility call — dreg","text":"Regression data frames dutility call","code":""},{"path":"http://kkholst.github.io/mets/reference/dreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression for data frames with dutility call — dreg","text":"","code":"dreg(   data,   y,   x = NULL,   z = NULL,   x.oneatatime = TRUE,   x.base.names = NULL,   z.arg = c(\"clever\", \"base\", \"group\", \"condition\"),   fun. = lm,   summary. = summary,   regex = FALSE,   convert = NULL,   doSummary = TRUE,   special = NULL,   equal = TRUE,   test = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/dreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression for data frames with dutility call — dreg","text":"data data frame y name variable, fomula, names variables data frame. x name variable, fomula, names variables data frame. z name variable, fomula, names variables data frame. x.oneatatime x's one time x.base.names base covarirates z.arg Z, c(\"clever\",\"base\",\"group\",\"condition\"), clever decides based type Z, base means Z used fixed baseline covaraites X, group means analyses done based groups Z, condition means Z specifies condition data fun. function  lm default summary. summary use regex regex convert convert doSummary doSummary special special's equal pairwise stuff test development argument ... Additional arguments fun","code":""},{"path":"http://kkholst.github.io/mets/reference/dreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression for data frames with dutility call — dreg","text":"Klaus K. Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression for data frames with dutility call — dreg","text":"","code":"##' data(iris) dat <- iris drename(dat) <- ~. names(dat) #> [1] \"sepal.length\" \"sepal.width\"  \"petal.length\" \"petal.width\"  \"species\"      set.seed(1) dat$time <- runif(nrow(dat)) dat$time1 <- runif(nrow(dat)) dat$status <- rbinom(nrow(dat),1,0.5) dat$S1 <- with(dat, Surv(time,status)) dat$S2 <- with(dat, Surv(time1,status)) dat$id <- 1:nrow(dat)  mm <- dreg(dat, \"*.length\"~\"*.width\"|I(species==\"setosa\" & status==1)) mm <- dreg(dat, \"*.length\"~\"*.width\"|species+status) mm <- dreg(dat, \"*.length\"~\"*.width\"|species) mm <- dreg(dat, \"*.length\"~\"*.width\"|species+status,z.arg=\"group\")   ## Reduce Ex.Timings y <- \"S*\"~\"*.width\" xs <- dreg(dat, y, fun.=phreg) ## xs <- dreg(dat, y, fun.=survdiff)  y <- \"S*\"~\"*.width\" xs <- dreg(dat, y, x.oneatatime=FALSE, fun.=phreg)  ## under condition y <- S1~\"*.width\"|I(species==\"setosa\" & sepal.width>3) xs <- dreg(dat, y, z.arg=\"condition\", fun.=phreg) xs <- dreg(dat, y, fun.=phreg)  ## under condition y <- S1~\"*.width\"|species==\"setosa\" xs <- dreg(dat, y, z.arg=\"condition\", fun.=phreg) xs <- dreg(dat, y, fun.=phreg)  ## with baseline  after | y <- S1~\"*.width\"|sepal.length xs <- dreg(dat, y, fun.=phreg)  ## by group by species, not working y <- S1~\"*.width\"|species ss <- split(dat, paste(dat$species, dat$status))  xs <- dreg(dat, y, fun.=phreg)  ## species as base, species is factor so assumes that this is grouping y <- S1~\"*.width\"|species xs <- dreg(dat, y, z.arg=\"base\", fun.=phreg)  ##  background var after | and then one of x's at at time y <- S1~\"*.width\"|status+\"sepal*\" xs <- dreg(dat, y, fun.=phreg)  ##  background var after | and then one of x's at at time ##y <- S1~\"*.width\"|status+\"sepal*\" ##xs <- dreg(dat, y, x.oneatatime=FALSE, fun.=phreg) ##xs <- dreg(dat, y, fun.=phreg)  ##  background var after | and then one of x's at at time ##y <- S1~\"*.width\"+factor(species) ##xs <- dreg(dat, y, fun.=phreg) ##xs <- dreg(dat, y, fun.=phreg, x.oneatatime=FALSE)  y <- S1~\"*.width\"|factor(species) xs <- dreg(dat, y, z.arg=\"base\", fun.=phreg)  y <- S1~\"*.width\"|cluster(id)+factor(species) xs <- dreg(dat, y, z.arg=\"base\", fun.=phreg) xs <- dreg(dat, y, z.arg=\"base\", fun.=survival::coxph)  ## under condition with groups y <- S1~\"*.width\"|I(sepal.length>4) xs <- dreg(subset(dat, species==\"setosa\"), y,z.arg=\"group\",fun.=phreg)  ## under condition with groups y <- S1~\"*.width\"+I(log(sepal.length))|I(sepal.length>4) xs <- dreg(subset(dat, species==\"setosa\"), y,z.arg=\"group\",fun.=phreg)  y <- S1~\"*.width\"+I(dcut(sepal.length))|I(sepal.length>4) xs <- dreg(subset(dat,species==\"setosa\"), y,z.arg=\"group\",fun.=phreg)  ff <- function(formula,data,...) {  ss <- survfit(formula,data,...)  kmplot(ss,...)  return(ss) }  if (interactive()) { dcut(dat) <- ~\"*.width\" y <- S1~\"*.4\"|I(sepal.length>4) par(mfrow=c(1, 2)) xs <- dreg(dat, y, fun.=ff) }"},{"path":"http://kkholst.github.io/mets/reference/drelevel.html","id":null,"dir":"Reference","previous_headings":"","what":"relev levels for data frames — drelevel","title":"relev levels for data frames — drelevel","text":"levels shows levels variables data frame, relevel relevels factor data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/drelevel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"relev levels for data frames — drelevel","text":"","code":"drelevel(   data,   y = NULL,   x = NULL,   ref = NULL,   newlevels = NULL,   regex = mets.options()$regex,   sep = NULL,   overwrite = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/drelevel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"relev levels for data frames — drelevel","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. x name variable, fomula, names variables data frame. ref new reference variable newlevels combine levels factor data frame regex regular expressions. sep seperator naming cut names. overwrite overwrite variable ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/drelevel.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"relev levels for data frames — drelevel","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/drelevel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"relev levels for data frames — drelevel","text":"","code":"data(mena) dstr(mena) #> 'data.frame':\t2000 obs. of  7 variables: #>  $ cohort    : Factor w/ 4 levels \"[1973,1975]\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ agemena   : num  13.8 12.7 12.4 13.1 13.5 ... #>  $ status    : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ zyg       : Factor w/ 2 levels \"MZ\",\"DZ\": 2 2 1 1 1 1 1 1 1 1 ... #>  $ twinnum   : int  2 1 2 1 2 1 2 1 2 1 ... #>  $ id        : num  1 1 2 2 3 3 4 4 5 5 ... #>  $ mothermena: num  13.5 13.5 12.5 12.5 14.7 ... dfactor(mena)  <- ~twinnum dnumeric(mena) <- ~twinnum.f  dstr(mena) #> 'data.frame':\t2000 obs. of  9 variables: #>  $ cohort     : Factor w/ 4 levels \"[1973,1975]\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ agemena    : num  13.8 12.7 12.4 13.1 13.5 ... #>  $ status     : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ zyg        : Factor w/ 2 levels \"MZ\",\"DZ\": 2 2 1 1 1 1 1 1 1 1 ... #>  $ twinnum    : int  2 1 2 1 2 1 2 1 2 1 ... #>  $ id         : num  1 1 2 2 3 3 4 4 5 5 ... #>  $ mothermena : num  13.5 13.5 12.5 12.5 14.7 ... #>  $ twinnum.f  : Factor w/ 2 levels \"1\",\"2\": 2 1 2 1 2 1 2 1 2 1 ... #>  $ twinnum.f.n: num  2 1 2 1 2 1 2 1 2 1 ...  mena2 <- drelevel(mena,\"cohort\",ref=\"(1980,1982]\") mena2 <- drelevel(mena,~cohort,ref=\"(1980,1982]\") mena2 <- drelevel(mena,cohortII~cohort,ref=\"(1980,1982]\") dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- dlevels(mena2) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohortII #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- drelevel(mena,ref=\"(1975,1977]\")  <-  ~cohort drelevel(mena,ref=\"(1980,1982]\")  <-  ~cohort dlevels(mena,\"coh*\") #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- dtable(mena,\"coh*\",level=1) #>  #> cohort #> [1973,1975] (1975,1977] (1977,1980] (1980,1982]  #>         710         412         540         338  #>  #> cohort.(1975,1977] #> (1975,1977] [1973,1975] (1977,1980] (1980,1982]  #>         412         710         540         338  #>  #> cohort.(1980,1982] #> (1980,1982] [1973,1975] (1975,1977] (1977,1980]  #>         338         710         412         540  #>   ### level 1 of zyg as baseline for new variable drelevel(mena,ref=1) <- ~zyg drelevel(mena,ref=c(\"DZ\",\"[1973,1975]\")) <- ~ zyg+cohort drelevel(mena,ref=c(\"DZ\",\"[1973,1975]\")) <- zygdz+cohort.early~ zyg+cohort ### level 2 of zyg and cohort as baseline for new variables drelevel(mena,ref=2) <- ~ zyg+cohort dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> -----------------------------------------  ##################### combining factor levels with newlevels argument  dcut(mena,labels=c(\"I\",\"II\",\"III\",\"IV\")) <- cat4~agemena dlevels(drelevel(mena,~cat4,newlevels=1:3)) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> cat4.1:3 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- dlevels(drelevel(mena,ncat4~cat4,newlevels=3:2)) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- drelevel(mena,newlevels=3:2) <- ncat4~cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> -----------------------------------------  dlevels(drelevel(mena,nca4~cat4,newlevels=list(c(1,4),2:3))) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> -----------------------------------------  drelevel(mena,newlevels=list(c(1,4),2:3)) <- nca4..2 ~ cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4..2 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> -----------------------------------------  drelevel(mena,newlevels=list(\"I-III\"=c(\"I\",\"II\",\"III\"),\"IV\"=\"IV\")) <- nca4..3 ~ cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4..2 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> ----------------------------------------- #> nca4..3 #levels=:2  #> [1] \"I-III\" \"IV\"    #> -----------------------------------------  drelevel(mena,newlevels=list(\"I-III\"=c(\"I\",\"II\",\"III\"))) <- nca4..4 ~ cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4..2 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> ----------------------------------------- #> nca4..3 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- #> nca4..4 #levels=:2  #> [1] \"I-III\" \"IV\"    #> -----------------------------------------  drelevel(mena,newlevels=list(group1=c(\"I\",\"II\",\"III\"))) <- nca4..5 ~ cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4..2 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> ----------------------------------------- #> nca4..3 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- #> nca4..4 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- #> nca4..5 #levels=:2  #> [1] \"group1\" \"IV\"     #> -----------------------------------------  drelevel(mena,newlevels=list(g1=c(\"I\",\"II\",\"III\"),g2=\"IV\")) <- nca4..6 ~ cat4 dlevels(mena) #> cohort #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> twinnum.f #levels=:2  #> [1] \"1\" \"2\" #> ----------------------------------------- #> cohort.(1975,1977] #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cohort.(1980,1982] #levels=:4  #> [1] \"(1980,1982]\" \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" #> ----------------------------------------- #> zyg.1 #levels=:2  #> [1] \"MZ\" \"DZ\" #> ----------------------------------------- #> zyg.DZ #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.[1973,1975] #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zygdz #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.early #levels=:4  #> [1] \"[1973,1975]\" \"(1975,1977]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> zyg.2 #levels=:2  #> [1] \"DZ\" \"MZ\" #> ----------------------------------------- #> cohort.2 #levels=:4  #> [1] \"(1975,1977]\" \"[1973,1975]\" \"(1977,1980]\" \"(1980,1982]\" #> ----------------------------------------- #> cat4 #levels=:4  #> [1] \"I\"   \"II\"  \"III\" \"IV\"  #> ----------------------------------------- #> ncat4 #levels=:3  #> [1] \"III-II\" \"I\"      \"IV\"     #> ----------------------------------------- #> nca4..2 #levels=:2  #> [1] \"I-IV\"   \"II-III\" #> ----------------------------------------- #> nca4..3 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- #> nca4..4 #levels=:2  #> [1] \"I-III\" \"IV\"    #> ----------------------------------------- #> nca4..5 #levels=:2  #> [1] \"group1\" \"IV\"     #> ----------------------------------------- #> nca4..6 #levels=:2  #> [1] \"g1\" \"g2\" #> -----------------------------------------"},{"path":"http://kkholst.github.io/mets/reference/dsort.html","id":null,"dir":"Reference","previous_headings":"","what":"Sort data frame — dsort","title":"Sort data frame — dsort","text":"Sort data according columns data frame","code":""},{"path":"http://kkholst.github.io/mets/reference/dsort.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sort data frame — dsort","text":"","code":"dsort(data, x, ..., decreasing = FALSE, return.order = FALSE)"},{"path":"http://kkholst.github.io/mets/reference/dsort.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sort data frame — dsort","text":"data Data frame x variable order ... additional variables order decreasing sort order (vector length x) return.order return order","code":""},{"path":"http://kkholst.github.io/mets/reference/dsort.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sort data frame — dsort","text":"data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/dsort.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sort data frame — dsort","text":"","code":"data(data=\"hubble\",package=\"lava\") dsort(hubble, \"sigma\") #>        v     D sigma #> 14  5935  88.6   2.1 #> 24 15567 236.1   2.1 #> 8   7880 121.5   2.2 #> 1   9065 134.7   2.3 #> 16 13518 202.5   2.3 #> 11  4227  58.0   2.4 #> 12 30253 467.0   2.4 #> 15 10696 151.4   2.4 #> 19  5434  77.9   2.4 #> 23 21190 303.4   2.4 #> 25 15002 215.4   2.4 #> 31  4847  66.8   2.5 #> 35  9024 136.0   2.5 #> 17 17371 235.9   2.6 #> 18 12871 176.8   2.6 #> 20 23646 309.5   2.6 #> 29  7241  96.7   2.6 #> 32 10715 149.9   2.6 #> 10  7765 102.1   2.7 #> 27 14764 202.3   2.7 #> 30  8691 127.8   2.7 #> 33 14634 185.6   2.7 #> 36 10446 132.7   2.7 #> 3  15055 198.6   2.8 #> 4  16687 238.9   2.8 #> 22 18997 280.1   2.8 #> 34  6673  82.4   2.8 #> 6   4124  56.0   2.9 #> 13 18212 262.2   2.9 #> 26  8604 119.7   2.9 #> 2  12012 158.9   3.1 #> 7  13707 183.9   3.1 #> 21 26318 391.5   3.1 #> 28  5424  71.8   3.1 #> 5   9801 117.1   3.4 #> 9  22426 274.6   3.4 dsort(hubble, hubble$sigma,\"v\") #>        v     D sigma #> 14  5935  88.6   2.1 #> 24 15567 236.1   2.1 #> 8   7880 121.5   2.2 #> 1   9065 134.7   2.3 #> 16 13518 202.5   2.3 #> 11  4227  58.0   2.4 #> 19  5434  77.9   2.4 #> 15 10696 151.4   2.4 #> 25 15002 215.4   2.4 #> 23 21190 303.4   2.4 #> 12 30253 467.0   2.4 #> 31  4847  66.8   2.5 #> 35  9024 136.0   2.5 #> 29  7241  96.7   2.6 #> 32 10715 149.9   2.6 #> 18 12871 176.8   2.6 #> 17 17371 235.9   2.6 #> 20 23646 309.5   2.6 #> 10  7765 102.1   2.7 #> 30  8691 127.8   2.7 #> 36 10446 132.7   2.7 #> 33 14634 185.6   2.7 #> 27 14764 202.3   2.7 #> 34  6673  82.4   2.8 #> 3  15055 198.6   2.8 #> 4  16687 238.9   2.8 #> 22 18997 280.1   2.8 #> 6   4124  56.0   2.9 #> 26  8604 119.7   2.9 #> 13 18212 262.2   2.9 #> 28  5424  71.8   3.1 #> 2  12012 158.9   3.1 #> 7  13707 183.9   3.1 #> 21 26318 391.5   3.1 #> 5   9801 117.1   3.4 #> 9  22426 274.6   3.4 dsort(hubble,~sigma+v) #>        v     D sigma #> 14  5935  88.6   2.1 #> 24 15567 236.1   2.1 #> 8   7880 121.5   2.2 #> 1   9065 134.7   2.3 #> 16 13518 202.5   2.3 #> 11  4227  58.0   2.4 #> 19  5434  77.9   2.4 #> 15 10696 151.4   2.4 #> 25 15002 215.4   2.4 #> 23 21190 303.4   2.4 #> 12 30253 467.0   2.4 #> 31  4847  66.8   2.5 #> 35  9024 136.0   2.5 #> 29  7241  96.7   2.6 #> 32 10715 149.9   2.6 #> 18 12871 176.8   2.6 #> 17 17371 235.9   2.6 #> 20 23646 309.5   2.6 #> 10  7765 102.1   2.7 #> 30  8691 127.8   2.7 #> 36 10446 132.7   2.7 #> 33 14634 185.6   2.7 #> 27 14764 202.3   2.7 #> 34  6673  82.4   2.8 #> 3  15055 198.6   2.8 #> 4  16687 238.9   2.8 #> 22 18997 280.1   2.8 #> 6   4124  56.0   2.9 #> 26  8604 119.7   2.9 #> 13 18212 262.2   2.9 #> 28  5424  71.8   3.1 #> 2  12012 158.9   3.1 #> 7  13707 183.9   3.1 #> 21 26318 391.5   3.1 #> 5   9801 117.1   3.4 #> 9  22426 274.6   3.4 dsort(hubble,~sigma-v) #>        v     D sigma #> 24 15567 236.1   2.1 #> 14  5935  88.6   2.1 #> 8   7880 121.5   2.2 #> 16 13518 202.5   2.3 #> 1   9065 134.7   2.3 #> 12 30253 467.0   2.4 #> 23 21190 303.4   2.4 #> 25 15002 215.4   2.4 #> 15 10696 151.4   2.4 #> 19  5434  77.9   2.4 #> 11  4227  58.0   2.4 #> 35  9024 136.0   2.5 #> 31  4847  66.8   2.5 #> 20 23646 309.5   2.6 #> 17 17371 235.9   2.6 #> 18 12871 176.8   2.6 #> 32 10715 149.9   2.6 #> 29  7241  96.7   2.6 #> 27 14764 202.3   2.7 #> 33 14634 185.6   2.7 #> 36 10446 132.7   2.7 #> 30  8691 127.8   2.7 #> 10  7765 102.1   2.7 #> 22 18997 280.1   2.8 #> 4  16687 238.9   2.8 #> 3  15055 198.6   2.8 #> 34  6673  82.4   2.8 #> 13 18212 262.2   2.9 #> 26  8604 119.7   2.9 #> 6   4124  56.0   2.9 #> 21 26318 391.5   3.1 #> 7  13707 183.9   3.1 #> 2  12012 158.9   3.1 #> 28  5424  71.8   3.1 #> 9  22426 274.6   3.4 #> 5   9801 117.1   3.4  ## with direct asignment dsort(hubble) <- ~sigma-v"},{"path":"http://kkholst.github.io/mets/reference/dspline.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple linear spline — dspline","title":"Simple linear spline — dspline","text":"Constructs simple linear spline  data frame using formula syntax dutils adds (x-cuti)* (x>cuti) data-set knot spline. full spline thus given x spline variables added data-set.","code":""},{"path":"http://kkholst.github.io/mets/reference/dspline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple linear spline — dspline","text":"","code":"dspline(   data,   y = NULL,   x = NULL,   breaks = 4,   probs = NULL,   equi = FALSE,   regex = mets.options()$regex,   sep = NULL,   na.rm = TRUE,   labels = NULL,   all = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/dspline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple linear spline — dspline","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. x name variable, fomula, names variables data frame. breaks number breaks, variables vector break points, probs groups defined quantiles equi equi-spaced breaks regex regular expressions. sep seperator naming cut names. na.rm remove NA grouping variables. labels use cut groups variables, even breaks unique ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/dspline.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simple linear spline — dspline","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dspline.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simple linear spline — dspline","text":"","code":"data(TRACE) TRACE <- dspline(TRACE,~wmi,breaks=c(1,1.3,1.7)) cca <- survival::coxph(Surv(time,status==9)~age+vf+chf+wmi,data=TRACE) cca2 <- survival::coxph(Surv(time,status==9)~age+wmi+vf+chf+                            wmi.spline1+wmi.spline2+wmi.spline3,data=TRACE) anova(cca,cca2) #> Analysis of Deviance Table #>  Cox model: response is  Surv(time, status == 9) #>  Model 1: ~ age + vf + chf + wmi #>  Model 2: ~ age + wmi + vf + chf + wmi.spline1 + wmi.spline2 + wmi.spline3 #>    loglik  Chisq Df Pr(>|Chi|)     #> 1 -6529.8                          #> 2 -6519.6 20.323  3  0.0001455 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  nd <- data.frame(age=50,vf=0,chf=0,wmi=seq(0.4,3,by=0.01)) nd <- dspline(nd,~wmi,breaks=c(1,1.3,1.7)) pl <- predict(cca2,newdata=nd) plot(nd$wmi,pl,type=\"l\")"},{"path":"http://kkholst.github.io/mets/reference/dtable.html","id":null,"dir":"Reference","previous_headings":"","what":"tables for data frames — dtable","title":"tables for data frames — dtable","text":"tables data frames","code":""},{"path":"http://kkholst.github.io/mets/reference/dtable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tables for data frames — dtable","text":"","code":"dtable(   data,   y = NULL,   x = NULL,   ...,   level = -1,   response = NULL,   flat = TRUE,   total = FALSE,   prop = FALSE,   summary = NULL )"},{"path":"http://kkholst.github.io/mets/reference/dtable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tables for data frames — dtable","text":"data x formula names data frame data frame needed. y name variable, fomula, names variables data frame. x name variable, fomula, names variables data frame. ... Optional additional arguments level 1 marginal tables, 2 2 2 tables, null full table, possible versus group variable response level=2, produce tables columns given 'response' (index) flat produce flat tables total add total counts/proportions prop Proportions instead counts (vector margins) summary summary function","code":""},{"path":"http://kkholst.github.io/mets/reference/dtable.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tables for data frames — dtable","text":"Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/dtable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"tables for data frames — dtable","text":"","code":"data(\"sTRACE\",package=\"timereg\")  dtable(sTRACE,~status) #>  #> status #>   0   7   9  #> 236   5 259  #>  dtable(sTRACE,~status+vf) #>  #>        vf   0   1 #> status            #> 0         225  11 #> 7           5   0 #> 9         241  18 dtable(sTRACE,~status+vf,level=1) #>  #> status #>   0   7   9  #> 236   5 259  #>  #> vf #>   0   1  #> 471  29  #>  dtable(sTRACE,~status+vf,~chf+diabetes) #> chf: 0 #> diabetes: 0 #>  #>        vf   0   1 #> status            #> 0         143   4 #> 7           4   0 #> 9          70   2 #> ------------------------------------------------------------  #> chf: 1 #> diabetes: 0 #>  #>        vf   0   1 #> status            #> 0          66   7 #> 7           1   0 #> 9         141  15 #> ------------------------------------------------------------  #> chf: 0 #> diabetes: 1 #>  #>        vf 0 #> status      #> 0         8 #> 9         8 #> ------------------------------------------------------------  #> chf: 1 #> diabetes: 1 #>  #>        vf  0  1 #> status          #> 0          8  0 #> 9         22  1  dtable(sTRACE,c(\"*f*\",\"status\"),~diabetes) #> diabetes: 0 #>  #>        status   0   7   9 #> chf vf                    #> 0   0         143   4  70 #>     1           4   0   2 #> 1   0          66   1 141 #>     1           7   0  15 #> ------------------------------------------------------------  #> diabetes: 1 #>  #>        status  0  9 #> chf vf              #> 0   0          8  8 #>     1          0  0 #> 1   0          8 22 #>     1          0  1 dtable(sTRACE,c(\"*f*\",\"status\"),~diabetes, level=2) #> diabetes: 0 #>  #>    chf #> vf    0   1 #>   0 217 208 #>   1   6  22 #>  #>       chf #> status   0   1 #>      0 147  73 #>      7   4   1 #>      9  72 156 #>  #>       vf #> status   0   1 #>      0 209  11 #>      7   5   0 #>      9 211  17 #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #>    chf #> vf   0  1 #>   0 16 30 #>   1  0  1 #>  #>       chf #> status  0  1 #>      0  8  8 #>      9  8 23 #>  #>       vf #> status  0  1 #>      0 16  0 #>      9 30  1 #>  dtable(sTRACE,c(\"*f*\",\"status\"),level=1) #>  #> chf #>   0   1  #> 239 261  #>  #> vf #>   0   1  #> 471  29  #>  #> status #>   0   7   9  #> 236   5 259  #>   dtable(sTRACE,~\"*f*\"+status,level=1) #>  #> chf #>   0   1  #> 239 261  #>  #> vf #>   0   1  #> 471  29  #>  #> status #>   0   7   9  #> 236   5 259  #>  dtable(sTRACE,~\"*f*\"+status+I(wmi>1.4)|age>60,level=2) #>  #>    chf #> vf    0   1 #>   0 145 209 #>   1   5  16 #>  #>       chf #> status   0   1 #>      0  83  58 #>      7   3   0 #>      9  64 167 #>  #>             chf #> I(wmi > 1.4)   0   1 #>        FALSE  57 149 #>        TRUE   93  76 #>  #>       vf #> status   0   1 #>      0 135   6 #>      7   3   0 #>      9 216  15 #>  #>             vf #> I(wmi > 1.4)   0   1 #>        FALSE 191  15 #>        TRUE  163   6 #>  #>             status #> I(wmi > 1.4)   0   7   9 #>        FALSE  53   0 153 #>        TRUE   88   3  78 #>  dtable(sTRACE,\"*f*\"+status~I(wmi>0.5)|age>60,level=1) #> I(wmi > 0.5): FALSE #>  #> chf #> 1  #> 3  #>  #> vf #> 0 1  #> 2 1  #>  #> status #> 0 9  #> 1 2  #>  #> ------------------------------------------------------------  #> I(wmi > 0.5): TRUE #>  #> chf #>   0   1  #> 150 222  #>  #> vf #>   0   1  #> 352  20  #>  #> status #>   0   7   9  #> 140   3 229  #>  dtable(sTRACE,status~dcut(age)) #> dcut(age): [28.1,60] #>  #> status #>  0  7  9  #> 95  2 28  #>  #> ------------------------------------------------------------  #> dcut(age): (60,68.7] #>  #> status #>  0  7  9  #> 66  2 57  #>  #> ------------------------------------------------------------  #> dcut(age): (68.7,76] #>  #> status #>  0  7  9  #> 55  1 69  #>  #> ------------------------------------------------------------  #> dcut(age): (76,92.1] #>  #> status #>   0   9  #>  20 105  #>   dtable(sTRACE,~status+vf+sex|age>60) #>  #>           sex   0   1 #> status vf             #> 0      0       52  83 #>        1        3   3 #> 7      0        0   3 #>        1        0   0 #> 9      0       80 136 #>        1        6   9 dtable(sTRACE,status+vf+sex~+1|age>60, level=2) #>  #>    status #> vf    0   7   9 #>   0 135   3 216 #>   1   6   0  15 #>  #>    status #> sex   0   7   9 #>   0  55   0  86 #>   1  86   3 145 #>  #>    vf #> sex   0   1 #>   0 132   9 #>   1 222  12 #>  dtable(sTRACE,.~status+vf+sex|age>60,level=1) #> status: 0 #> vf: 0 #> sex: 0 #>  #> no #>   64  207  295  313  385  386  672  762  775  879  977  979 1171 1483 1599 2457  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 2520 2549 2712 2820 2850 2927 2952 3587 3832 3872 3902 3970 3999 4049 4189 4295  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 4419 4515 4522 4623 4868 4911 4955 5032 5192 5305 5363 5470 5643 5783 5997 6150  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 6231 6258 6309 6549  #>    1    1    1    1  #>  #> wmi #> 0.8 0.9   1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9   2  #>   1   2   1   6   2   3   4   4   7   5   9   5   3  #>  #> chf #>  0  1  #> 30 22  #>  #> age #> 60.422 60.918 61.242  61.42 61.755 61.782 62.182 62.997  63.38 64.354 65.461  #>      1      1      1      1      1      1      1      1      1      1      1  #> 65.546 67.389  67.77  68.31 68.507 68.663 68.869 69.842 70.059 70.791 70.999  #>      1      1      1      1      1      1      1      1      1      1      1  #> 71.183 71.227 71.273 71.326 71.411 71.942 72.326 73.829 73.996 74.174 74.193  #>      1      1      1      1      1      1      1      1      1      1      1  #>   74.3 74.777 74.986 75.117 75.194 75.597 75.964 76.249 76.551 76.691 76.803  #>      1      1      1      1      1      1      1      1      1      1      1  #> 77.703 78.004 80.222 80.288 80.529 82.021 82.437 86.163  #>      1      1      1      1      1      1      1      1  #>  #> diabetes #>  0  1  #> 50  2  #>  #> time #> 6.124 6.146 6.149 6.151 6.162 6.187 6.272 6.323 6.343 6.406  6.45 6.499 6.505  #>     1     1     1     1     1     1     1     1     1     1     1     1     1  #> 6.511 6.521 6.526  6.56 6.601  6.62 6.628 6.631 6.666  6.71 6.721  6.74 6.831  #>     1     2     1     1     1     1     1     1     1     1     1     1     1  #> 6.875  6.91 6.943 7.009 7.011 7.063 7.118 7.126 7.261 7.294  7.34 7.346 7.406  #>     1     1     1     1     1     1     1     1     1     1     1     1     1  #> 7.422 7.458 7.505 7.562 7.598 7.604 7.619 7.683 7.702 7.743 7.872 8.033  #>     1     1     1     1     1     1     1     1     1     1     1     1  #>  #> ------------------------------------------------------------  #> status: 7 #> vf: 0 #> sex: 0 #> NULL #> ------------------------------------------------------------  #> status: 9 #> vf: 0 #> sex: 0 #>  #> no #>   27  312  454  476  565  707  728  784  796  808  914  999 1085 1554 1563 1592  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 1611 1620 1654 1748 1973 2239 2255 2386 2570 2729 2867 2902 2938 3035 3039 3127  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 3234 3245 3286 3333 3372 3623 3742 3756 3857 3985 4006 4021 4239 4399 4641 4776  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 4802 4827 4847 4894 4914 4933 5003 5072 5080 5249 5277 5514 5589 5603 5647 5716  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 5740 5764 5857 5866 6044 6045 6054 6180 6193 6268 6311 6317 6523 6542 6564 6628  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #>  #> wmi #> 0.6 0.7 0.8 0.9   1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9   2 2.1  #>   1   4   8   3   3  12   3   3   7   4   8   8   6   3   6   1  #>  #> chf #>  0  1  #> 19 61  #>  #> age #> 60.477 63.005 63.057 63.737 64.474 65.382 65.442 65.618 66.393 66.539 66.884  #>      1      1      1      1      1      1      1      1      1      1      1  #> 67.608 67.964 69.352 69.502 70.207   70.5 71.422 71.671 71.767 71.811 72.628  #>      1      1      1      1      1      1      1      1      1      1      1  #> 72.839 73.258 73.406 74.048 74.147 74.163 75.347 75.358 75.419 75.666 76.907  #>      1      1      1      1      1      1      1      1      1      1      1  #> 77.031 77.119 77.297 77.385 77.434 77.914 78.081 78.185 78.221 78.687  78.72  #>      1      1      1      1      1      1      1      1      1      1      1  #> 78.753 78.827 79.537 79.542 79.997 80.121 80.576 80.888 81.047 81.892   82.3  #>      1      1      1      1      1      1      1      1      1      1      1  #> 82.626 82.676 82.706 82.961 82.988 83.282 83.405  83.52 83.764 84.269 84.529  #>      1      1      1      1      1      1      1      1      1      1      1  #> 84.562 84.825 85.141 85.404 85.763 85.867   86.1 86.561 87.175 88.348 88.828  #>      1      1      1      1      1      1      1      1      1      1      1  #> 88.847 90.673  92.11  #>      1      1      1  #>  #> diabetes #>  0  1  #> 68 12  #>  #> time #> 0.00924501924891956  0.0102543342947029  0.0127033231498208  0.0148339297396597  #>                   1                   1                   1                   1  #>  0.0244941983071622  0.0291877593353856  0.0449010478444397  0.0491303009549156  #>                   1                   1                   1                   1  #>               0.061  0.0812215200138744   0.103132670026505   0.106090152188204  #>                   1                   1                   1                   1  #>               0.124               0.135               0.145               0.175  #>                   1                   1                   1                   1  #>               0.196   0.217822634588927               0.233               0.381  #>                   1                   1                   1                   1  #>                0.48               0.493               0.534   0.551569005857687  #>                   1                   1                   1                   1  #>               0.554               0.567               0.642               0.787  #>                   1                   1                   1                   1  #>               0.795               0.913               0.959               0.998  #>                   1                   1                   1                   1  #>               1.121               1.179                1.25               1.337  #>                   1                   1                   1                   1  #>               1.412                1.51               1.603               1.752  #>                   1                   1                   1                   1  #>               1.801               1.893               1.968    2.05902000734583  #>                   1                   1                   1                   1  #>               2.392    2.47266312449542               2.554               2.773  #>                   1                   1                   1                   1  #>               2.795               2.937               3.261    3.39589819746953  #>                   1                   1                   1                   1  #>               3.593               3.894               3.943                4.08  #>                   1                   1                   1                   1  #>               4.176               4.338               4.559               4.595  #>                   1                   1                   1                   1  #>               4.814               4.859               4.883               4.904  #>                   1                   1                   1                   1  #>               4.943               5.151               5.359               5.571  #>                   1                   1                   1                   1  #>               5.767               5.927               5.954    5.95500826347712  #>                   1                   1                   1                   1  #>                6.02               6.108               6.362               6.579  #>                   1                   1                   1                   1  #>               6.836               7.222               7.261               7.379  #>                   1                   1                   1                   1  #>  #> ------------------------------------------------------------  #> status: 0 #> vf: 1 #> sex: 0 #>  #> no #> 1881 3847 5248  #>    1    1    1  #>  #> wmi #> 1.4 1.6 1.7  #>   1   1   1  #>  #> chf #> 1  #> 3  #>  #> age #> 60.688 66.119 78.498  #>      1      1      1  #>  #> diabetes #> 0  #> 3  #>  #> time #> 6.031 6.447 6.496  #>     1     1     1  #>  #> ------------------------------------------------------------  #> status: 7 #> vf: 1 #> sex: 0 #> NULL #> ------------------------------------------------------------  #> status: 9 #> vf: 1 #> sex: 0 #>  #> no #>  965 1787 5378 5481 5841 6487  #>    1    1    1    1    1    1  #>  #> wmi #> 0.7   1 1.2 1.3 1.5  #>   1   1   2   1   1  #>  #> chf #> 0 1  #> 1 5  #>  #> age #> 62.912  64.31 70.846 73.352 75.646  77.28  #>      1      1      1      1      1      1  #>  #> diabetes #> 0 1  #> 5 1  #>  #> time #> 0.00723927922639996  0.0130957318809815  0.0159947743292432               0.044  #>                   1                   1                   1                   1  #>               0.302               0.411  #>                   1                   1  #>  #> ------------------------------------------------------------  #> status: 0 #> vf: 0 #> sex: 1 #>  #> no #>   22   23   25  137  236  244  366  369  384  427  473  636  680  774  804  899  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #>  929 1052 1062 1246 1285 1306 1328 1354 1466 1519 1694 1749 1993 2172 2228 2230  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 2263 2297 2311 2358 2382 2411 2458 2550 2889 2897 3137 3323 3350 3384 3460 3522  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 3585 3650 3661 4280 4468 4516 4711 4723 4855 4936 5173 5234 5434 5478 5479 5519  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 5565 5619 5664 5709 5750 5766 5851 5896 5906 6049 6075 6153 6175 6195 6254 6414  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 6436 6525 6553  #>    1    1    1  #>  #> wmi #> 0.4 0.6 0.7 0.8 0.9   1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9   2 2.2  #>   1   3   1   2   1   3   7   4   5   5   3   9  13  11   3  11   1  #>  #> chf #>  0  1  #> 50 33  #>  #> age #> 60.047 60.208 60.302 60.803  60.96 61.036 61.491 61.897 61.919 62.319 62.492  #>      1      1      1      1      1      1      1      1      1      1      1  #> 62.827  62.84 62.912 62.928 63.287 63.298 63.885 63.893 64.005 64.186 64.329  #>      1      1      1      1      1      1      1      1      1      1      1  #> 64.373 64.606 65.368 65.428 65.746 65.798 65.878 66.064 66.459  67.06 67.194  #>      1      1      1      1      1      1      1      1      1      1      1  #> 67.367 67.597 67.655 67.701 67.885 68.003 68.151 68.154 68.211 68.318 68.324  #>      1      1      1      1      1      1      1      1      1      1      1  #> 68.345 68.913 69.036 69.236 69.343 69.933 70.182 70.314 70.654   70.7 70.917  #>      1      1      1      1      1      1      1      1      1      1      1  #> 71.046  71.32 71.811 72.195 72.595 72.759 72.869 73.141 73.195 73.236  74.07  #>      1      1      1      1      1      1      1      1      1      1      1  #> 74.404 74.437 74.618 74.816 74.988 75.027 75.068 75.191 75.358 75.663 77.025  #>      1      1      1      1      1      1      1      1      1      1      1  #> 77.171 78.511  79.09 79.578  81.67   83.4  #>      1      1      1      1      1      1  #>  #> diabetes #>  0  1  #> 79  4  #>  #> time #> 5.998 6.003 6.004 6.034 6.069 6.077  6.11 6.151 6.156 6.162 6.219  6.28 6.283  #>     1     1     1     1     1     1     1     1     1     1     1     1     1  #> 6.302 6.316 6.346 6.348 6.359 6.384 6.392 6.401 6.433 6.436 6.463 6.467 6.496  #>     1     1     1     1     1     1     1     1     2     1     1     1     1  #> 6.502 6.554 6.592 6.601 6.611 6.658  6.71 6.732 6.768 6.773 6.776 6.795 6.803  #>     1     1     1     1     1     1     1     1     1     1     1     1     1  #> 6.823 6.836 6.847 6.979 6.992 7.028 7.074 7.091 7.096 7.102 7.209 7.217 7.239  #>     1     1     1     1     1     1     2     1     1     1     1     1     2  #> 7.261 7.302 7.326 7.335 7.345 7.352 7.364 7.493 7.494 7.571 7.579 7.625 7.652  #>     1     1     1     1     1     1     1     1     2     1     1     1     1  #> 7.746 7.809 7.811 7.817 7.828 7.861 7.863 7.897  7.97 7.976 8.011 8.099 8.157  #>     1     1     1     1     1     1     2     1     1     1     1     1     1  #>  #> ------------------------------------------------------------  #> status: 7 #> vf: 0 #> sex: 1 #>  #> no #>  792 4441 6242  #>    1    1    1  #>  #> wmi #> 1.7 1.9  #>   2   1  #>  #> chf #> 0  #> 3  #>  #> age #> 63.241 66.089 73.645  #>      1      1      1  #>  #> diabetes #> 0  #> 3  #>  #> time #>  0.02 0.028  #>     2     1  #>  #> ------------------------------------------------------------  #> status: 9 #> vf: 0 #> sex: 1 #>  #> no #>   48  125  181  219  235  267  275  277  335  436  477  483  489  506  509  535  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #>  649  656  695  727  921  969  983 1029 1044 1064 1077 1089 1116 1157 1166 1173  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 1214 1225 1260 1280 1301 1337 1456 1490 1615 1671 1703 1723 1739 1757 1813 1924  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 1933 1939 1944 1946 2041 2210 2320 2345 2349 2585 2648 2672 2703 2907 2971 3025  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 3045 3093 3103 3214 3242 3256 3301 3320 3349 3373 3414 3428 3517 3541 3544 3552  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 3627 3647 3685 3793 3809 3946 4018 4051 4143 4228 4247 4369 4417 4479 4511 4524  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 4532 4538 4621 4687 4727 4777 4784 4786 4996 5009 5073 5106 5177 5201 5232 5279  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 5281 5318 5412 5438 5449 5459 5475 5587 5721 5723 5868 5977 5985 6011 6057 6121  #>    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1  #> 6297 6377 6476 6482 6502 6605 6639 6645  #>    1    1    1    1    1    1    1    1  #>  #> wmi #> 0.4 0.6 0.7 0.8 0.9   1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9   2 2.7  #>   1   6   8   6  10  13  16  15  14   7   8   6   6  10   4   5   1  #>  #> chf #>  0  1  #> 43 93  #>  #> age #> 60.258 60.573 60.592 60.628 61.291 61.823 62.191 62.437 62.478 62.541 64.036  #>      1      1      1      1      1      1      1      1      1      2      1  #> 64.074  64.17 64.403 64.562 64.685 64.847 65.242 65.379 65.439 65.459 65.461  #>      1      1      1      1      1      1      1      1      1      1      1  #> 65.681 66.023 66.909 67.265 67.271 67.293 67.323 67.389 67.581 67.644 67.668  #>      1      1      1      1      1      1      1      1      1      1      1  #> 67.753     68 68.025 68.345 68.568 68.691 68.902 68.943 69.121 69.204   69.7  #>      2      1      1      1      1      1      1      1      1      1      1  #> 70.026 70.032 70.073 70.075 70.081 70.319 70.402 71.687 71.827 72.049  72.74  #>      1      1      1      1      1      1      1      1      1      1      1  #> 72.762 72.847 72.886 73.086 73.099 73.453 73.508 73.714 74.177 74.377 74.615  #>      1      1      1      1      1      1      1      1      1      1      1  #> 74.829 74.843 74.873 74.999 75.134 75.224 75.263 75.284 75.306 75.408 75.485  #>      1      1      1      1      1      1      1      1      1      1      1  #>  75.55 75.594 75.652 76.016 76.101 76.164 76.189 76.247 76.255 76.258 76.376  #>      1      1      1      1      2      1      1      1      1      1      1  #> 76.817 77.001 77.047 77.272 77.395 77.574 77.829 77.947 78.045 78.169 78.256  #>      1      1      1      1      1      1      1      1      1      1      1  #> 78.385  78.41 78.476 78.563 78.613 78.769 78.923 79.024 79.197 79.602 79.611  #>      1      1      1      1      1      1      1      1      1      1      1  #>  79.66 80.713 80.913 81.034 82.169 82.396 82.654 82.939 83.375 83.422 83.542  #>      1      1      1      1      1      1      1      1      1      1      1  #> 83.781 84.255  84.34 84.661 84.839 84.924 85.683 85.988  86.72 87.682 89.746  #>      1      1      1      1      1      1      1      1      1      1      1  #> 91.515  #>      1  #>  #> diabetes #>   0   1  #> 118  18  #>  #> time #>          0.00068749 0.00479720664164051               0.005  0.0147687892620452  #>                   1                   1                   1                   1  #>  0.0148054198783357  0.0150148906719405  0.0155063371912111  0.0157959215559531  #>                   1                   1                   1                   1  #>               0.019  0.0234311041657347  0.0246992394567933  0.0294018010152504  #>                   1                   1                   1                   1  #>  0.0320110500110313  0.0335205551215913   0.033717549782712   0.043362094122218  #>                   1                   1                   1                   1  #>  0.0455668172480073               0.048  0.0513728010684717  0.0569486366831698  #>                   1                   1                   1                   1  #>               0.064               0.066               0.096   0.103118983768858  #>                   1                   1                   1                   1  #>               0.115               0.121                0.14                0.16  #>                   1                   1                   1                   1  #>               0.171               0.189               0.206               0.211  #>                   1                   1                   1                   1  #>               0.215                0.25               0.255               0.311  #>                   1                   1                   1                   1  #>   0.339372734833974               0.398               0.403               0.415  #>                   1                   1                   1                   1  #>               0.474               0.538               0.674               0.743  #>                   1                   1                   1                   1  #>               0.773               0.801               0.858               0.968  #>                   1                   1                   1                   1  #>               0.984               1.055                1.06               1.085  #>                   1                   1                   1                   1  #>               1.091               1.132               1.157               1.173  #>                   1                   1                   1                   1  #>               1.264                1.28               1.327               1.334  #>                   1                   1                   1                   1  #>               1.338               1.345               1.354                1.37  #>                   1                   1                   1                   1  #>               1.467               1.475               1.505                1.65  #>                   1                   1                   1                   1  #>               1.809               1.839    1.85956658420642               1.912  #>                   1                   1                   1                   1  #>               1.946               2.088               2.099    2.17698150922544  #>                   1                   1                   1                   1  #>               2.209               2.214               2.225               2.249  #>                   1                   1                   1                   1  #>               2.266               2.293               2.296               2.327  #>                   1                   1                   1                   1  #>                2.42               2.423               2.661               2.694  #>                   1                   1                   1                   1  #>               2.735               2.801               2.875               2.902  #>                   1                   1                   1                   1  #>               2.932               2.971               3.001               3.006  #>                   1                   1                   1                   1  #>               3.025               3.069    3.07071981293638               3.162  #>                   1                   1                   1                   1  #>               3.168               3.282                3.65               3.855  #>                   1                   1                   1                   1  #>               3.976               4.143    4.20126902716421               4.332  #>                   1                   1                   1                   1  #>    4.53260631947941               4.557               4.694               4.732  #>                   1                   1                   1                   1  #>               4.737               4.778               4.803               4.828  #>                   1                   1                   1                   1  #>               5.014               5.094               5.102               5.247  #>                   1                   1                   1                   1  #>               5.351               5.441               5.691               5.727  #>                   1                   1                   1                   1  #>                5.82               5.943               6.086               6.132  #>                   1                   1                   1                   1  #>               6.206               6.414               6.496               6.521  #>                   1                   1                   1                   1  #>               6.641               6.672               6.809               6.877  #>                   1                   1                   1                   1  #>  #> ------------------------------------------------------------  #> status: 0 #> vf: 1 #> sex: 1 #>  #> no #> 1983 5559 5751  #>    1    1    1  #>  #> wmi #> 1.4 1.6 1.7  #>   1   1   1  #>  #> chf #> 0  #> 3  #>  #> age #> 62.722 67.339 70.435  #>      1      1      1  #>  #> diabetes #> 0  #> 3  #>  #> time #> 7.751 7.828 7.924  #>     1     1     1  #>  #> ------------------------------------------------------------  #> status: 7 #> vf: 1 #> sex: 1 #> NULL #> ------------------------------------------------------------  #> status: 9 #> vf: 1 #> sex: 1 #>  #> no #> 1207 1245 1296 2677 2890 3018 4082 6376 6416  #>    1    1    1    1    1    1    1    1    1  #>  #> wmi #> 0.4 0.7 0.8   1 1.3 1.4 1.9  #>   1   2   1   2   1   1   1  #>  #> chf #> 0 1  #> 1 8  #>  #> age #> 61.431 64.123 68.666 70.303 71.929 72.392 73.491 77.173 81.883  #>      1      1      1      1      1      1      1      1      1  #>  #> diabetes #> 0  #> 9  #>  #> time #> 0.0131395697484259 0.0200790603328496 0.0269757455710787 0.0466610295798164  #>                  1                  1                  1                  1  #> 0.0473181052301079  0.086709953642916  0.245226208094042              1.765  #>                  1                  1                  1                  1  #>              1.773  #>                  1  #>  dtable(sTRACE,status+vf+sex~diabetes|age>60) #> diabetes: 0 #>  #>           sex   0   1 #> status vf             #> 0      0       50  79 #>        1        3   3 #> 7      0        0   3 #>        1        0   0 #> 9      0       68 118 #>        1        5   9 #> ------------------------------------------------------------  #> diabetes: 1 #>  #>           sex  0  1 #> status vf           #> 0      0       2  4 #>        1       0  0 #> 9      0      12 18 #>        1       1  0 dtable(sTRACE,status+vf+sex~diabetes|age>60, flat=FALSE) #> diabetes: 0 #>  #> , , sex = 0 #>  #>       vf #> status   0   1 #>      0  50   3 #>      7   0   0 #>      9  68   5 #>  #> , , sex = 1 #>  #>       vf #> status   0   1 #>      0  79   3 #>      7   3   0 #>      9 118   9 #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #> , , sex = 0 #>  #>       vf #> status  0  1 #>      0  2  0 #>      9 12  1 #>  #> , , sex = 1 #>  #>       vf #> status  0  1 #>      0  4  0 #>      9 18  0 #>   dtable(sTRACE,status+vf+sex~diabetes|age>60, level=1) #> diabetes: 0 #>  #> status #>   0   7   9  #> 135   3 200  #>  #> vf #>   0   1  #> 318  20  #>  #> sex #>   0   1  #> 126 212  #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #> status #>  0  9  #>  6 31  #>  #> vf #>  0  1  #> 36  1  #>  #> sex #>  0  1  #> 15 22  #>  dtable(sTRACE,status+vf+sex~diabetes|age>60, level=2) #> diabetes: 0 #>  #>    status #> vf    0   7   9 #>   0 129   3 186 #>   1   6   0  14 #>  #>    status #> sex   0   7   9 #>   0  53   0  73 #>   1  82   3 127 #>  #>    vf #> sex   0   1 #>   0 118   8 #>   1 200  12 #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #>    status #> vf   0  9 #>   0  6 30 #>   1  0  1 #>  #>    status #> sex  0  9 #>   0  2 13 #>   1  4 18 #>  #>    vf #> sex  0  1 #>   0 14  1 #>   1 22  0 #>   dtable(sTRACE,status+vf+sex~diabetes|age>60, level=2, prop=1, total=TRUE) #> diabetes: 0 #>  #>    status #> vf            0           7           9         Sum #>   0 0.405660377 0.009433962 0.584905660 1.000000000 #>   1 0.300000000 0.000000000 0.700000000 1.000000000 #>  #>    status #> sex          0          7          9        Sum #>   0 0.42063492 0.00000000 0.57936508 1.00000000 #>   1 0.38679245 0.01415094 0.59905660 1.00000000 #>  #>    vf #> sex          0          1        Sum #>   0 0.93650794 0.06349206 1.00000000 #>   1 0.94339623 0.05660377 1.00000000 #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #>    status #> vf          0         9       Sum #>   0 0.1666667 0.8333333 1.0000000 #>   1 0.0000000 1.0000000 1.0000000 #>  #>    status #> sex         0         9       Sum #>   0 0.1333333 0.8666667 1.0000000 #>   1 0.1818182 0.8181818 1.0000000 #>  #>    vf #> sex          0          1        Sum #>   0 0.93333333 0.06666667 1.00000000 #>   1 1.00000000 0.00000000 1.00000000 #>  dtable(sTRACE,status+vf+sex~diabetes|age>60, level=2, prop=2, total=TRUE) #> diabetes: 0 #>  #>      status #> vf             0          7          9 #>   0   0.95555556 1.00000000 0.93000000 #>   1   0.04444444 0.00000000 0.07000000 #>   Sum 1.00000000 1.00000000 1.00000000 #>  #>      status #> sex           0         7         9 #>   0   0.3925926 0.0000000 0.3650000 #>   1   0.6074074 1.0000000 0.6350000 #>   Sum 1.0000000 1.0000000 1.0000000 #>  #>      vf #> sex           0         1 #>   0   0.3710692 0.4000000 #>   1   0.6289308 0.6000000 #>   Sum 1.0000000 1.0000000 #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #>      status #> vf             0          9 #>   0   1.00000000 0.96774194 #>   1   0.00000000 0.03225806 #>   Sum 1.00000000 1.00000000 #>  #>      status #> sex           0         9 #>   0   0.3333333 0.4193548 #>   1   0.6666667 0.5806452 #>   Sum 1.0000000 1.0000000 #>  #>      vf #> sex           0         1 #>   0   0.3888889 1.0000000 #>   1   0.6111111 0.0000000 #>   Sum 1.0000000 1.0000000 #>  dtable(sTRACE,status+vf+sex~diabetes|age>60, level=2, prop=1:2, summary=summary) #> diabetes: 0 #>  #>    status #> vf           0          7          9 #>   0 0.38165680 0.00887574 0.55029586 #>   1 0.01775148 0.00000000 0.04142012 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.003361, df = 2, p-value = 0.9983 #> \tChi-squared approximation may be incorrect #>  #>    status #> sex          0          7          9 #>   0 0.15680473 0.00000000 0.21597633 #>   1 0.24260355 0.00887574 0.37573964 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.006099, df = 2, p-value = 0.997 #> \tChi-squared approximation may be incorrect #>  #>    vf #> sex          0          1 #>   0 0.34911243 0.02366864 #>   1 0.59171598 0.03550296 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.00019928, df = 1, p-value = 0.9887 #> \tChi-squared approximation may be incorrect #>  #> ------------------------------------------------------------  #> diabetes: 1 #>  #>    status #> vf           0          9 #>   0 0.16216216 0.81081081 #>   1 0.00000000 0.02702703 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.005376, df = 1, p-value = 0.9415 #> \tChi-squared approximation may be incorrect #>  #>    status #> sex          0          9 #>   0 0.05405405 0.35135135 #>   1 0.10810811 0.48648649 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.004171, df = 1, p-value = 0.9485 #> \tChi-squared approximation may be incorrect #>  #>    vf #> sex          0          1 #>   0 0.37837838 0.02702703 #>   1 0.59459459 0.00000000 #> Number of cases in table: 1  #> Number of factors: 2  #> Test for independence of all factors: #> \tChisq = 0.04074, df = 1, p-value = 0.84 #> \tChi-squared approximation may be incorrect #>"},{"path":"http://kkholst.github.io/mets/reference/dtransform.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform that allows condition — dtransform","title":"Transform that allows condition — dtransform","text":"Defines new variables condition data frame","code":""},{"path":"http://kkholst.github.io/mets/reference/dtransform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform that allows condition — dtransform","text":"","code":"dtransform(data, ...)"},{"path":"http://kkholst.github.io/mets/reference/dtransform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform that allows condition — dtransform","text":"data data frame ... new variable definitions including possible condition","code":""},{"path":"http://kkholst.github.io/mets/reference/dtransform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform that allows condition — dtransform","text":"","code":"data(mena)  xx <- dtransform(mena,ll=log(agemena)+twinnum)  xx <- dtransform(mena,ll=log(agemena)+twinnum,agemena<15) xx <- dtransform(xx  ,ll=100+agemena,ll2=1000,agemena>15) dsummary(xx,ll+ll2~I(agemena>15)) #> I(agemena > 15): FALSE #>        ll             ll2       #>  Min.   :3.227   Min.   : NA    #>  1st Qu.:3.557   1st Qu.: NA    #>  Median :3.702   Median : NA    #>  Mean   :4.048   Mean   :NaN    #>  3rd Qu.:4.557   3rd Qu.: NA    #>  Max.   :4.707   Max.   : NA    #>                  NA's   :1859   #> ------------------------------------------------------------  #> I(agemena > 15): TRUE #>        ll             ll2       #>  Min.   :115.0   Min.   :1000   #>  1st Qu.:115.3   1st Qu.:1000   #>  Median :115.5   Median :1000   #>  Mean   :115.7   Mean   :1000   #>  3rd Qu.:115.9   3rd Qu.:1000   #>  Max.   :118.2   Max.   :1000"},{"path":"http://kkholst.github.io/mets/reference/easy.binomial.twostage.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","title":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","text":"clusters contain two times, algoritm uses compososite likelihood based pairwise bivariate models.","code":""},{"path":"http://kkholst.github.io/mets/reference/easy.binomial.twostage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","text":"","code":"easy.binomial.twostage(   margbin = NULL,   data = parent.frame(),   method = \"nr\",   response = \"response\",   id = \"id\",   Nit = 60,   detail = 0,   silent = 1,   weights = NULL,   control = list(),   theta = NULL,   theta.formula = NULL,   desnames = NULL,   deshelp = 0,   var.link = 1,   iid = 1,   step = 1,   model = \"plackett\",   marginal.p = NULL,   strata = NULL,   max.clust = NULL,   se.clusters = NULL )"},{"path":"http://kkholst.github.io/mets/reference/easy.binomial.twostage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","text":"margbin Marginal binomial model data data frame method Scoring method response name response variable data frame id name cluster variable data frame Nit Number iterations detail Detail output iterations silent Debug information weights Weights log-likelihood, can used type outcome 2x2 tables. control Optimization arguments theta Starting values variance components theta.formula design depedence, either formula design function desnames names dependence parameters deshelp 1 prints data sets used, design function operates var.link Link function variance iid Calculate ..d. decomposition step Step size model model marginal.p vector marginal probabilities strata strata fitting max.clust max clusters used ..d. decompostion se.clusters clusters iid decomposition roubst standard errors","code":""},{"path":"http://kkholst.github.io/mets/reference/easy.binomial.twostage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","text":"reported standard errors based estimated information likelihood assuming marginals known. gives correct standard errors case plackett distribution (model dependence), incorrect clayton-oakes types model. model often known ALR model. fitting procedures gives correct standard errors due ortogonality fast.","code":""},{"path":"http://kkholst.github.io/mets/reference/easy.binomial.twostage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fits two-stage binomial for describing depdendence in binomial data using marginals that are on logistic form using the binomial.twostage funcion, but call is different and easier and the data manipulation is build into the function. Useful in particular for family design data. — easy.binomial.twostage","text":"","code":"data(twinstut) twinstut0 <- subset(twinstut, tvparnr<4000) twinstut <- twinstut0 twinstut$binstut <- (twinstut$stutter==\"yes\")*1 theta.des <- model.matrix( ~-1+factor(zyg),data=twinstut) margbin <- glm(binstut~factor(sex)+age,data=twinstut,family=binomial()) bin <- binomial.twostage(margbin,data=twinstut,var.link=1,              clusters=twinstut$tvparnr,theta.des=theta.des,detail=0,                    method=\"nr\") summary(bin) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                    theta        se #> factor(zyg)dz -0.2853738 0.9894082 #> factor(zyg)mz  3.3391390 0.5590195 #> factor(zyg)os  0.4920396 0.7634939 #>  #> $or #>               Estimate Std.Err   2.5%  97.5% P-value #> factor(zyg)dz   0.7517  0.7438 -0.706  2.209 0.31216 #> factor(zyg)mz  28.1948 15.7615 -2.697 59.087 0.07364 #> factor(zyg)os   1.6356  1.2488 -0.812  4.083 0.19027 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" lava::estimate(coef=bin$theta,vcov=bin$var.theta,f=function(p) exp(p)) #>      Estimate Std.Err   2.5%  97.5% P-value #> NA     0.7517  0.7438 -0.706  2.209 0.31216 #> NA.1  28.1948 15.7615 -2.697 59.087 0.07364 #> NA.2   1.6356  1.2488 -0.812  4.083 0.19027  twinstut$cage <- scale(twinstut$age) theta.des <- model.matrix( ~-1+factor(zyg)+cage,data=twinstut) bina <- binomial.twostage(margbin,data=twinstut,var.link=1,              clusters=twinstut$tvparnr,theta.des=theta.des,detail=0) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                    theta        se #> factor(zyg)dz -0.2684851 0.9930894 #> factor(zyg)mz  3.4239727 0.5773886 #> factor(zyg)os  0.4778091 0.7628390 #> cage           0.2519096 0.4821619 #>  #> $or #>               Estimate Std.Err     2.5%  97.5% P-value #> factor(zyg)dz   0.7645  0.7593 -0.72357  2.253 0.31395 #> factor(zyg)mz  30.6911 17.7207 -4.04082 65.423 0.08328 #> factor(zyg)os   1.6125  1.2301 -0.79843  4.024 0.18989 #> cage            1.2865  0.6203  0.07073  2.502 0.03808 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  theta.des <- model.matrix( ~-1+factor(zyg)+factor(zyg)*cage,data=twinstut) bina <- binomial.twostage(margbin,data=twinstut,var.link=1,              clusters=twinstut$tvparnr,theta.des=theta.des) summary(bina) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                          theta        se #> factor(zyg)dz      -0.27701246 1.0713974 #> factor(zyg)mz       3.52484148 0.5951743 #> factor(zyg)os       0.48859941 0.7644050 #> cage                0.06420907 3.6225641 #> factor(zyg)mz:cage  0.49441325 3.6865646 #> factor(zyg)os:cage -0.12312666 3.6921748 #>  #> $or #>                    Estimate Std.Err     2.5%  97.5% P-value #> factor(zyg)dz        0.7580  0.8122  -0.8338  2.350 0.35063 #> factor(zyg)mz       33.9484 20.2052  -5.6531 73.550 0.09292 #> factor(zyg)os        1.6300  1.2460  -0.8121  4.072 0.19080 #> cage                 1.0663  3.8628  -6.5046  8.637 0.78251 #> factor(zyg)mz:cage   1.6395  6.0443 -10.2070 13.486 0.78619 #> factor(zyg)os:cage   0.8842  3.2644  -5.5140  7.282 0.78651 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  out <- easy.binomial.twostage(stutter~factor(sex)+age,data=twinstut,                               response=\"binstut\",id=\"tvparnr\",var.link=1,                 theta.formula=~-1+factor(zyg1)) summary(out) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                     theta        se #> factor(zyg1)dz -0.2853738 0.9894082 #> factor(zyg1)mz  3.3391390 0.5590195 #> factor(zyg1)os  0.4920396 0.7634939 #>  #> $or #>                Estimate Std.Err   2.5%  97.5% P-value #> factor(zyg1)dz   0.7517  0.7438 -0.706  2.209 0.31216 #> factor(zyg1)mz  28.1948 15.7615 -2.697 59.087 0.07364 #> factor(zyg1)os   1.6356  1.2488 -0.812  4.083 0.19027 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ## refers to zygosity of first subject in eash pair : zyg1 ## could also use zyg2 (since zyg2=zyg1 within twinpair's)) ## do not run t save time # desfs <- function(x,num1=\"zyg1\",namesdes=c(\"mz\",\"dz\",\"os\")) #     c(x[num1]==\"mz\",x[num1]==\"dz\",x[num1]==\"os\")*1 # #out3 <- easy.binomial.twostage(binstut~factor(sex)+age, #                               data=twinstut, response=\"binstut\",id=\"tvparnr\", #                               var.link=1,theta.formula=desfs, #                               desnames=c(\"mz\",\"dz\",\"os\")) #summary(out3)   ## Reduce Ex.Timings n <- 1000 set.seed(100) dd <- simBinFam(n,beta=0.3) binfam <- fast.reshape(dd,varying=c(\"age\",\"x\",\"y\")) ## mother, father, children  (ordered) head(binfam) #>   id      age x y num #> 1  1 26.68147 1 1   m #> 2  1 32.93535 0 1   f #> 3  1  9.09635 0 1  b1 #> 4  1 11.05365 1 1  b2 #> 5  2 26.06935 1 1   m #> 6  2 28.33452 0 1   f  ########### ########### ########### ########### ########### ########### ####  simple analyses of binomial family data ########### ########### ########### ########### ########### ########### desfs <- function(x,num1=\"num1\",num2=\"num2\") {      pp <- 1*(((x[num1]==\"m\")*(x[num2]==\"f\"))|(x[num1]==\"f\")*(x[num2]==\"m\"))      pc <- (x[num1]==\"m\" | x[num1]==\"f\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1      cc <- (x[num1]==\"b1\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1      c(pp,pc,cc) }  ud <- easy.binomial.twostage(y~+1,data=binfam,      response=\"y\",id=\"id\",      theta.formula=desfs,desnames=c(\"pp\",\"pc\",\"cc\")) summary(ud) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>        theta        se #> pp 0.6561647 0.2508517 #> pc 0.5022035 0.1393882 #> cc 0.8892008 0.2124875 #>  #> $or #>    Estimate Std.Err   2.5% 97.5%   P-value #> pp    1.927  0.4835 0.9798 2.875 6.708e-05 #> pc    1.652  0.2303 1.2009 2.104 7.273e-13 #> cc    2.433  0.5170 1.4198 3.447 2.524e-06 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  udx <- easy.binomial.twostage(y~+x,data=binfam,      response=\"y\",id=\"id\",      theta.formula=desfs,desnames=c(\"pp\",\"pc\",\"cc\")) summary(udx) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>        theta        se #> pp 0.5704932 0.2541853 #> pc 0.5125353 0.1410238 #> cc 0.8840448 0.2153984 #>  #> $or #>    Estimate Std.Err   2.5% 97.5%   P-value #> pp    1.769  0.4497 0.8878 2.651 8.350e-05 #> pc    1.670  0.2354 1.2081 2.131 1.331e-12 #> cc    2.421  0.5214 1.3987 3.443 3.441e-06 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ########### ########### ########### ########### ########### ########### ####  now allowing parent child POR to be different for mother and father ########### ########### ########### ########### ########### ###########  desfsi <- function(x,num1=\"num1\",num2=\"num2\") {     pp <- (x[num1]==\"m\")*(x[num2]==\"f\")*1     mc <- (x[num1]==\"m\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     fc <- (x[num1]==\"f\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     cc <- (x[num1]==\"b1\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     c(pp,mc,fc,cc) }  udi <- easy.binomial.twostage(y~+1,data=binfam,      response=\"y\",id=\"id\",      theta.formula=desfsi,desnames=c(\"pp\",\"mother-child\",\"father-child\",\"cc\")) summary(udi) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                  theta        se #> pp           0.6561647 0.2508517 #> mother-child 0.7897223 0.1715685 #> father-child 0.1784171 0.1944482 #> cc           0.8892008 0.2124875 #>  #> $or #>              Estimate Std.Err   2.5% 97.5%   P-value #> pp              1.927  0.4835 0.9798 2.875 6.708e-05 #> mother-child    2.203  0.3779 1.4621 2.944 5.590e-09 #> father-child    1.195  0.2324 0.7398 1.651 2.707e-07 #> cc              2.433  0.5170 1.4198 3.447 2.524e-06 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ##now looking to see if interactions with age or age influences marginal models ##converting factors to numeric to make all involved covariates numeric ##to use desfai2 rather then desfai that works on binfam  nbinfam <- binfam nbinfam$num <- as.numeric(binfam$num) head(nbinfam) #>   id      age x y num #> 1  1 26.68147 1 1   1 #> 2  1 32.93535 0 1   2 #> 3  1  9.09635 0 1   3 #> 4  1 11.05365 1 1   4 #> 5  2 26.06935 1 1   1 #> 6  2 28.33452 0 1   2  desfsai <- function(x,num1=\"num1\",num2=\"num2\") {     pp <- (x[num1]==\"m\")*(x[num2]==\"f\")*1 ### av age for pp=1 i.e parent pairs     agepp <- ((as.numeric(x[\"age1\"])+as.numeric(x[\"age2\"]))/2-30)*pp     mc <- (x[num1]==\"m\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     fc <- (x[num1]==\"f\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     cc <- (x[num1]==\"b1\")*(x[num2]==\"b1\" | x[num2]==\"b2\")*1     agecc <- ((as.numeric(x[\"age1\"])+as.numeric(x[\"age2\"]))/2-12)*cc     c(pp,agepp,mc,fc,cc,agecc) }  desfsai2 <- function(x,num1=\"num1\",num2=\"num2\") {     pp <- (x[num1]==1)*(x[num2]==2)*1     agepp <- (((x[\"age1\"]+x[\"age2\"]))/2-30)*pp ### av age for pp=1 i.e parent pairs     mc <- (x[num1]==1)*(x[num2]==3 | x[num2]==4)*1     fc <- (x[num1]==2)*(x[num2]==3 | x[num2]==4)*1     cc <- (x[num1]==3)*(x[num2]==3 | x[num2]==4)*1     agecc <- ((x[\"age1\"]+x[\"age2\"])/2-12)*cc ### av age for children     c(pp,agepp,mc,fc,cc,agecc) }  udxai2 <- easy.binomial.twostage(y~+x+age,data=binfam,      response=\"y\",id=\"id\",      theta.formula=desfsai,      desnames=c(\"pp\",\"pp-age\",\"mother-child\",\"father-child\",\"cc\",\"cc-age\")) summary(udxai2) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>                    theta         se #> pp            0.38859947 0.40359602 #> pp-age       -0.05025909 0.08249533 #> mother-child  0.80559724 0.17583430 #> father-child  0.18500933 0.19599386 #> cc            0.85277851 0.22046314 #> cc-age       -0.04548620 0.07212976 #>  #> $or #>              Estimate Std.Err   2.5% 97.5%   P-value #> pp             1.4749 0.59527 0.3082 2.642 1.322e-02 #> pp-age         0.9510 0.07845 0.7972 1.105 8.086e-34 #> mother-child   2.2380 0.39352 1.4667 3.009 1.292e-08 #> father-child   1.2032 0.23583 0.7410 1.665 3.357e-07 #> cc             2.3462 0.51724 1.3324 3.360 5.736e-06 #> cc-age         0.9555 0.06892 0.8204 1.091 1.048e-43 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/reference/evalTerminal.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates piece constant covariates at min(D,t) where D is a terminal event — evalTerminal","title":"Evaluates piece constant covariates at min(D,t) where D is a terminal event — evalTerminal","text":"returns X(min(D,t)) min(D,t) ratio. censored observation 0. use IPCW models implemented.","code":""},{"path":"http://kkholst.github.io/mets/reference/evalTerminal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates piece constant covariates at min(D,t) where D is a terminal event — evalTerminal","text":"","code":"evalTerminal(   formula,   data = data,   death.code = 2,   time = NULL,   marks = NULL,   mark.codes = NULL )"},{"path":"http://kkholst.github.io/mets/reference/evalTerminal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates piece constant covariates at min(D,t) where D is a terminal event — evalTerminal","text":"formula formula 'Event' outcome X evaluate min(D,t) data data frame death.code codes death (terminating event, 2 default) time evaluation marks terminal events add marks*(D <=t ,epsilon \"\" mark.codes)  X(min(D,t)) mark.codes gives death codes add mark value","code":""},{"path":"http://kkholst.github.io/mets/reference/evalTerminal.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Evaluates piece constant covariates at min(D,t) where D is a terminal event — evalTerminal","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/event.split.html","id":null,"dir":"Reference","previous_headings":"","what":"event.split (SurvSplit). — event.split","title":"event.split (SurvSplit). — event.split","text":"contstructs start stop formulation event time data variable data.set. Similar SurvSplit survival package can also split random time given data frame.","code":""},{"path":"http://kkholst.github.io/mets/reference/event.split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"event.split (SurvSplit). — event.split","text":"","code":"event.split(   data,   time = \"time\",   status = \"status\",   cuts = \"cuts\",   name.id = \"id\",   name.start = \"start\",   cens.code = 0,   order.id = TRUE,   time.group = FALSE )"},{"path":"http://kkholst.github.io/mets/reference/event.split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"event.split (SurvSplit). — event.split","text":"data data split time time variable. status status variable. cuts cuts variable numeric cut (one value) name.id name id variable. name.start name start variable data, start can also numeric \"0\" cens.code code censoring. order.id order data id start. time.group make variable \"\".\"cut\" keeps track wether start,stop (1) cut (0).","code":""},{"path":"http://kkholst.github.io/mets/reference/event.split.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"event.split (SurvSplit). — event.split","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/event.split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"event.split (SurvSplit). — event.split","text":"","code":"set.seed(1) d <- data.frame(event=round(5*runif(5),2),start=1:5,time=2*1:5,     status=rbinom(5,1,0.5),x=1:5) d #>   event start time status x #> 1  1.33     1    2      1 1 #> 2  1.86     2    4      1 2 #> 3  2.86     3    6      1 3 #> 4  4.54     4    8      1 4 #> 5  1.01     5   10      0 5  d0 <- event.split(d,cuts=\"event\",name.start=0) d0 #>     event start  time status x start.0 id #> 1    1.33     1  1.33      0 1    0.00  1 #> 1.1  1.33     1  2.00      1 1    1.33  1 #> 2    1.86     2  1.86      0 2    0.00  2 #> 2.1  1.86     2  4.00      1 2    1.86  2 #> 3    2.86     3  2.86      0 3    0.00  3 #> 3.1  2.86     3  6.00      1 3    2.86  3 #> 4    4.54     4  4.54      0 4    0.00  4 #> 4.1  4.54     4  8.00      1 4    4.54  4 #> 5    1.01     5  1.01      0 5    0.00  5 #> 5.1  1.01     5 10.00      0 5    1.01  5  dd <- event.split(d,cuts=\"event\") dd #>     event start  time status x id #> 1    1.33  1.00  1.33      0 1  1 #> 1.1  1.33  1.33  2.00      1 1  1 #> 2    1.86  2.00  4.00      1 2  2 #> 3    2.86  3.00  6.00      1 3  3 #> 4    4.54  4.00  4.54      0 4  4 #> 4.1  4.54  4.54  8.00      1 4  4 #> 5    1.01  5.00 10.00      0 5  5 ddd <- event.split(dd,cuts=3.5) ddd #>     event start  time status x id cut.3.5 #> 1    1.33  1.00  1.33      0 1  1     3.5 #> 1.1  1.33  1.33  2.00      1 1  1     3.5 #> 2    1.86  2.00  3.50      0 2  2     3.5 #> 2.1  1.86  3.50  4.00      1 2  2     3.5 #> 3    2.86  3.00  3.50      0 3  3     3.5 #> 3.1  2.86  3.50  6.00      1 3  3     3.5 #> 4    4.54  4.00  4.54      0 4  4     3.5 #> 4.1  4.54  4.54  8.00      1 4  4     3.5 #> 5    1.01  5.00 10.00      0 5  5     3.5 event.split(ddd,cuts=5.5) #>       event start  time status x id cut.3.5 cut.5.5 #> 1      1.33  1.00  1.33      0 1  1     3.5     5.5 #> 1.1    1.33  1.33  2.00      1 1  1     3.5     5.5 #> 2      1.86  2.00  3.50      0 2  2     3.5     5.5 #> 2.1    1.86  3.50  4.00      1 2  2     3.5     5.5 #> 3      2.86  3.00  3.50      0 3  3     3.5     5.5 #> 3.1    2.86  3.50  5.50      0 3  3     3.5     5.5 #> 3.1.1  2.86  5.50  6.00      1 3  3     3.5     5.5 #> 4      4.54  4.00  4.54      0 4  4     3.5     5.5 #> 4.1    4.54  4.54  5.50      0 4  4     3.5     5.5 #> 4.1.1  4.54  5.50  8.00      1 4  4     3.5     5.5 #> 5      1.01  5.00  5.50      0 5  5     3.5     5.5 #> 5.1    1.01  5.50 10.00      0 5  5     3.5     5.5  ### successive cutting for many values  dd <- d for  (cuts in seq(2,3,by=0.3)) dd <- event.split(dd,cuts=cuts) dd #>         event start time status x cut.2 id cut.2.3 cut.2.6 cut.2.9 #> 1        1.33   1.0  2.0      1 1     2  1     2.3     2.6     2.9 #> 2        1.86   2.0  2.3      0 2     2  2     2.3     2.6     2.9 #> 2.1      1.86   2.3  2.6      0 2     2  2     2.3     2.6     2.9 #> 2.1.1    1.86   2.6  2.9      0 2     2  2     2.3     2.6     2.9 #> 2.1.1.1  1.86   2.9  4.0      1 2     2  2     2.3     2.6     2.9 #> 3        2.86   3.0  6.0      1 3     2  3     2.3     2.6     2.9 #> 4        4.54   4.0  8.0      1 4     2  4     2.3     2.6     2.9 #> 5        1.01   5.0 10.0      0 5     2  5     2.3     2.6     2.9  ########################################################################### ### same but for situation with multiple events along the time-axis ########################################################################### d <- data.frame(event1=1:5+runif(5)*0.5,start=1:5,time=2*1:5,     status=rbinom(5,1,0.5),x=1:5,start0=0) d$event2 <- d$event1+0.2 d$event2[4:5] <- NA  d #>     event1 start time status x start0   event2 #> 1 1.102987     1    2      0 1      0 1.302987 #> 2 2.088278     2    4      1 2      0 2.288278 #> 3 3.343511     3    6      1 3      0 3.543511 #> 4 4.192052     4    8      0 4      0       NA #> 5 5.384921     5   10      1 5      0       NA  d0 <- event.split(d,cuts=\"event1\",name.start=\"start\",time=\"time\",status=\"status\") d0 #>       event1    start      time status x start0   event2 id #> 1   1.102987 1.000000  1.102987      0 1      0 1.302987  1 #> 1.1 1.102987 1.102987  2.000000      0 1      0 1.302987  1 #> 2   2.088278 2.000000  2.088278      0 2      0 2.288278  2 #> 2.1 2.088278 2.088278  4.000000      1 2      0 2.288278  2 #> 3   3.343511 3.000000  3.343511      0 3      0 3.543511  3 #> 3.1 3.343511 3.343511  6.000000      1 3      0 3.543511  3 #> 4   4.192052 4.000000  4.192052      0 4      0       NA  4 #> 4.1 4.192052 4.192052  8.000000      0 4      0       NA  4 #> 5   5.384921 5.000000  5.384921      0 5      0       NA  5 #> 5.1 5.384921 5.384921 10.000000      1 5      0       NA  5 ### d00 <- event.split(d0,cuts=\"event2\",name.start=\"start\",time=\"time\",status=\"status\") d00 #>         event1    start      time status x start0   event2 id #> 1     1.102987 1.000000  1.102987      0 1      0 1.302987  1 #> 1.1   1.102987 1.102987  1.302987      0 1      0 1.302987  1 #> 1.1.1 1.102987 1.302987  2.000000      0 1      0 1.302987  1 #> 2     2.088278 2.000000  2.088278      0 2      0 2.288278  2 #> 2.1   2.088278 2.088278  2.288278      0 2      0 2.288278  2 #> 2.1.1 2.088278 2.288278  4.000000      1 2      0 2.288278  2 #> 3     3.343511 3.000000  3.343511      0 3      0 3.543511  3 #> 3.1   3.343511 3.343511  3.543511      0 3      0 3.543511  3 #> 3.1.1 3.343511 3.543511  6.000000      1 3      0 3.543511  3 #> 4     4.192052 4.000000  4.192052      0 4      0       NA  4 #> 4.1   4.192052 4.192052  8.000000      0 4      0       NA  4 #> 5     5.384921 5.000000  5.384921      0 5      0       NA  5 #> 5.1   5.384921 5.384921 10.000000      1 5      0       NA  5"},{"path":"http://kkholst.github.io/mets/reference/eventpois.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract survival estimates from lifetable analysis — eventpois","title":"Extract survival estimates from lifetable analysis — eventpois","text":"Summary survival analyses via 'lifetable' function","code":""},{"path":"http://kkholst.github.io/mets/reference/eventpois.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract survival estimates from lifetable analysis — eventpois","text":"","code":"eventpois(   object,   ...,   timevar,   time,   int.len,   confint = FALSE,   level = 0.95,   individual = FALSE,   length.out = 25 )"},{"path":"http://kkholst.github.io/mets/reference/eventpois.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract survival estimates from lifetable analysis — eventpois","text":"object glm object (poisson regression) ... Contrast arguments timevar Name time variable time Time points (optional) int.len Time interval length (optional) confint TRUE confidence limits supplied level Level confidence limits individual Individual predictions length.Length time vector","code":""},{"path":"http://kkholst.github.io/mets/reference/eventpois.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract survival estimates from lifetable analysis — eventpois","text":"Summary survival analyses via 'lifetable' function","code":""},{"path":"http://kkholst.github.io/mets/reference/eventpois.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract survival estimates from lifetable analysis — eventpois","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":null,"dir":"Reference","previous_headings":"","what":"Finds all pairs within a cluster (family) — familycluster.index","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"Finds pairs within cluster (family)","code":""},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"","code":"familycluster.index(clusters, index.type = FALSE, num = NULL, Rindex = 1)"},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"clusters list indeces index.type argument cluster index num num Rindex index starts 1 R, 0 C","code":""},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"Cluster indeces","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"Klaus Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/familycluster.index.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Finds all pairs within a cluster (family) — familycluster.index","text":"","code":"i<-c(1,1,2,2,1,3) d<- familycluster.index(i) print(d) #> $familypairindex #> [1] 1 2 1 5 2 5 3 4 #>  #> $subfamilyindex #> [1] 0 0 1 1 2 2 3 3 #>  #> $pairs #>      [,1] [,2] #> [1,]    1    2 #> [2,]    1    5 #> [3,]    2    5 #> [4,]    3    4 #>  #> $clusters #> [1] 1 1 1 2 #>"},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":null,"dir":"Reference","previous_headings":"","what":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"second column pairs probands first column related subjects","code":""},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"","code":"familyclusterWithProbands.index(   clusters,   probands,   index.type = FALSE,   num = NULL,   Rindex = 1 )"},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"clusters list indeces giving clusters (families) probands list 0,1 1 specifices subjects probands index.type argument passed functions num argument passed functions Rindex index starts 1, C 0","code":""},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"Cluster indeces","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"Klaus Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/familyclusterWithProbands.index.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Finds all pairs within a cluster (famly) with the proband (case/control) — familyclusterWithProbands.index","text":"","code":"i<-c(1,1,2,2,1,3) p<-c(1,0,0,1,0,1) d<- familyclusterWithProbands.index(i,p) print(d) #> $familypairindex #> [1] 2 1 5 1 3 4 #>  #> $subfamilyindex #> [1] 0 0 1 1 3 3 #>  #> $pairs #>      [,1] [,2] #> [1,]    2    1 #> [2,]    5    1 #> [3,]    3    4 #>  #> $clusters #> [1] 1 1 2 #>"},{"path":"http://kkholst.github.io/mets/reference/fast.approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast approximation — fast.approx","title":"Fast approximation — fast.approx","text":"Fast approximation","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast approximation — fast.approx","text":"","code":"fast.approx(   time,   new.time,   equal = FALSE,   type = c(\"nearest\", \"right\", \"left\"),   sorted = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/fast.approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast approximation — fast.approx","text":"time Original ordered time points new.time New time points equal TRUE list returned additional element type Type matching, nearest index, nearest greater equal (right), number elements smaller y otherwise closest value new.time returned. sorted Set true new.time already sorted ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.approx.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast approximation — fast.approx","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.approx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast approximation — fast.approx","text":"","code":"id <- c(1,1,2,2,7,7,10,10) fast.approx(unique(id),id) #> [1] 1 1 2 2 3 3 4 4  t <- 0:6 n <- c(-1,0,0.1,0.9,1,1.1,1.2,6,6.5) fast.approx(t,n,type=\"left\") #> [1] 0 1 1 1 2 2 2 7 7"},{"path":"http://kkholst.github.io/mets/reference/fast.pattern.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast pattern — fast.pattern","title":"Fast pattern — fast.pattern","text":"Fast pattern","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.pattern.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast pattern — fast.pattern","text":"","code":"fast.pattern(x, y, categories = 2, ...)"},{"path":"http://kkholst.github.io/mets/reference/fast.pattern.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast pattern — fast.pattern","text":"x Matrix (binary) patterns. Optionally y also passed argument, pattern matrix defined elements agreeing two matrices. y Optional matrix argument dimensions x (see ) categories Default 2 (binary) ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.pattern.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast pattern — fast.pattern","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.pattern.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast pattern — fast.pattern","text":"","code":"X <- matrix(rbinom(100,1,0.5),ncol=4) fast.pattern(X) #> $pattern #>       [,1] [,2] [,3] [,4] #>  [1,]    1    0    1    1 #>  [2,]    0    0    1    1 #>  [3,]    0    0    0    0 #>  [4,]    1    0    1    0 #>  [5,]    0    1    0    0 #>  [6,]    0    1    0    1 #>  [7,]    1    1    0    1 #>  [8,]    1    0    0    0 #>  [9,]    1    0    0    1 #> [10,]    1    1    1    0 #> [11,]    1    1    1    1 #> [12,]    0    1    1    0 #>  #> $group #>       [,1] #>  [1,]    0 #>  [2,]    1 #>  [3,]    2 #>  [4,]    3 #>  [5,]    4 #>  [6,]    5 #>  [7,]    0 #>  [8,]    6 #>  [9,]    0 #> [10,]    2 #> [11,]    7 #> [12,]    4 #> [13,]    7 #> [14,]    8 #> [15,]    9 #> [16,]    3 #> [17,]   10 #> [18,]    0 #> [19,]    4 #> [20,]    2 #> [21,]    3 #> [22,]    0 #> [23,]   11 #> [24,]    6 #> [25,]    2 #>   X <- matrix(rbinom(100,3,0.5),ncol=4) fast.pattern(X,categories=4) #> $pattern #>       [,1] [,2] [,3] [,4] #>  [1,]    1    1    2    1 #>  [2,]    2    2    3    1 #>  [3,]    2    1    1    2 #>  [4,]    0    2    1    2 #>  [5,]    0    0    3    1 #>  [6,]    2    1    2    1 #>  [7,]    3    1    3    2 #>  [8,]    2    1    1    3 #>  [9,]    2    3    2    0 #> [10,]    2    1    2    2 #> [11,]    3    2    3    3 #> [12,]    2    3    2    2 #> [13,]    2    0    1    2 #> [14,]    1    1    0    3 #> [15,]    1    2    3    3 #> [16,]    2    1    1    1 #> [17,]    1    2    2    1 #> [18,]    1    2    0    1 #> [19,]    2    2    2    1 #> [20,]    0    1    1    2 #> [21,]    2    1    2    3 #> [22,]    2    3    1    2 #> [23,]    2    2    1    1 #> [24,]    1    2    2    0 #>  #> $group #>       [,1] #>  [1,]    0 #>  [2,]    1 #>  [3,]    2 #>  [4,]    3 #>  [5,]    4 #>  [6,]    5 #>  [7,]    6 #>  [8,]    7 #>  [9,]    8 #> [10,]    9 #> [11,]   10 #> [12,]   11 #> [13,]    5 #> [14,]   12 #> [15,]   13 #> [16,]   14 #> [17,]   15 #> [18,]   16 #> [19,]   17 #> [20,]   18 #> [21,]   19 #> [22,]   20 #> [23,]   21 #> [24,]   22 #> [25,]   23 #>"},{"path":"http://kkholst.github.io/mets/reference/fast.reshape.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast reshape — fast.reshape","title":"Fast reshape — fast.reshape","text":"Fast reshape/tranpose data","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.reshape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast reshape — fast.reshape","text":"","code":"fast.reshape(   data,   varying,   id,   num,   sep = \"\",   keep,   idname = \"id\",   numname = \"num\",   factor = FALSE,   idcombine = TRUE,   labelnum = FALSE,   labels,   regex = mets.options()$regex,   dropid = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/fast.reshape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast reshape — fast.reshape","text":"data data.frame matrix varying Vector prefix-names time varying variables. Optional Long->Wide reshaping. id id-variable. omitted reshape Wide->Long. num Optional number/time variable sep String seperating prefix-name number/time keep Vector column names keep idname Name id-variable (Wide->Long) numname Name number-variable (Wide->Long) factor true factors kept (otherwise treated character) idcombine TRUE id vector several variables, unique id combined variables. Otherwise first variable used identifier. labelnum TRUE varying variables wide format (going long->wide) labeled 1,2,3,... otherwise use 'num' variable. long-format (going wide->long) varying variables matching 'varying' prefix selected postfix number. labels Optional labels number variable regex Use regular expressions dropid Drop id long format (default FALSE) ... Optional additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.reshape.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast reshape — fast.reshape","text":"Thomas Scheike, Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/fast.reshape.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast reshape — fast.reshape","text":"","code":"m <- lava::lvm(c(y1,y2,y3,y4)~x) d <- lava::sim(m,5) d #>           y1         y2         y3           y4           x #> 1 -1.9292578 -2.0424073 -0.8237835 -1.476289383 -1.53644982 #> 2 -0.6209690  1.0420627 -0.3745405 -0.889870613 -0.30097613 #> 3 -0.8073932 -0.7428593 -0.5659141  0.003216288 -0.52827990 #> 4 -0.1579064 -0.8316513 -1.3337553 -2.170488862 -0.65209478 #> 5 -0.2342273 -0.1570875 -0.3811671  0.249661083 -0.05689678 fast.reshape(d,\"y\") #>              x            y id num #> 1  -1.53644982 -1.929257753  1   1 #> 2  -1.53644982 -2.042407286  1   2 #> 3  -1.53644982 -0.823783516  1   3 #> 4  -1.53644982 -1.476289383  1   4 #> 5  -0.30097613 -0.620968995  2   1 #> 6  -0.30097613  1.042062698  2   2 #> 7  -0.30097613 -0.374540531  2   3 #> 8  -0.30097613 -0.889870613  2   4 #> 9  -0.52827990 -0.807393207  3   1 #> 10 -0.52827990 -0.742859313  3   2 #> 11 -0.52827990 -0.565914076  3   3 #> 12 -0.52827990  0.003216288  3   4 #> 13 -0.65209478 -0.157906449  4   1 #> 14 -0.65209478 -0.831651311  4   2 #> 15 -0.65209478 -1.333755259  4   3 #> 16 -0.65209478 -2.170488862  4   4 #> 17 -0.05689678 -0.234227260  5   1 #> 18 -0.05689678 -0.157087519  5   2 #> 19 -0.05689678 -0.381167050  5   3 #> 20 -0.05689678  0.249661083  5   4 fast.reshape(fast.reshape(d,\"y\"),id=\"id\") #>            x1         y1 id num1          x2         y2 num2          x3 #> 1 -1.53644982 -1.9292578  1    1 -1.53644982 -2.0424073    2 -1.53644982 #> 2 -0.30097613 -0.6209690  2    1 -0.30097613  1.0420627    2 -0.30097613 #> 3 -0.52827990 -0.8073932  3    1 -0.52827990 -0.7428593    2 -0.52827990 #> 4 -0.65209478 -0.1579064  4    1 -0.65209478 -0.8316513    2 -0.65209478 #> 5 -0.05689678 -0.2342273  5    1 -0.05689678 -0.1570875    2 -0.05689678 #>           y3 num3          x4           y4 num4 #> 1 -0.8237835    3 -1.53644982 -1.476289383    4 #> 2 -0.3745405    3 -0.30097613 -0.889870613    4 #> 3 -0.5659141    3 -0.52827990  0.003216288    4 #> 4 -1.3337553    3 -0.65209478 -2.170488862    4 #> 5 -0.3811671    3 -0.05689678  0.249661083    4  ##### From wide-format (dd <- fast.reshape(d,\"y\")) #>              x            y id num #> 1  -1.53644982 -1.929257753  1   1 #> 2  -1.53644982 -2.042407286  1   2 #> 3  -1.53644982 -0.823783516  1   3 #> 4  -1.53644982 -1.476289383  1   4 #> 5  -0.30097613 -0.620968995  2   1 #> 6  -0.30097613  1.042062698  2   2 #> 7  -0.30097613 -0.374540531  2   3 #> 8  -0.30097613 -0.889870613  2   4 #> 9  -0.52827990 -0.807393207  3   1 #> 10 -0.52827990 -0.742859313  3   2 #> 11 -0.52827990 -0.565914076  3   3 #> 12 -0.52827990  0.003216288  3   4 #> 13 -0.65209478 -0.157906449  4   1 #> 14 -0.65209478 -0.831651311  4   2 #> 15 -0.65209478 -1.333755259  4   3 #> 16 -0.65209478 -2.170488862  4   4 #> 17 -0.05689678 -0.234227260  5   1 #> 18 -0.05689678 -0.157087519  5   2 #> 19 -0.05689678 -0.381167050  5   3 #> 20 -0.05689678  0.249661083  5   4 ## Same with explicit setting new id and number variable/column names ## and seperator \"\" (default) and dropping x fast.reshape(d,\"y\",idname=\"a\",timevar=\"b\",sep=\"\",keep=c()) #>               y a num #> 1  -1.929257753 1   1 #> 2  -2.042407286 1   2 #> 3  -0.823783516 1   3 #> 4  -1.476289383 1   4 #> 5  -0.620968995 2   1 #> 6   1.042062698 2   2 #> 7  -0.374540531 2   3 #> 8  -0.889870613 2   4 #> 9  -0.807393207 3   1 #> 10 -0.742859313 3   2 #> 11 -0.565914076 3   3 #> 12  0.003216288 3   4 #> 13 -0.157906449 4   1 #> 14 -0.831651311 4   2 #> 15 -1.333755259 4   3 #> 16 -2.170488862 4   4 #> 17 -0.234227260 5   1 #> 18 -0.157087519 5   2 #> 19 -0.381167050 5   3 #> 20  0.249661083 5   4 ## Same with 'reshape' list-syntax fast.reshape(d,list(c(\"y1\",\"y2\",\"y3\",\"y4\")),labelnum=TRUE) #>              x           y1 id num #> 1  -1.53644982 -1.929257753  1   1 #> 2  -1.53644982 -2.042407286  1   2 #> 3  -1.53644982 -0.823783516  1   3 #> 4  -1.53644982 -1.476289383  1   4 #> 5  -0.30097613 -0.620968995  2   1 #> 6  -0.30097613  1.042062698  2   2 #> 7  -0.30097613 -0.374540531  2   3 #> 8  -0.30097613 -0.889870613  2   4 #> 9  -0.52827990 -0.807393207  3   1 #> 10 -0.52827990 -0.742859313  3   2 #> 11 -0.52827990 -0.565914076  3   3 #> 12 -0.52827990  0.003216288  3   4 #> 13 -0.65209478 -0.157906449  4   1 #> 14 -0.65209478 -0.831651311  4   2 #> 15 -0.65209478 -1.333755259  4   3 #> 16 -0.65209478 -2.170488862  4   4 #> 17 -0.05689678 -0.234227260  5   1 #> 18 -0.05689678 -0.157087519  5   2 #> 19 -0.05689678 -0.381167050  5   3 #> 20 -0.05689678  0.249661083  5   4  ##### From long-format fast.reshape(dd,id=\"id\") #>            x1         y1 id num1          x2         y2 num2          x3 #> 1 -1.53644982 -1.9292578  1    1 -1.53644982 -2.0424073    2 -1.53644982 #> 2 -0.30097613 -0.6209690  2    1 -0.30097613  1.0420627    2 -0.30097613 #> 3 -0.52827990 -0.8073932  3    1 -0.52827990 -0.7428593    2 -0.52827990 #> 4 -0.65209478 -0.1579064  4    1 -0.65209478 -0.8316513    2 -0.65209478 #> 5 -0.05689678 -0.2342273  5    1 -0.05689678 -0.1570875    2 -0.05689678 #>           y3 num3          x4           y4 num4 #> 1 -0.8237835    3 -1.53644982 -1.476289383    4 #> 2 -0.3745405    3 -0.30097613 -0.889870613    4 #> 3 -0.5659141    3 -0.52827990  0.003216288    4 #> 4 -1.3337553    3 -0.65209478 -2.170488862    4 #> 5 -0.3811671    3 -0.05689678  0.249661083    4 ## Restrict set up within-cluster varying variables fast.reshape(dd,\"y\",id=\"id\") #>             x         y1 id num         y2         y3           y4 #> 1 -1.53644982 -1.9292578  1   1 -2.0424073 -0.8237835 -1.476289383 #> 2 -0.30097613 -0.6209690  2   1  1.0420627 -0.3745405 -0.889870613 #> 3 -0.52827990 -0.8073932  3   1 -0.7428593 -0.5659141  0.003216288 #> 4 -0.65209478 -0.1579064  4   1 -0.8316513 -1.3337553 -2.170488862 #> 5 -0.05689678 -0.2342273  5   1 -0.1570875 -0.3811671  0.249661083 fast.reshape(dd,\"y\",id=\"id\",keep=\"x\",sep=\".\") #>             x        y.1 id        y.2        y.3          y.4 #> 1 -1.53644982 -1.9292578  1 -2.0424073 -0.8237835 -1.476289383 #> 2 -0.30097613 -0.6209690  2  1.0420627 -0.3745405 -0.889870613 #> 3 -0.52827990 -0.8073932  3 -0.7428593 -0.5659141  0.003216288 #> 4 -0.65209478 -0.1579064  4 -0.8316513 -1.3337553 -2.170488862 #> 5 -0.05689678 -0.2342273  5 -0.1570875 -0.3811671  0.249661083  ##### x <- data.frame(id=c(5,5,6,6,7),y=1:5,x=1:5,tv=c(1,2,2,1,2)) x #>   id y x tv #> 1  5 1 1  1 #> 2  5 2 2  2 #> 3  6 3 3  2 #> 4  6 4 4  1 #> 5  7 5 5  2 (xw <- fast.reshape(x,id=\"id\")) #>   id y1 x1 tv1 y2 x2 tv2 #> 1  5  1  1   1  2  2   2 #> 2  6  3  3   2  4  4   1 #> 3  7  5  5   2 NA NA  NA (xl <- fast.reshape(xw,c(\"y\",\"x\"),idname=\"id2\",keep=c())) #>    y  x id2 num #> 1  1  1   1   1 #> 2  2  2   1   2 #> 3  3  3   2   1 #> 4  4  4   2   2 #> 5  5  5   3   1 #> 6 NA NA   3   2 (xl <- fast.reshape(xw,c(\"y\",\"x\",\"tv\"))) #>   id  y  x tv num #> 1  5  1  1  1   1 #> 2  5  2  2  2   2 #> 3  6  3  3  2   1 #> 4  6  4  4  1   2 #> 5  7  5  5  2   1 #> 6  7 NA NA NA   2 (xw2 <- fast.reshape(xl,id=\"id\",num=\"num\")) #>   id y1 x1 tv1 y2 x2 tv2 #> 1  5  1  1   1  2  2   2 #> 2  6  3  3   2  4  4   1 #> 3  7  5  5   2 NA NA  NA fast.reshape(xw2,c(\"y\",\"x\"),idname=\"id\") #>   id tv1 tv2  y  x num #> 1  5   1   2  1  1   1 #> 2  5   1   2  2  2   2 #> 3  6   2   1  3  3   1 #> 4  6   2   1  4  4   2 #> 5  7   2  NA  5  5   1 #> 6  7   2  NA NA NA   2  ### more generally: ### varying=list(c(\"ym\",\"yf\",\"yb1\",\"yb2\"), c(\"zm\",\"zf\",\"zb1\",\"zb2\")) ### varying=list(c(\"ym\",\"yf\",\"yb1\",\"yb2\")))  ##### Family cluster example d <- mets:::simBinFam(3) d #>       agem     agef    ageb1     ageb2 xm xf xb1 xb2 ym yf yb1 yb2 id #> 1 22.93606 23.91995 5.771517  7.924428  1  1   0   1  1  1   1   1  1 #> 2 23.99351 30.75988 8.636968 11.665460  1  1   1   1  1  0   0   0  2 #> 3 28.12132 29.26337 9.425925 11.233806  1  1   1   1  1  1   1   0  3 fast.reshape(d,var=\"y\") #>        agem     agef    ageb1     ageb2 xm xf xb1 xb2 id y num #> 1  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1 1   m #> 2  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1 1   f #> 3  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1 1  b1 #> 4  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1 1  b2 #> 5  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2 1   m #> 6  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2 0   f #> 7  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2 0  b1 #> 8  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2 0  b2 #> 9  28.12132 29.26337 9.425925 11.233806  1  1   1   1  3 1   m #> 10 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3 1   f #> 11 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3 1  b1 #> 12 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3 0  b2 fast.reshape(d,varying=list(c(\"ym\",\"yf\",\"yb1\",\"yb2\"))) #>        agem     agef    ageb1     ageb2 xm xf xb1 xb2 id ym num #> 1  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1  1  ym #> 2  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1  1  yf #> 3  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1  1 yb1 #> 4  22.93606 23.91995 5.771517  7.924428  1  1   0   1  1  1 yb2 #> 5  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2  1  ym #> 6  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2  0  yf #> 7  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2  0 yb1 #> 8  23.99351 30.75988 8.636968 11.665460  1  1   1   1  2  0 yb2 #> 9  28.12132 29.26337 9.425925 11.233806  1  1   1   1  3  1  ym #> 10 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3  1  yf #> 11 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3  1 yb1 #> 12 28.12132 29.26337 9.425925 11.233806  1  1   1   1  3  0 yb2  d <- lava::sim(lava::lvm(~y1+y2+ya),10) d #>             y1         y2         ya #> 1  -0.07715294 -1.4672500  0.8303732 #> 2  -0.33400084  0.5210227 -1.2080828 #> 3  -0.03472603 -0.1587546 -1.0479844 #> 4   0.78763961  1.4645873  1.4411577 #> 5   2.07524501 -0.7660820 -1.0158475 #> 6   1.02739244 -0.4302118  0.4119747 #> 7   1.20790840 -0.9261095 -0.3810761 #> 8  -1.23132342 -0.1771040  0.4094018 #> 9   0.98389557  0.4020118  1.6888733 #> 10  0.21992480 -0.7317482  1.5865884 (dd <- fast.reshape(d,\"y\")) #>              y id num #> 1  -0.07715294  1   1 #> 2  -1.46725003  1   2 #> 3   0.83037317  1   a #> 4  -0.33400084  2   1 #> 5   0.52102274  2   2 #> 6  -1.20808279  2   a #> 7  -0.03472603  3   1 #> 8  -0.15875460  3   2 #> 9  -1.04798441  3   a #> 10  0.78763961  4   1 #> 11  1.46458731  4   2 #> 12  1.44115771  4   a #> 13  2.07524501  5   1 #> 14 -0.76608200  5   2 #> 15 -1.01584747  5   a #> 16  1.02739244  6   1 #> 17 -0.43021175  6   2 #> 18  0.41197471  6   a #> 19  1.20790840  7   1 #> 20 -0.92610950  7   2 #> 21 -0.38107605  7   a #> 22 -1.23132342  8   1 #> 23 -0.17710396  8   2 #> 24  0.40940184  8   a #> 25  0.98389557  9   1 #> 26  0.40201178  9   2 #> 27  1.68887329  9   a #> 28  0.21992480 10   1 #> 29 -0.73174817 10   2 #> 30  1.58658843 10   a fast.reshape(d,\"y\",labelnum=TRUE) #>            ya           y id num #> 1   0.8303732 -0.07715294  1   1 #> 2   0.8303732 -1.46725003  1   2 #> 3  -1.2080828 -0.33400084  2   1 #> 4  -1.2080828  0.52102274  2   2 #> 5  -1.0479844 -0.03472603  3   1 #> 6  -1.0479844 -0.15875460  3   2 #> 7   1.4411577  0.78763961  4   1 #> 8   1.4411577  1.46458731  4   2 #> 9  -1.0158475  2.07524501  5   1 #> 10 -1.0158475 -0.76608200  5   2 #> 11  0.4119747  1.02739244  6   1 #> 12  0.4119747 -0.43021175  6   2 #> 13 -0.3810761  1.20790840  7   1 #> 14 -0.3810761 -0.92610950  7   2 #> 15  0.4094018 -1.23132342  8   1 #> 16  0.4094018 -0.17710396  8   2 #> 17  1.6888733  0.98389557  9   1 #> 18  1.6888733  0.40201178  9   2 #> 19  1.5865884  0.21992480 10   1 #> 20  1.5865884 -0.73174817 10   2 fast.reshape(dd,id=\"id\",num=\"num\") #>             y1 id         y2         ya #> 1  -0.07715294  1 -1.4672500  0.8303732 #> 2  -0.33400084  2  0.5210227 -1.2080828 #> 3  -0.03472603  3 -0.1587546 -1.0479844 #> 4   0.78763961  4  1.4645873  1.4411577 #> 5   2.07524501  5 -0.7660820 -1.0158475 #> 6   1.02739244  6 -0.4302118  0.4119747 #> 7   1.20790840  7 -0.9261095 -0.3810761 #> 8  -1.23132342  8 -0.1771040  0.4094018 #> 9   0.98389557  9  0.4020118  1.6888733 #> 10  0.21992480 10 -0.7317482  1.5865884 fast.reshape(dd,id=\"id\",num=\"num\",labelnum=TRUE) #>             y1 id         y2         y3 #> 1  -0.07715294  1 -1.4672500  0.8303732 #> 2  -0.33400084  2  0.5210227 -1.2080828 #> 3  -0.03472603  3 -0.1587546 -1.0479844 #> 4   0.78763961  4  1.4645873  1.4411577 #> 5   2.07524501  5 -0.7660820 -1.0158475 #> 6   1.02739244  6 -0.4302118  0.4119747 #> 7   1.20790840  7 -0.9261095 -0.3810761 #> 8  -1.23132342  8 -0.1771040  0.4094018 #> 9   0.98389557  9  0.4020118  1.6888733 #> 10  0.21992480 10 -0.7317482  1.5865884 fast.reshape(d,c(a=\"y\"),labelnum=TRUE) ## New column name #>            ya           a id num #> 1   0.8303732 -0.07715294  1   1 #> 2   0.8303732 -1.46725003  1   2 #> 3  -1.2080828 -0.33400084  2   1 #> 4  -1.2080828  0.52102274  2   2 #> 5  -1.0479844 -0.03472603  3   1 #> 6  -1.0479844 -0.15875460  3   2 #> 7   1.4411577  0.78763961  4   1 #> 8   1.4411577  1.46458731  4   2 #> 9  -1.0158475  2.07524501  5   1 #> 10 -1.0158475 -0.76608200  5   2 #> 11  0.4119747  1.02739244  6   1 #> 12  0.4119747 -0.43021175  6   2 #> 13 -0.3810761  1.20790840  7   1 #> 14 -0.3810761 -0.92610950  7   2 #> 15  0.4094018 -1.23132342  8   1 #> 16  0.4094018 -0.17710396  8   2 #> 17  1.6888733  0.98389557  9   1 #> 18  1.6888733  0.40201178  9   2 #> 19  1.5865884  0.21992480 10   1 #> 20  1.5865884 -0.73174817 10   2   ##### Unbalanced data m <- lava::lvm(c(y1,y2,y3,y4)~ x+z1+z3+z5) d <- lava::sim(m,3) d #>          y1        y2        y3         y4          x          z1          z3 #> 1  0.676638  1.674612  1.517654  0.6072991  1.5197450  0.64224131  0.00213186 #> 2 -1.465844  1.360719  0.655016 -0.5508160 -0.3087406 -0.04470914 -0.63030033 #> 3 -1.160947 -3.672008 -3.237914 -2.6707705 -1.2532898 -1.73321841 -0.34096858 #>          z5 #> 1 -1.156572 #> 2  1.803142 #> 3 -0.331132 fast.reshape(d,c(\"y\",\"z\")) #>             x          y           z id num #> 1   1.5197450  0.6766380  0.64224131  1   1 #> 2   1.5197450  1.6746120          NA  1   2 #> 3   1.5197450  1.5176543  0.00213186  1   3 #> 4   1.5197450  0.6072991          NA  1   4 #> 5   1.5197450         NA -1.15657236  1   5 #> 6  -0.3087406 -1.4658437 -0.04470914  2   1 #> 7  -0.3087406  1.3607192          NA  2   2 #> 8  -0.3087406  0.6550160 -0.63030033  2   3 #> 9  -0.3087406 -0.5508160          NA  2   4 #> 10 -0.3087406         NA  1.80314191  2   5 #> 11 -1.2532898 -1.1609472 -1.73321841  3   1 #> 12 -1.2532898 -3.6720083          NA  3   2 #> 13 -1.2532898 -3.2379141 -0.34096858  3   3 #> 14 -1.2532898 -2.6707705          NA  3   4 #> 15 -1.2532898         NA -0.33113204  3   5  ##### not-varying syntax: fast.reshape(d,-c(\"x\")) #>             x          y           z id num #> 1   1.5197450  0.6766380  0.64224131  1   1 #> 2   1.5197450  1.6746120          NA  1   2 #> 3   1.5197450  1.5176543  0.00213186  1   3 #> 4   1.5197450  0.6072991          NA  1   4 #> 5   1.5197450         NA -1.15657236  1   5 #> 6  -0.3087406 -1.4658437 -0.04470914  2   1 #> 7  -0.3087406  1.3607192          NA  2   2 #> 8  -0.3087406  0.6550160 -0.63030033  2   3 #> 9  -0.3087406 -0.5508160          NA  2   4 #> 10 -0.3087406         NA  1.80314191  2   5 #> 11 -1.2532898 -1.1609472 -1.73321841  3   1 #> 12 -1.2532898 -3.6720083          NA  3   2 #> 13 -1.2532898 -3.2379141 -0.34096858  3   3 #> 14 -1.2532898 -2.6707705          NA  3   4 #> 15 -1.2532898         NA -0.33113204  3   5  ##### Automatically define varying variables from trailing digits fast.reshape(d) #>             x          y           z id num #> 1   1.5197450  0.6766380  0.64224131  1   1 #> 2   1.5197450  1.6746120          NA  1   2 #> 3   1.5197450  1.5176543  0.00213186  1   3 #> 4   1.5197450  0.6072991          NA  1   4 #> 5   1.5197450         NA -1.15657236  1   5 #> 6  -0.3087406 -1.4658437 -0.04470914  2   1 #> 7  -0.3087406  1.3607192          NA  2   2 #> 8  -0.3087406  0.6550160 -0.63030033  2   3 #> 9  -0.3087406 -0.5508160          NA  2   4 #> 10 -0.3087406         NA  1.80314191  2   5 #> 11 -1.2532898 -1.1609472 -1.73321841  3   1 #> 12 -1.2532898 -3.6720083          NA  3   2 #> 13 -1.2532898 -3.2379141 -0.34096858  3   3 #> 14 -1.2532898 -2.6707705          NA  3   4 #> 15 -1.2532898         NA -0.33113204  3   5  ##### Prostate cancer example data(prt) head(prtw <- fast.reshape(prt,\"cancer\",id=\"id\")) #>    country      time status zyg id cancer1 cancer2 #> 31 Denmark  96.98833      1  DZ  1       0       0 #> 39 Denmark  68.04498      1  DZ  3       0       0 #> 51 Denmark  78.78068      1  DZ  5       0       0 #> 70 Denmark 100.95488      1  MZ  9       0       0 #> 83 Denmark 104.55035      1  DZ 12       0       1 #> 95 Denmark  95.65324      1  DZ 15       0       0 ftable(cancer1~cancer2,data=prtw) #>         cancer1     0     1 #> cancer2                     #> 0               13405   349 #> 1                 362   106 rm(prtw)"},{"path":"http://kkholst.github.io/mets/reference/glm_IPTW.html","id":null,"dir":"Reference","previous_headings":"","what":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","title":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","text":"Fits GLM model treatment weights $$ w()= \\sum_a (=)/P(=|X) $$, computes standard errors via influence functions returned IID argument. Propensity scores fitted using either logistic regression (glm) multinomial model (mlogit) two categories treatment. treatment needs factor identified rhs \"treat.model\".","code":""},{"path":"http://kkholst.github.io/mets/reference/glm_IPTW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","text":"","code":"glm_IPTW(   formula,   data,   treat.model = NULL,   family = binomial(),   id = NULL,   weights = NULL,   estpr = 1,   pi0 = 0.5,   ... )"},{"path":"http://kkholst.github.io/mets/reference/glm_IPTW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","text":"formula glm data data frame risk averaging treat.model propensity score model (binary multinomial) family glm (logistic regression) id cluster id standard errors weights may given, uses weights*w() weights estpr estimate propensity scores get infuence function contribution uncertainty pi0 fixed simple weights ... arguments glm call","code":""},{"path":"http://kkholst.github.io/mets/reference/glm_IPTW.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","text":"Also works cluster argument.","code":""},{"path":"http://kkholst.github.io/mets/reference/glm_IPTW.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"IPTW GLM, Inverse Probaibilty of Treatment Weighted GLM — glm_IPTW","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/gof.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"GOF for Cox PH regression — gof.phreg","title":"GOF for Cox PH regression — gof.phreg","text":"Cumulative score process residuals Cox PH regression p-values based Lin, Wei, Ying resampling.","code":""},{"path":"http://kkholst.github.io/mets/reference/gof.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GOF for Cox PH regression — gof.phreg","text":"","code":"# S3 method for class 'phreg' gof(object, n.sim = 1000, silent = 1, robust = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/gof.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GOF for Cox PH regression — gof.phreg","text":"object phreg object n.sim number simulations score processes silent show timing estimate produced longer jobs robust control wether robust dM_i(t) dN_i  used simulations ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/gof.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"GOF for Cox PH regression — gof.phreg","text":"Thomas Scheike Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/gof.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"GOF for Cox PH regression — gof.phreg","text":"","code":"library(mets) data(sTRACE)  m1 <- phreg(Surv(time,status==9)~vf+chf+diabetes,data=sTRACE)  gg <- gof(m1) gg #> Cumulative score process test for Proportionality: #>          Sup|U(t)|  pval #> vf        7.276731 0.010 #> chf       8.971263 0.074 #> diabetes  3.044404 0.800 par(mfrow=c(1,3)) plot(gg)   m1 <- phreg(Surv(time,status==9)~strata(vf)+chf+diabetes,data=sTRACE)  ## to get Martingale ~ dN based simulations gg <- gof(m1) gg #> Cumulative score process test for Proportionality: #>          Sup|U(t)|  pval #> chf       8.036132 0.138 #> diabetes  3.441389 0.697  ## to get Martingale robust simulations, specify cluster in  call  sTRACE$id <- 1:500 m1 <- phreg(Surv(time,status==9)~vf+chf+diabetes+cluster(id),data=sTRACE)  gg <- gof(m1) gg #> Cumulative score process test for Proportionality: #>          Sup|U(t)|  pval #> vf        7.276731 0.005 #> chf       8.971263 0.057 #> diabetes  3.044404 0.807  m1 <- phreg(Surv(time,status==9)~strata(vf)+chf+diabetes+cluster(id),data=sTRACE)  gg <- gof(m1) gg #> Cumulative score process test for Proportionality: #>          Sup|U(t)|  pval #> chf       8.036132 0.141 #> diabetes  3.441389 0.673"},{"path":"http://kkholst.github.io/mets/reference/gofG.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","title":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","text":"Looks stratified baseline Cox model plots baselines versus see lines straight, 50 resample versions assumptiosn stratified Cox correct","code":""},{"path":"http://kkholst.github.io/mets/reference/gofG.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","text":"","code":"gofG.phreg(x, sim = 0, silent = 1, lm = TRUE, ...)"},{"path":"http://kkholst.github.io/mets/reference/gofG.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","text":"x phreg object sim simulate som variation cox model put graph silent keep absolutely silent lm addd line plot, regressing cumulatives ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/gofG.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","text":"Thomas Scheike Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/gofG.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stratified baseline graphical GOF test for Cox covariates in PH regression — gofG.phreg","text":"","code":"data(tTRACE)  m1 <- phreg(Surv(time,status==9)~strata(vf)+chf+wmi,data=tTRACE)  m2 <- phreg(Surv(time,status==9)~vf+strata(chf)+wmi,data=tTRACE)  par(mfrow=c(2,2))  gofG.phreg(m1) gofG.phreg(m2)"},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"GOF for Cox covariates in PH regression — gofM.phreg","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":"Cumulative residuals model matrix Cox PH regression p-values based Lin, Wei, Ying resampling.","code":""},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":"","code":"gofM.phreg(   formula,   data,   offset = NULL,   weights = NULL,   modelmatrix = NULL,   n.sim = 1000,   silent = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":"formula formula cox regression data data model offset offset weights weights modelmatrix matrix cumulating residuals n.sim number simulations score processes silent keep absolutely silent, otherwise timing estimate prduced longer jobs. ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":", computes $$  U(t) = \\int_0^t M^t d \\hat M $$ resamples asymptotic distribution. show residuals consistent model. Typically, M design matrix continous covariates gives example quartiles, plot show different quartiles covariate risk prediction consistent time  (time x covariate interaction).","code":""},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":"Thomas Scheike Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/gofM.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"GOF for Cox covariates in PH regression — gofM.phreg","text":"","code":"library(mets) data(TRACE) set.seed(1) TRACEsam <- blocksample(TRACE,idvar=\"id\",replace=FALSE,100) dcut(TRACEsam)  <- ~.  mm <- model.matrix(~-1+factor(wmicat.4),data=TRACEsam) m1 <- gofM.phreg(Surv(time,status==9)~vf+chf+wmi,data=TRACEsam,modelmatrix=mm) summary(m1) #> Cumulative residuals versus modelmatrix : #>                            Sup_t |U(t)|  pval #> factor(wmicat.4)[0.4,1.1]      5.788752 0.021 #> factor(wmicat.4)(1.1,1.4]      2.633143 0.591 #> factor(wmicat.4)(1.4,1.72]     5.657370 0.038 #> factor(wmicat.4)(1.72,2]       1.227485 0.948 #>  #> Cumulative score process versus covariates (discrete z via model.matrix): #>         Sup_z |U(tau,z)|  pval #> matrixZ         1.801626 0.812 if (interactive()) { par(mfrow=c(2,2)) plot(m1) }  m1 <- gofM.phreg(Surv(time,status==9)~strata(vf)+chf+wmi,data=TRACEsam,modelmatrix=mm)  summary(m1) #> Cumulative residuals versus modelmatrix : #>                            Sup_t |U(t)|  pval #> factor(wmicat.4)[0.4,1.1]      5.679902 0.035 #> factor(wmicat.4)(1.1,1.4]      2.557399 0.593 #> factor(wmicat.4)(1.4,1.72]     5.411115 0.058 #> factor(wmicat.4)(1.72,2]       1.384392 0.931 #>  #> Cumulative score process versus covariates (discrete z via model.matrix): #>         Sup_z |U(tau,z)|  pval #> matrixZ         1.771578 0.812  ## cumulative sums in covariates, via design matrix mm  mm <- cumContr(TRACEsam$wmi,breaks=10,equi=TRUE) m1 <- gofM.phreg(Surv(time,status==9)~strata(vf)+chf+wmi,data=TRACEsam,       modelmatrix=mm,silent=0) #> Cumulative score process test for modelmatrix: #>        Sup_t |U(t)| pval #> <=0.56         0.88 0.33 #> <=0.72         2.25 0.28 #> <=0.88         5.28 0.03 #> <=1.04         2.68 0.55 #> <=1.2          4.12 0.20 #> <=1.36         4.09 0.16 #> <=1.52         3.34 0.27 #> <=1.68         1.78 0.84 #> <=1.84         1.17 0.85 #> <=2            0.00 1.00 summary(m1) #> Cumulative residuals versus modelmatrix : #>        Sup_t |U(t)|  pval #> <=0.56    0.8821214 0.330 #> <=0.72    2.2521756 0.276 #> <=0.88    5.2766972 0.029 #> <=1.04    2.6807567 0.552 #> <=1.2     4.1217296 0.201 #> <=1.36    4.0926272 0.161 #> <=1.52    3.3447151 0.271 #> <=1.68    1.7766131 0.844 #> <=1.84    1.1652990 0.853 #> <=2       0.0000000 1.000 #>  #> Cumulative score process versus covariates (discrete z via model.matrix): #>         Sup_z |U(tau,z)|  pval #> matrixZ         3.931466 0.225"},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"GOF for Cox covariates in PH regression — gofZ.phreg","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":", computes $$  U(z,\\tau) = \\int_0^\\tau M(z)^t d \\hat M $$ resamples asymptotic distribution.","code":""},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":"","code":"gofZ.phreg(   formula,   data,   vars = NULL,   offset = NULL,   weights = NULL,   breaks = 50,   equi = FALSE,   n.sim = 1000,   silent = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":"formula formula cox regression data data model vars variables test linearity offset offset weights weights breaks number breaks cumulatives covarirate direction equi equidistant breaks  n.sim number simulations score processes silent keep absolutely silent, otherwise timing estimate prduced longer jobs. ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":"show residuals consistent model evaulated z covariate. M chosen based grid (z_1, ..., z_m) different columns \\((Z_i \\leq z_l)\\). \\(l=1,...,m\\). process z resampled find extreme values.  time-points evuluation default 50 points, chosen 2 p-value valid depends chosen grid. number break points high give orginal test Lin, Wei Ying linearity, also computed timereg package.","code":""},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":"Thomas Scheike Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/gofZ.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"GOF for Cox covariates in PH regression — gofZ.phreg","text":"","code":"library(mets) data(TRACE) set.seed(1) TRACEsam <- blocksample(TRACE,idvar=\"id\",replace=FALSE,100)  ## cumulative sums in covariates, via design matrix mm  ## Reduce Ex.Timings m1 <- gofZ.phreg(Surv(time,status==9)~strata(vf)+chf+wmi+age,data=TRACEsam) summary(m1)  #> Cumulative residuals versus modelmatrix : #>     Sup_z |U(tau,z)|  pval #> wmi         3.891570 0.331 #> age         5.665722 0.093 plot(m1,type=\"z\")"},{"path":"http://kkholst.github.io/mets/reference/haplo.html","id":null,"dir":"Reference","previous_headings":"","what":"haplo fun data — haplo","title":"haplo fun data — haplo","text":"haplo fun data","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"haplo fun data — haplo","text":"hapfreqs : haplo frequencies haploX:  covariates response haplo survival discrete survival ghaplos:  haplo-types subjects haploX data","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"haplo fun data — haplo","text":"Estimated data","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Discrete time to event haplo type analysis — haplo.surv.discrete","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"Can used logistic regression time variable \"1\" id.","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"","code":"haplo.surv.discrete(   X = NULL,   y = \"y\",   time.name = \"time\",   Haplos = NULL,   id = \"id\",   desnames = NULL,   designfunc = NULL,   beta = NULL,   no.opt = FALSE,   method = \"NR\",   stderr = TRUE,   designMatrix = NULL,   response = NULL,   idhap = NULL,   design.only = FALSE,   covnames = NULL,   fam = binomial,   weights = NULL,   offsets = NULL,   idhapweights = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"X design matrix data-frame (sorted id time variable) id time response  desnames y name response (binary response logistic link) X time.name sort time  X Haplos (data.frame id, haplo1, haplo2 (haplotypes (h))  p=P(h|G)) haplotypes given factor. id name id variale X desnames names design matrix designfunc function computes design given haplotypes h=(h1,h2) x(h) beta starting values .opt optimization TRUE/FALSE method NR, nlm stderr return estimate designMatrix gives response designMatrix directly implemented (mush contain: p, id, idhap) response gives response design directly designMatrix implemented idhap name id-hap variable specify different haplotypes different id design.return design matrices haplo-type analyses. covnames names covariates extract object regression fam family models, now binomial default option weights weights following id GLM offsets following id  GLM idhapweights weights following id-hap GLM (WIP) ... Additional arguments lower level funtions lava::NR  optimizer nlm","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"Cycle-specific logistic regression haplo-type effects known haplo-type probabilities. Given observed genotype G unobserved haplotypes H mix possible haplotypes using P(H|G) provided. $$ S(t|x,G)) = E( S(t|x,H) | G)  = \\sum_{h \\G} P(h|G) S(t|z,h) $$ survival can computed mixing possible h given g. Survival based logistic regression discrete hazard function form $$ logit(P(T=t| T \\geq t, x,h)) = \\alpha_t + x(h) \\beta $$ x(h) regression design x haplotypes \\(h=(h_1,h_2)\\) Likelihood maximized standard errors assumes P(H|G) known. design possible haplotypes constructed merging X Haplos can viewed design.=TRUE","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/haplo.surv.discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discrete time to event haplo type analysis — haplo.surv.discrete","text":"","code":"## some haplotypes of interest types <- c(\"DCGCGCTCACG\",\"DTCCGCTGACG\",\"ITCAGTTGACG\",\"ITCCGCTGAGG\")  ## some haplotypes frequencies for simulations  data(haplo) hapfreqs <- haplo$hapfreqs   www <-which(hapfreqs$haplotype %in% types) hapfreqs$freq[www] #> [1] 0.138387 0.103394 0.048124 0.291273  baseline=hapfreqs$haplotype[9] baseline #> [1] \"DTGCGCTCGCG\"  designftypes <- function(x,sm=0) {# {{{ hap1=x[1] hap2=x[2] if (sm==0) y <- 1*( (hap1==types) | (hap2==types)) if (sm==1) y <- 1*(hap1==types) + 1*(hap2==types) return(y) }# }}}  tcoef=c(-1.93110204,-0.47531630,-0.04118204,-1.57872602,-0.22176426,-0.13836416, 0.88830288,0.60756224,0.39802821,0.32706859)  ghaplos <- haplo$ghaplos haploX  <- haplo$haploX  haploX$time <- haploX$times Xdes <- model.matrix(~factor(time),haploX) colnames(Xdes) <- paste(\"X\",1:ncol(Xdes),sep=\"\") X <- dkeep(haploX,~id+y+time) X <- cbind(X,Xdes) Haplos <- dkeep(ghaplos,~id+\"haplo*\"+p) desnames=paste(\"X\",1:6,sep=\"\")   # six X's related to 6 cycles  out <- haplo.surv.discrete(X=X,y=\"y\",time.name=\"time\",          Haplos=Haplos,desnames=desnames,designfunc=designftypes)  names(out$coef) <- c(desnames,types) out$coef #>          X1          X2          X3          X4          X5          X6  #> -1.82153345 -0.61608261 -0.17143057 -1.27152045 -0.28635976 -0.19349091  #> DCGCGCTCACG DTCCGCTGACG ITCAGTTGACG ITCCGCTGAGG  #>  0.79753613  0.65747412  0.06119231  0.31666905  summary(out) #>             Estimate Std.Err     2.5%   97.5%   P-value #> X1          -1.82153  0.1619 -2.13892 -1.5041 2.355e-29 #> X2          -0.61608  0.1895 -0.98748 -0.2447 1.149e-03 #> X3          -0.17143  0.1799 -0.52398  0.1811 3.406e-01 #> X4          -1.27152  0.2631 -1.78719 -0.7559 1.346e-06 #> X5          -0.28636  0.2030 -0.68425  0.1115 1.584e-01 #> X6          -0.19349  0.2134 -0.61184  0.2249 3.647e-01 #> DCGCGCTCACG  0.79754  0.1494  0.50465  1.0904 9.445e-08 #> DTCCGCTGACG  0.65747  0.1621  0.33971  0.9752 5.007e-05 #> ITCAGTTGACG  0.06119  0.2145 -0.35931  0.4817 7.755e-01 #> ITCCGCTGAGG  0.31667  0.1361  0.04989  0.5834 1.999e-02"},{"path":"http://kkholst.github.io/mets/reference/hfactioncpx12.html","id":null,"dir":"Reference","previous_headings":"","what":"hfaction, subset of block randmized study HF-ACtion from WA package — hfactioncpx12","title":"hfaction, subset of block randmized study HF-ACtion from WA package — hfactioncpx12","text":"Data HF-action trial slightly modified WA package, consisting 741 nonischemic patients baseline cardiopulmonary test duration less equal 12 minutes.","code":""},{"path":"http://kkholst.github.io/mets/reference/hfactioncpx12.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"hfaction, subset of block randmized study HF-ACtion from WA package — hfactioncpx12","text":"Randomized study status : 1-event, 2-death, 0-censoring treatment : 1/0","code":""},{"path":"http://kkholst.github.io/mets/reference/hfactioncpx12.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"hfaction, subset of block randmized study HF-ACtion from WA package — hfactioncpx12","text":"WA package, Connor et al. 2009","code":""},{"path":"http://kkholst.github.io/mets/reference/hfactioncpx12.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"hfaction, subset of block randmized study HF-ACtion from WA package — hfactioncpx12","text":"","code":"data(hfactioncpx12)"},{"path":"http://kkholst.github.io/mets/reference/iidBaseline.html","id":null,"dir":"Reference","previous_headings":"","what":"Influence functions or IID decomposition of baseline for recrec/phreg/cifregFG — iidBaseline","title":"Influence functions or IID decomposition of baseline for recrec/phreg/cifregFG — iidBaseline","text":"Influence functions IID decomposition baseline recrec/phreg/cifregFG","code":""},{"path":"http://kkholst.github.io/mets/reference/iidBaseline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Influence functions or IID decomposition of baseline for recrec/phreg/cifregFG — iidBaseline","text":"","code":"iidBaseline(   object,   time = NULL,   ft = NULL,   fixbeta = NULL,   beta.iid = NULL,   tminus = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/iidBaseline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Influence functions or IID decomposition of baseline for recrec/phreg/cifregFG — iidBaseline","text":"object phreg/recreg/cifregFG object time baseline IID ft function compute IID baseline integrated f(t) fixbeta fix coefficients beta.iid use iid beta tminus get predictions t- ... additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/iidBaseline.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Influence functions or IID decomposition of baseline for recrec/phreg/cifregFG — iidBaseline","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/internal.html","id":null,"dir":"Reference","previous_headings":"","what":"For internal use — npc","title":"For internal use — npc","text":"internal use","code":""},{"path":"http://kkholst.github.io/mets/reference/internal.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"For internal use — npc","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Discrete time to event interval censored data — interval.logitsurv.discrete","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"consider cumulative odds model $$    P(T \\leq t | x) =  \\frac{G(t) \\exp(x \\beta) }{1 + G(t) exp( x \\beta) } $$ equivalently $$    logit(P(T \\leq t | x)) = log(G(t)) + x \\beta $$ can thus also compute probability surviving $$    P(T >t | x) =  \\frac{1}{1 + G(t) exp( x \\beta) } $$","code":""},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"","code":"interval.logitsurv.discrete(   formula,   data,   beta = NULL,   no.opt = FALSE,   method = \"NR\",   stderr = TRUE,   weights = NULL,   offsets = NULL,   exp.link = 1,   increment = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"formula formula data data beta starting values .opt optimization TRUE/FALSE method NR, nlm stderr return estimate weights weights following id GLM offsets following id  GLM exp.link parametrize increments exp(alpha) > 0 increment using increments dG(t)=exp(alpha) parameters ... Additional arguments lower level funtions lava::NR  optimizer nlm","code":""},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"baseline \\(G(t)\\) written \\(cumsum(exp(\\alpha))\\) standard parametrization takes log \\(G(t)\\) parameters. Note regression coefficients describing probability dying time t. Input intervals given ]t_l,t_r] t_r can infinity right-censored intervals truly discrete ]0,1] observation 1,  ]j,j+1] observation j+1. Can used fitting usual ordinal regression model (logit link) contrast, however, describes probibility surviving time t (thus leads -beta). Likelihood maximized: $$  \\prod  P(T_i >t_{il} | x) - P(T_i> t_{ir}| x) $$","code":""},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/interval.logitsurv.discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discrete time to event interval censored data — interval.logitsurv.discrete","text":"","code":"library(mets) data(ttpd)  dtable(ttpd,~entry+time2) #>  #>       time2   1   2   3   4   5   6 Inf #> entry                                   #> 0           316   0   0   0   0   0   0 #> 1             0 133   0   0   0   0   0 #> 2             0   0 150   0   0   0   0 #> 3             0   0   0  23   0   0   0 #> 4             0   0   0   0  90   0   0 #> 5             0   0   0   0   0  68   0 #> 6             0   0   0   0   0   0 220  out <- interval.logitsurv.discrete(Interval(entry,time2)~X1+X2+X3+X4,ttpd) summary(out) #> $baseline #>       Estimate Std.Err   2.5%   97.5%   P-value #> time1  -2.0064  0.1523 -2.305 -1.7079 1.273e-39 #> time2  -2.1749  0.1599 -2.488 -1.8614 4.118e-42 #> time3  -1.4581  0.1544 -1.761 -1.1554 3.636e-21 #> time4  -2.9260  0.2453 -3.407 -2.4453 8.379e-33 #> time5  -1.2051  0.1706 -1.539 -0.8706 1.633e-12 #> time6  -0.9102  0.1860 -1.275 -0.5457 9.843e-07 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1   0.9913  0.1179 0.76024 1.2223 4.100e-17 #> X2   0.6962  0.1162 0.46847 0.9238 2.064e-09 #> X3   0.3466  0.1159 0.11941 0.5738 2.788e-03 #> X4   0.3223  0.1151 0.09668 0.5478 5.111e-03 #>  #> $or #>    Estimate     2.5%    97.5% #> X1 2.694610 2.138791 3.394874 #> X2 2.006032 1.597554 2.518953 #> X3 1.414239 1.126834 1.774950 #> X4 1.380231 1.101503 1.729490 #>  head(iid(out))  #>            [,1]         [,2]          [,3]          [,4]          [,5] #> 1  0.0045687959  0.004769499  0.0053427163  0.0059138018  0.0066308444 #> 2  0.0016959549  0.002038630  0.0025477402  0.0029776943 -0.0102830496 #> 3  0.0045687959  0.004769499  0.0053427163  0.0059138018  0.0066308444 #> 4  0.0027545442 -0.006047556 -0.0007244072 -0.0006949805 -0.0006704063 #> 5 -0.0002919658 -0.008889214 -0.0026820744 -0.0026532556 -0.0026268232 #> 6  0.0001497624 -0.008530642 -0.0033151419 -0.0032325395 -0.0031636812 #>            [,6]          [,7]          [,8]          [,9]         [,10] #> 1  0.0081721788 -0.0033482398 -0.0034168560  0.0034308192 -0.0034212419 #> 2 -0.0012875717 -0.0007883982 -0.0005310631 -0.0004080546 -0.0000776067 #> 3  0.0081721788 -0.0033482398 -0.0034168560  0.0034308192 -0.0034212419 #> 4 -0.0006379316  0.0003557924  0.0007697270  0.0008855193 -0.0013506040 #> 5 -0.0025608456 -0.0026170215  0.0016772465  0.0020412533  0.0017043055 #> 6 -0.0030621110  0.0015290328  0.0016662399  0.0020179143  0.0017471657  pred <- predictlogitSurvd(out,se=FALSE) plotSurvd(pred)   ttpd <- dfactor(ttpd,fentry~entry) out <- cumoddsreg(fentry~X1+X2+X3+X4,ttpd) summary(out) #> $baseline #>       Estimate Std.Err   2.5%   97.5%   P-value #> time1  -2.0064  0.1523 -2.305 -1.7079 1.273e-39 #> time2  -2.1749  0.1599 -2.488 -1.8614 4.118e-42 #> time3  -1.4581  0.1544 -1.761 -1.1554 3.636e-21 #> time4  -2.9260  0.2453 -3.407 -2.4453 8.379e-33 #> time5  -1.2051  0.1706 -1.539 -0.8706 1.633e-12 #> time6  -0.9102  0.1860 -1.275 -0.5457 9.843e-07 #>  #> $logor #>    Estimate Std.Err    2.5%  97.5%   P-value #> X1   0.9913  0.1179 0.76024 1.2223 4.100e-17 #> X2   0.6962  0.1162 0.46847 0.9238 2.064e-09 #> X3   0.3466  0.1159 0.11941 0.5738 2.788e-03 #> X4   0.3223  0.1151 0.09668 0.5478 5.111e-03 #>  #> $or #>    Estimate     2.5%    97.5% #> X1 2.694610 2.138791 3.394874 #> X2 2.006032 1.597554 2.518953 #> X3 1.414239 1.126834 1.774950 #> X4 1.380231 1.101503 1.729490 #>  head(iid(out))  #>            [,1]         [,2]          [,3]          [,4]          [,5] #> 1  0.0045687959  0.004769499  0.0053427163  0.0059138018  0.0066308444 #> 2  0.0016959549  0.002038630  0.0025477402  0.0029776943 -0.0102830496 #> 3  0.0045687959  0.004769499  0.0053427163  0.0059138018  0.0066308444 #> 4  0.0027545442 -0.006047556 -0.0007244072 -0.0006949805 -0.0006704063 #> 5 -0.0002919658 -0.008889214 -0.0026820744 -0.0026532556 -0.0026268232 #> 6  0.0001497624 -0.008530642 -0.0033151419 -0.0032325395 -0.0031636812 #>            [,6]          [,7]          [,8]          [,9]         [,10] #> 1  0.0081721788 -0.0033482398 -0.0034168560  0.0034308192 -0.0034212419 #> 2 -0.0012875717 -0.0007883982 -0.0005310631 -0.0004080546 -0.0000776067 #> 3  0.0081721788 -0.0033482398 -0.0034168560  0.0034308192 -0.0034212419 #> 4 -0.0006379316  0.0003557924  0.0007697270  0.0008855193 -0.0013506040 #> 5 -0.0025608456 -0.0026170215  0.0016772465  0.0020412533  0.0017043055 #> 6 -0.0030621110  0.0015290328  0.0016662399  0.0020179143  0.0017471657"},{"path":"http://kkholst.github.io/mets/reference/ipw.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Probability of Censoring Weights — ipw","title":"Inverse Probability of Censoring Weights — ipw","text":"Internal function. Calculates Inverse Probability Censoring Weights (IPCW) adds data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Probability of Censoring Weights — ipw","text":"","code":"ipw(   formula,   data,   cluster,   same.cens = FALSE,   obs.only = FALSE,   weight.name = \"w\",   trunc.prob = FALSE,   weight.name2 = \"wt\",   indi.weight = \"pr\",   cens.model = \"aalen\",   pairs = FALSE,   theta.formula = ~1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/ipw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Probability of Censoring Weights — ipw","text":"formula Formula specifying censoring model data data frame cluster clustering variable .cens clustered data, censoring assumed (bivariate probability calculated mininum marginal probabilities) obs.Return data uncensored observations weight.name Name weight variable new data.frame trunc.prob TRUE truncation probabilities also calculated stored 'weight.name2' (based Clayton-Oakes gamma frailty model) weight.name2 Name truncation probabilities indi.weight Name individual censoring weight  new data.frame cens.model Censoring model (default Aalens additive model) pairs paired data (e.g. twins) complete pairs returned (pairs=TRUE) theta.formula Model dependence parameter Clayton-Oakes model (truncation ) ... Additional arguments censoring model","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Inverse Probability of Censoring Weights — ipw","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Probability of Censoring Weights — ipw","text":"","code":"if (FALSE) { # \\dontrun{ data(\"prt\",package=\"mets\") prtw <- ipw(Surv(time,status==0)~country, data=prt[sample(nrow(prt),5000),],             cluster=\"id\",weight.name=\"w\") plot(0,type=\"n\",xlim=range(prtw$time),ylim=c(0,1),xlab=\"Age\",ylab=\"Probability\") count <- 0 for (l in unique(prtw$country)) {     count <- count+1     prtw <- prtw[order(prtw$time),]     with(subset(prtw,country==l),          lines(time,w,col=count,lwd=2)) } legend(\"topright\",legend=unique(prtw$country),col=1:4,pch=-1,lty=1) } # }"},{"path":"http://kkholst.github.io/mets/reference/ipw2.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Probability of Censoring Weights — ipw2","title":"Inverse Probability of Censoring Weights — ipw2","text":"Internal function. Calculates Inverse Probability Censoring Truncation Weights adds data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Probability of Censoring Weights — ipw2","text":"","code":"ipw2(   data,   times = NULL,   entrytime = NULL,   time = \"time\",   cause = \"cause\",   same.cens = FALSE,   cluster = NULL,   pairs = FALSE,   strata = NULL,   obs.only = TRUE,   cens.formula = NULL,   cens.code = 0,   pair.cweight = \"pcw\",   pair.tweight = \"ptw\",   pair.weight = \"weights\",   cname = \"cweights\",   tname = \"tweights\",   weight.name = \"indi.weights\",   prec.factor = 100,   ... )"},{"path":"http://kkholst.github.io/mets/reference/ipw2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Probability of Censoring Weights — ipw2","text":"data data frame times possible time argument speciying maximum value time tau=max(times), specify things considered censored . entrytime nam entry-time truncation. time name time variable data frame. cause name cause indicator data frame. .cens clustered data, censoring assumed truncation (bivariate probability calculated mininum marginal probabilities) cluster name clustering variable pairs paired data (e.g. twins) complete pairs returned (pairs=TRUE) strata name strata variable get weights stratified. obs.Return data uncensored observations cens.formula model Cox models truncation right censoring times. cens.code censoring.code pair.cweight Name weight variable new data.frame right censorig pairs pair.tweight Name weight variable new data.frame left truncation pairs pair.weight Name weight variable new data.frame right censoring left truncation pairs cname Name weight variable new data.frame right censoring individuals tname Name weight variable new data.frame left truncation individuals weight.name Name weight variable new data.frame right censoring left truncation individuals prec.factor let tied censoring truncation times come death times. ... Additional arguments censoring model","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw2.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Inverse Probability of Censoring Weights — ipw2","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/ipw2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Probability of Censoring Weights — ipw2","text":"","code":"library(\"timereg\") set.seed(1) d <- simnordic.random(5000,delayed=TRUE,ptrunc=0.7,       cordz=0.5,cormz=2,lam0=0.3,country=FALSE) d$strata <- as.numeric(d$country)+(d$zyg==\"MZ\")*4 times <- seq(60,100,by=10) ## c1 <- timereg::comp.risk(Event(time,cause)~1+cluster(id),data=d,cause=1, ##   model=\"fg\",times=times,max.clust=NULL,n.sim=0) ## mm=model.matrix(~-1+zyg,data=d) ## out1<-random.cif(c1,data=d,cause1=1,cause2=1,same.cens=TRUE,theta.des=mm) ## summary(out1) ## pc1 <- predict(c1,X=1,se=0) ## plot(pc1) ##  ## dl <- d[!d$truncated,] ## dl <- ipw2(dl,cluster=\"id\",same.cens=TRUE,time=\"time\",entrytime=\"entry\",cause=\"cause\", ##            strata=\"strata\",prec.factor=100) ## cl <- timereg::comp.risk(Event(time,cause)~+1+ ##     cluster(id), ##      data=dl,cause=1,model=\"fg\", ##     weights=dl$indi.weights,cens.weights=rep(1,nrow(dl)), ##           times=times,max.clust=NULL,n.sim=0) ## pcl <- predict(cl,X=1,se=0) ## lines(pcl$time,pcl$P1,col=2) ## mm=model.matrix(~-1+factor(zyg),data=dl) ## out2<-random.cif(cl,data=dl,cause1=1,cause2=1,theta.des=mm, ##                  weights=dl$weights,censoring.weights=rep(1,nrow(dl))) ## summary(out2)"},{"path":"http://kkholst.github.io/mets/reference/km.html","id":null,"dir":"Reference","previous_headings":"","what":"Kaplan-Meier with robust standard errors — km","title":"Kaplan-Meier with robust standard errors — km","text":"Kaplan-Meier robust standard errors Robust variance default variance obtained predict call","code":""},{"path":"http://kkholst.github.io/mets/reference/km.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kaplan-Meier with robust standard errors — km","text":"","code":"km(formula, data = data, ...)"},{"path":"http://kkholst.github.io/mets/reference/km.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kaplan-Meier with robust standard errors — km","text":"formula formula 'Surv' 'Event' outcome data data frame ... Additional arguments phreg","code":""},{"path":"http://kkholst.github.io/mets/reference/km.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Kaplan-Meier with robust standard errors — km","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/km.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kaplan-Meier with robust standard errors — km","text":"","code":"library(mets) data(sTRACE) sTRACE$cluster <- sample(1:100,500,replace=TRUE) out1 <- km(Surv(time,status==9)~strata(vf,chf),data=sTRACE) out2 <- km(Surv(time,status==9)~strata(vf,chf)+cluster(cluster),data=sTRACE)  summary(out1,times=1:3) #> $pred #>           [,1]      [,2]      [,3] #> [1,] 0.9216911 0.8606809 0.8171022 #> [2,] 0.7180838 0.6043700 0.5032911 #> [3,] 0.6930406 0.6930406 0.6930406 #> [4,] 0.5320165 0.4044111 0.4044111 #>  #> $se.pred #>            [,1]       [,2]       [,3] #> [1,] 0.01768107 0.02279610 0.02545049 #> [2,] 0.02911535 0.03163815 0.03234100 #> [3,] 0.16275118 0.16275118 0.16275118 #> [4,] 0.09970269 0.09739478 0.09739478 #>  #> $lower #>           [,1]      [,2]      [,3] #> [1,] 0.8876802 0.8171413 0.7687123 #> [2,] 0.6632273 0.5454355 0.4437332 #> [3,] 0.4373867 0.4373867 0.4373867 #> [4,] 0.3684728 0.2522477 0.2522477 #>  #> $upper #>           [,1]      [,2]      [,3] #> [1,] 0.9570051 0.9065405 0.8685383 #> [2,] 0.7774775 0.6696724 0.5708430 #> [3,] 1.0000000 1.0000000 1.0000000 #> [4,] 0.7681477 0.6483640 0.6483640 #>  #> $times #> [1] 1 2 3 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\" summary(out2,times=1:3) #> $pred #>           [,1]      [,2]      [,3] #> [1,] 0.9216911 0.8606809 0.8171022 #> [2,] 0.7180838 0.6043700 0.5032911 #> [3,] 0.6930406 0.6930406 0.6930406 #> [4,] 0.5320165 0.4044111 0.4044111 #>  #> $se.pred #>            [,1]       [,2]       [,3] #> [1,] 0.01642282 0.02138888 0.02742317 #> [2,] 0.02592998 0.02848191 0.02901227 #> [3,] 0.13371319 0.13371319 0.13371319 #> [4,] 0.09483034 0.08233461 0.08233461 #>  #> $lower #>           [,1]      [,2]      [,3] #> [1,] 0.8900585 0.8197641 0.7650835 #> [2,] 0.6690187 0.5510470 0.4495228 #> [3,] 0.4748215 0.4748215 0.4748215 #> [4,] 0.3751466 0.2713475 0.2713475 #>  #> $upper #>           [,1]      [,2]      [,3] #> [1,] 0.9544479 0.9036401 0.8726579 #> [2,] 0.7707472 0.6628529 0.5634909 #> [3,] 1.0000000 1.0000000 1.0000000 #> [4,] 0.7544825 0.6027266 0.6027266 #>  #> $times #> [1] 1 2 3 #>  #> attr(,\"class\") #> [1] \"summarypredictrecreg\"  par(mfrow=c(1,2)) plot(out1,se=TRUE) plot(out2,se=TRUE)"},{"path":"http://kkholst.github.io/mets/reference/lifecourse.html","id":null,"dir":"Reference","previous_headings":"","what":"Life-course plot — lifecourse","title":"Life-course plot — lifecourse","text":"Life-course plot event life data recurrent events","code":""},{"path":"http://kkholst.github.io/mets/reference/lifecourse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Life-course plot — lifecourse","text":"","code":"lifecourse(   formula,   data,   id = \"id\",   group = NULL,   type = \"l\",   lty = 1,   col = 1:10,   alpha = 0.3,   lwd = 1,   recurrent.col = NULL,   recurrent.lty = NULL,   legend = NULL,   pchlegend = NULL,   by = NULL,   status.legend = NULL,   place.sl = \"bottomright\",   xlab = \"Time\",   ylab = \"\",   add = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/lifecourse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Life-course plot — lifecourse","text":"formula Formula (Event(start,slut,status) ~ ...) data data.frame id Id variable group group variable type Type (line 'l', stair 's', ...) lty Line type col Colour alpha transparency (0-1) lwd Line width recurrent.col col recurrence type recurrent.lty lty's  recurrence type legend position optional id legend pchlegend point type legends make separate plot level '' (formula, name column, vector) status.legend Status legend place.sl Placement status legend xlab Label X-axis ylab Label Y-axis add Add existing device ... Additional arguments lower level arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/lifecourse.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Life-course plot — lifecourse","text":"Thomas Scheike, Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/lifecourse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Life-course plot — lifecourse","text":"","code":"data = data.frame(id=c(1,1,1,2,2),start=c(0,1,2,3,4),slut=c(1,2,4,4,7),                   type=c(1,2,3,2,3),status=c(0,1,2,1,2),group=c(1,1,1,2,2)) ll = lifecourse(Event(start,slut,status)~id,data,id=\"id\")  ll = lifecourse(Event(start,slut,status)~id,data,id=\"id\",recurrent.col=\"type\")   ll = lifecourse(Event(start,slut,status)~id,data,id=\"id\",group=~group,col=1:2)  op <- par(mfrow=c(1,2)) ll = lifecourse(Event(start,slut,status)~id,data,id=\"id\",by=~group)  par(op) legends=c(\"censored\",\"pregnant\",\"married\") ll = lifecourse(Event(start,slut,status)~id,data,id=\"id\",group=~group,col=1:2,status.legend=legends)"},{"path":"http://kkholst.github.io/mets/reference/lifetable.matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Life table — lifetable.matrix","title":"Life table — lifetable.matrix","text":"Create simple life table","code":""},{"path":"http://kkholst.github.io/mets/reference/lifetable.matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Life table — lifetable.matrix","text":"","code":"# S3 method for class 'matrix' lifetable(x, strata = list(), breaks = c(),    weights=NULL, confint = FALSE, ...)   # S3 method for class 'formula' lifetable(x, data=parent.frame(), breaks = c(),    weights=NULL, confint = FALSE, ...)"},{"path":"http://kkholst.github.io/mets/reference/lifetable.matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Life table — lifetable.matrix","text":"x time formula (Surv) matrix/data.frame columns time,status entry,exit,status strata strata breaks time intervals weights weights variable confint TRUE 95% confidence limits calculated ... additional arguments lower level functions data data.frame","code":""},{"path":"http://kkholst.github.io/mets/reference/lifetable.matrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Life table — lifetable.matrix","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/lifetable.matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Life table — lifetable.matrix","text":"","code":"library(timereg) data(TRACE)  d <- with(TRACE,lifetable(Surv(time,status==9)~sex+vf,breaks=c(0,0.2,0.5,8.5))) lava::estimate(glm(events ~ offset(log(atrisk))+factor(int.end)*vf + sex*vf,             data=d,poisson)) #>                        Estimate  Std.Err    2.5%   97.5%    P-value #> (Intercept)           -0.444337 0.010009 -0.4640 -0.4247  0.000e+00 #> factor(int.end)0.5    -1.197746 0.024205 -1.2452 -1.1503  0.000e+00 #> factor(int.end)8.5    -1.871838 0.008473 -1.8884 -1.8552  0.000e+00 #> vf                     1.830440 0.064658  1.7037  1.9572 2.631e-176 #> sex                   -0.239036 0.006105 -0.2510 -0.2271  0.000e+00 #> factor(int.end)0.5:vf -1.746744 0.218350 -2.1747 -1.3188  1.247e-15 #> factor(int.end)8.5:vf -1.927748 0.097242 -2.1183 -1.7372  1.838e-87 #> vf:sex                 0.009668 0.081323 -0.1497  0.1691  9.054e-01"},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":null,"dir":"Reference","previous_headings":"","what":"Proportional odds survival model — logitSurv","title":"Proportional odds survival model — logitSurv","text":"Semiparametric Proportional odds model, advantage $$ logit(S(t|x)) = \\log(\\Lambda(t)) + x \\beta $$ covariate effects give survival.","code":""},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Proportional odds survival model — logitSurv","text":"","code":"logitSurv(formula, data, offset = NULL, weights = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Proportional odds survival model — logitSurv","text":"formula formula 'Surv' outcome (see coxph) data data frame offset offsets exp(x beta) terms weights weights score equations ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Proportional odds survival model — logitSurv","text":"equivalent using hazards model $$   Z \\lambda(t) \\exp(x \\beta) $$ Z gamma distributed mean variance 1.","code":""},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Proportional odds survival model — logitSurv","text":"proportional odds cumulative incidence model competing risks, Eriksson, Frank Li, Jianing Scheike, Thomas Zhang, Mei-Jie, Biometrics, 2015, 3, 687–695, 71,","code":""},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Proportional odds survival model — logitSurv","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/logitSurv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Proportional odds survival model — logitSurv","text":"","code":"library(mets) data(TRACE) dcut(TRACE) <- ~. out1 <- logitSurv(Surv(time,status==9)~vf+chf+strata(wmicat.4),data=TRACE) summary(out1) #>  #>     n events #>  1878    958 #> coeffients: #>     Estimate    S.E. dU^-1/2 P-value #> vf   0.30049 0.22633 0.11154  0.1843 #> chf  1.26008 0.10095 0.07316  0.0000 #>  #> exp(coeffients): #>     Estimate    2.5%  97.5% #> vf   1.35052 0.86667 2.1045 #> chf  3.52570 2.89277 4.2971 #>  gof(out1) #> Cumulative score process test for Proportionality: #>     Sup|U(t)|  pval #> vf   42.28659 0.000 #> chf  20.75308 0.512 plot(out1)"},{"path":"http://kkholst.github.io/mets/reference/mediatorSurv.html","id":null,"dir":"Reference","previous_headings":"","what":"Mediation analysis in survival context — mediatorSurv","title":"Mediation analysis in survival context — mediatorSurv","text":"Mediation analysis survival context  robust standard errors taking weights account via influence function computations. Mediator exposure must factors.  based numerical derivative wrt parameters weighting.  See vignette examples.","code":""},{"path":"http://kkholst.github.io/mets/reference/mediatorSurv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mediation analysis in survival context — mediatorSurv","text":"","code":"mediatorSurv(   survmodel,   weightmodel,   data = data,   wdata = wdata,   id = \"id\",   silent = TRUE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/mediatorSurv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mediation analysis in survival context — mediatorSurv","text":"survmodel mediation model (binreg, aalenMets, phreg) weightmodel mediation model data computations wdata weighted data expansion computations id name id variable, important SE computations silent silent ... Additional arguments survival model","code":""},{"path":"http://kkholst.github.io/mets/reference/mediatorSurv.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Mediation analysis in survival context — mediatorSurv","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/mediatorSurv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mediation analysis in survival context — mediatorSurv","text":"","code":"library(mets) n <- 400 dat <- kumarsimRCT(n,rho1=0.5,rho2=0.5,rct=2,censpar=c(0,0,0,0),           beta = c(-0.67, 0.59, 0.55, 0.25, 0.98, 0.18, 0.45, 0.31),     treatmodel = c(-0.18, 0.56, 0.56, 0.54),restrict=1) dfactor(dat) <- dnr.f~dnr dfactor(dat) <- gp.f~gp drename(dat) <- ttt24~\"ttt24*\" dat$id <- 1:n dat$ftime <- 1  weightmodel <- fit <- glm(gp.f~dnr.f+preauto+ttt24,data=dat,family=binomial) wdata <- medweight(fit,data=dat)  ### fitting models with and without mediator aaMss2 <- binreg(Event(time,status)~gp+dnr+preauto+ttt24+cluster(id),data=dat,time=50,cause=2) aaMss22 <- binreg(Event(time,status)~dnr+preauto+ttt24+cluster(id),data=dat,time=50,cause=2)  ### estimating direct and indirect effects (under strong strong assumptions)  aaMss <- binreg(Event(time,status)~dnr.f0+dnr.f1+preauto+ttt24+cluster(id),                 data=wdata,time=50,weights=wdata$weights,cause=2) ## to compute standard errors , requires numDeriv library(numDeriv) ll <- mediatorSurv(aaMss,fit,data=dat,wdata=wdata) summary(ll) #>    n events #>  800    364 #>  #>  400 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept) -0.277563  0.165585 -0.602103  0.046978  0.0937 #> dnr.f01     -0.013552  0.264909 -0.532765  0.505661  0.9592 #> dnr.f11      0.243449  0.081661  0.083396  0.403502  0.0029 #> preauto      0.317339  0.234407 -0.142091  0.776768  0.1758 #> ttt24        0.331677  0.264380 -0.186498  0.849853  0.2096 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  0.75763 0.54766 1.0481 #> dnr.f01      0.98654 0.58698 1.6581 #> dnr.f11      1.27564 1.08697 1.4971 #> preauto      1.37347 0.86754 2.1744 #> ttt24        1.39330 0.82986 2.3393 #>  #>  ## not run bootstrap (to save time) ## bll <- BootmediatorSurv(aaMss,fit,data=dat,k.boot=500)"},{"path":"http://kkholst.github.io/mets/reference/medweight.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes mediation weights — medweight","title":"Computes mediation weights — medweight","text":"Computes mediation weights either binary multinomial mediators. important part influence functions can obtained compute standard errors.","code":""},{"path":"http://kkholst.github.io/mets/reference/medweight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes mediation weights — medweight","text":"","code":"medweight(   fit,   data = data,   var = NULL,   name.weight = \"weights\",   id.name = \"id\",   ... )"},{"path":"http://kkholst.github.io/mets/reference/medweight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes mediation weights — medweight","text":"fit either glm-binomial mlogit (mets package) data data frame data var NULL reads mediator exposure formulae fit. name.weight name weights id.name name id variable, important SE computations ... Additional arguments ","code":""},{"path":"http://kkholst.github.io/mets/reference/medweight.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Computes mediation weights — medweight","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/melanoma.html","id":null,"dir":"Reference","previous_headings":"","what":"The Melanoma Survival Data — melanoma","title":"The Melanoma Survival Data — melanoma","text":"melanoma data frame 205 rows 7 columns.  contains data relating survival patients operation malignant melanoma collected Odense University Hospital K.T.  Drzewiecki.","code":""},{"path":"http://kkholst.github.io/mets/reference/melanoma.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The Melanoma Survival Data — melanoma","text":"data frame contains following columns: numeric vector. Patient code. status numeric vector code. Survival status. 1: dead melanoma, 2: alive, 3: dead cause. days numeric vector. Survival time. ulc numeric vector code. Ulceration, 1: present, 0: absent. thick numeric vector. Tumour thickness (1/100 mm). sex numeric vector code. 0: female, 1: male.","code":""},{"path":"http://kkholst.github.io/mets/reference/melanoma.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The Melanoma Survival Data — melanoma","text":"Andersen, P.K., Borgan O, Gill R.D., Keiding N. (1993), Statistical Models Based Counting Processes, Springer-Verlag. Drzewiecki, K.T., Ladefoged, C., Christensen, H.E. (1980), Biopsy prognosis cutaneous malignant melanoma clinical stage . Scand. J. Plast. Reconstru. Surg. 14, 141-144.","code":""},{"path":"http://kkholst.github.io/mets/reference/melanoma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Melanoma Survival Data — melanoma","text":"","code":"data(melanoma) names(melanoma) #> [1] \"no\"     \"status\" \"days\"   \"ulc\"    \"thick\"  \"sex\""},{"path":"http://kkholst.github.io/mets/reference/mena.html","id":null,"dir":"Reference","previous_headings":"","what":"Menarche data set — mena","title":"Menarche data set — mena","text":"Menarche data set","code":""},{"path":"http://kkholst.github.io/mets/reference/mena.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Menarche data set — mena","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/mets-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mets: Analysis of Multivariate Event Times — mets-package","title":"mets: Analysis of Multivariate Event Times — mets-package","text":"Implementation various statistical models multivariate event history data doi:10.1007/s10985-013-9244-x . Including multivariate cumulative incidence models doi:10.1002/sim.6016 , bivariate random effects probit models (Liability models) doi:10.1016/j.csda.2015.01.014 . Modern methods survival analysis, including regression modelling (Cox, Fine-Gray, Ghosh-Lin, Binomial regression) fast computation influence functions. Implementation various statistical models multivariate event history data. Including multivariate cumulative incidence models, bivariate random effects probit models (Liability models)","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/mets-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mets: Analysis of Multivariate Event Times — mets-package","text":"Maintainer: Klaus K. Holst klaus@holst.Authors: Thomas Scheike Klaus K. Holst Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/mets-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"mets: Analysis of Multivariate Event Times — mets-package","text":"","code":"## To appear"},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set global options for mets — mets.options","title":"Set global options for mets — mets.options","text":"Extract set global parameters mets.","code":""},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set global options for mets — mets.options","text":"","code":"mets.options(...)"},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set global options for mets — mets.options","text":"... Arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set global options for mets — mets.options","text":"list parameters","code":""},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set global options for mets — mets.options","text":"regex: TRUE character vectors interpreted regular expressions (dby, dcut, ...) silent: Set FALSE disable various output messages","code":""},{"path":"http://kkholst.github.io/mets/reference/mets.options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set global options for mets — mets.options","text":"","code":"if (FALSE) { # \\dontrun{ mets.options(regex=TRUE) } # }"},{"path":"http://kkholst.github.io/mets/reference/migr.html","id":null,"dir":"Reference","previous_headings":"","what":"Migraine data — migr","title":"Migraine data — migr","text":"Migraine data","code":""},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":null,"dir":"Reference","previous_headings":"","what":"Multinomial regression based on phreg regression — mlogit","title":"Multinomial regression based on phreg regression — mlogit","text":"Fits multinomial regression model $$ P_i = \\frac{ \\exp( X^\\beta_i ) }{ \\sum_{j=1}^K \\exp( X^\\beta_j ) }$$ $$=1,..,K$$ $$\\beta_1 = 0$$, $$\\sum_j P_j = 1$$ using phreg function. Thefore ratio $$\\frac{P_i}{P_1} = \\exp( X^\\beta_i )$$","code":""},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multinomial regression based on phreg regression — mlogit","text":"","code":"mlogit(formula, data, offset = NULL, weights = NULL, fix.X = FALSE, ...)"},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multinomial regression based on phreg regression — mlogit","text":"formula formula outcome (see coxph) data data frame offset offsets partial likelihood weights score equations fix.X coefficients categories ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multinomial regression based on phreg regression — mlogit","text":"Coefficients give log-Relative-Risk relative baseline group (first level factor, can reset relevel command). Standard errors computed based sandwhich form $$ DU^-1  \\sum U_i^2 DU^-1$$. Can also get influence functions (possibly robust) via iid() function, response factor. Can fit cumulative odds model special case interval.logitsurv.discrete","code":""},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multinomial regression based on phreg regression — mlogit","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/mlogit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multinomial regression based on phreg regression — mlogit","text":"","code":"library(mets) data(bmt) bmt$id <- sample(200,408,replace=TRUE) dfactor(bmt) <- cause1f~cause drelevel(bmt,ref=3) <- cause3f~cause dlevels(bmt) #> cause1f #levels=:3  #> [1] \"0\" \"1\" \"2\" #> ----------------------------------------- #> cause3f #levels=:3  #> [1] \"2\" \"0\" \"1\" #> -----------------------------------------  mreg <- mlogit(cause1f~+1+cluster(id),bmt) summary(mreg) #>  #>     n events #>  1224    408 #>  #>  1224 clusters #> coeffients: #>              Estimate       S.E.    dU^-1/2 P-value #> Intercept2  0.0062305  0.1187375  0.1116297  0.9582 #> Intercept3 -0.6092657  0.1414469  0.1332076  0.0000 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> Intercept2  1.00625 0.79733 1.2699 #> Intercept3  0.54375 0.41210 0.7175 #>  head(iid(mreg)) #>      Intercept2    Intercept3 #> 1 -6.288820e-03 -1.005747e-03 #> 2  1.692758e-17  1.149425e-02 #> 3  6.172360e-03 -6.250000e-03 #> 4  6.211180e-03  2.298851e-02 #> 5  1.242236e-02 -2.801584e-17 #> 6  3.246739e-17  2.298851e-02 dim(iid(mreg)) #> [1] 174   2  mreg <- mlogit(cause1f~tcell+platelet,bmt) summary(mreg) #>  #>     n events #>  1224    408 #>  #>  1224 clusters #> coeffients: #>            Estimate     S.E.  dU^-1/2 P-value #> Intercept2  0.25002  0.13906  0.13858  0.0722 #> tcell2     -0.28389  0.36431  0.36285  0.4358 #> platelet2  -0.68611  0.24797  0.24956  0.0057 #> Intercept3 -0.56565  0.16921  0.17078  0.0008 #> tcell3      0.50505  0.36481  0.36226  0.1662 #> platelet3  -0.35890  0.29130  0.28727  0.2179 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> Intercept2  1.28406 0.97773 1.6864 #> tcell2      0.75285 0.36864 1.5375 #> platelet2   0.50353 0.30971 0.8187 #> Intercept3  0.56799 0.40767 0.7914 #> tcell3      1.65707 0.81062 3.3874 #> platelet3   0.69844 0.39462 1.2362 #>  head(iid(mreg)) #>      Intercept2       tcell2    platelet2 Intercept3      tcell3   platelet3 #> 1 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 #> 2 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 #> 3 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 #> 4 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 #> 5 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 #> 6 -0.0001355312 0.0009504519 8.857909e-05  0.0185731 -0.01020386 -0.01569271 dim(iid(mreg)) #> [1] 408   6  mreg3 <- mlogit(cause3f~tcell+platelet,bmt) summary(mreg3) #>  #>     n events #>  1224    408 #>  #>  1224 clusters #> coeffients: #>            Estimate     S.E.  dU^-1/2 P-value #> Intercept2  0.56565  0.16921  0.17078  0.0008 #> tcell2     -0.50505  0.36481  0.36226  0.1662 #> platelet2   0.35890  0.29130  0.28727  0.2179 #> Intercept3  0.81567  0.16346  0.16467  0.0000 #> tcell3     -0.78894  0.39244  0.38890  0.0444 #> platelet3  -0.32721  0.30423  0.30139  0.2821 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> Intercept2  1.76059 1.26364 2.4530 #> tcell2      0.60347 0.29521 1.2336 #> platelet2   1.43175 0.80893 2.5341 #> Intercept3  2.26070 1.64100 3.1144 #> tcell3      0.45433 0.21053 0.9804 #> platelet3   0.72093 0.39713 1.3087 #>   ## inverse information standard errors  lava::estimate(coef=mreg3$coef,vcov=mreg3$II) #>            Estimate Std.Err    2.5%    97.5%   P-value #> Intercept2   0.5657  0.1708  0.2309  0.90038 9.259e-04 #> tcell2      -0.5051  0.3623 -1.2151  0.20497 1.633e-01 #> platelet2    0.3589  0.2873 -0.2041  0.92194 2.115e-01 #> Intercept3   0.8157  0.1647  0.4929  1.13842 7.294e-07 #> tcell3      -0.7889  0.3889 -1.5512 -0.02672 4.249e-02 #> platelet3   -0.3272  0.3014 -0.9179  0.26350 2.776e-01  ## predictions based on seen response or not  ## all probabilities head(predict(mreg,response=FALSE)) #>           0         1         2 #> 1 0.3506254 0.4502227 0.1991519 #> 2 0.3506254 0.4502227 0.1991519 #> 3 0.3506254 0.4502227 0.1991519 #> 4 0.3506254 0.4502227 0.1991519 #> 5 0.3506254 0.4502227 0.1991519 #> 6 0.3506254 0.4502227 0.1991519 head(predict(mreg)) #>        pred         se    lower      upper #> 1 0.1991519 0.06681508 0.330107 0.06819674 #> 2 0.1991519 0.06681508 0.330107 0.06819674 #> 3 0.1991519 0.06681508 0.330107 0.06819674 #> 4 0.1991519 0.06681508 0.330107 0.06819674 #> 5 0.1991519 0.06681508 0.330107 0.06819674 #> 6 0.1991519 0.06681508 0.330107 0.06819674 ## using newdata  newdata <- data.frame(tcell=c(1,1,1),platelet=c(0,1,1),cause1f=c(\"2\",\"2\",\"0\")) ## only probability of seen response  predict(mreg,newdata) #>        pred         se     lower      upper #> 1 0.3236700 0.13603308 0.5902900 0.05705011 #> 2 0.3065921 0.10966904 0.5215395 0.09164473 #> 3 0.4663875 0.07376361 0.6109615 0.32181348 ## without response predict(mreg,newdata,response=FALSE) #>           0         1         2 #> 1 0.3438904 0.3324396 0.3236700 #> 2 0.4663875 0.2270204 0.3065921 #> 3 0.4663875 0.2270204 0.3065921 ## given indexx of P(Y=j) predict(mreg,newdata,Y=c(1,2,3)) #>        pred         se     lower       upper #> 1 0.3438904 0.06916812 0.4794574  0.20832337 #> 2 0.2270204 0.12225406 0.4666340 -0.01259315 #> 3 0.3065921 0.10966904 0.5215395  0.09164473 ##  reponse not given  newdata <- data.frame(tcell=c(1,1,1),platelet=c(0,1,1)) predict(mreg,newdata) #>           0         1         2 #> 1 0.3438904 0.3324396 0.3236700 #> 2 0.4663875 0.2270204 0.3065921 #> 3 0.4663875 0.2270204 0.3065921"},{"path":"http://kkholst.github.io/mets/reference/multcif.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate Cumulative Incidence Function example data set — multcif","title":"Multivariate Cumulative Incidence Function example data set — multcif","text":"Multivariate Cumulative Incidence Function example data set","code":""},{"path":"http://kkholst.github.io/mets/reference/multcif.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Multivariate Cumulative Incidence Function example data set — multcif","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/np.html","id":null,"dir":"Reference","previous_headings":"","what":"np data set — np","title":"np data set — np","text":"np data set","code":""},{"path":"http://kkholst.github.io/mets/reference/np.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"np data set — np","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast Cox PH regression — phreg","title":"Fast Cox PH regression — phreg","text":"Fast Cox PH regression Robust variance default variance summary.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast Cox PH regression — phreg","text":"","code":"phreg(formula, data, offset = NULL, weights = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast Cox PH regression — phreg","text":"formula formula 'Surv' outcome (see coxph) data data frame offset offsets Cox model weights weights Cox score equations ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast Cox PH regression — phreg","text":"influence functions (iid) follow numerical order given cluster variable ordering $id give iid order data-set.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast Cox PH regression — phreg","text":"Klaus K. Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast Cox PH regression — phreg","text":"","code":"library(mets) data(TRACE) dcut(TRACE) <- ~. out1 <- phreg(Surv(time,status==9)~wmi+age+strata(vf,chf)+cluster(id),data=TRACE) summary(out1) #>  #>     n events #>  1878    958 #> coeffients: #>       Estimate       S.E.    dU^-1/2 P-value #> wmi -0.8470690  0.0871082  0.0864370       0 #> age  0.0563491  0.0037111  0.0036386       0 #>  #> exp(coeffients): #>     Estimate    2.5%  97.5% #> wmi  0.42867 0.36139 0.5085 #> age  1.05797 1.05030 1.0657 #>   par(mfrow=c(1,2)) plot(out1)  ## computing robust variance for baseline rob1 <- robust.phreg(out1) plot(rob1,se=TRUE,robust=TRUE)   ## iid decomposition, with scaled influence functions ## for regression parameters head(iid(out1)) #>              wmi           age #> 3   2.315573e-04  8.113514e-05 #> 7  -1.175364e-03 -2.901628e-05 #> 13  1.826339e-03  1.670430e-04 #> 15 -3.739823e-05  2.673793e-05 #> 17 -2.997039e-03  5.371497e-05 #> 22 -9.146118e-04 -9.342247e-06 ## making iid decomposition of baseline at a specific time-point Aiiid <- iid(out1,time=30) head(Aiiid) #>          strata0       strata1       strata2       strata3 #> 3  -1.894877e-04 -4.120570e-04 -2.659857e-04 -7.350908e-04 #> 7   1.812767e-04  2.367745e-04  1.678711e-04  8.774722e-04 #> 13 -6.837818e-04 -1.063129e-03 -6.374803e-04 -1.717426e-03 #> 15 -8.543458e-05 -9.149095e-05 -8.008394e-05 -2.250601e-04 #> 17  4.118052e-05  3.243790e-05  3.366936e-05 -1.046447e-05 #> 22  5.011669e-05  1.201129e-04  8.975026e-05  2.188617e-04 ## both iid decompositions dd <- iidBaseline(out1,time=30) head(dd$beta.iid) #>             [,1]          [,2] #> 3   2.315573e-04  8.113514e-05 #> 7  -1.175364e-03 -2.901628e-05 #> 13  1.826339e-03  1.670430e-04 #> 15 -3.739823e-05  2.673793e-05 #> 17 -2.997039e-03  5.371497e-05 #> 22 -9.146118e-04 -9.342247e-06 head(dd$base.iid) #>          strata0       strata1       strata2       strata3 #> 3  -1.894877e-04 -4.120570e-04 -2.659857e-04 -7.350908e-04 #> 7   1.812767e-04  2.367745e-04  1.678711e-04  8.774722e-04 #> 13 -6.837818e-04 -1.063129e-03 -6.374803e-04 -1.717426e-03 #> 15 -8.543458e-05 -9.149095e-05 -8.008394e-05 -2.250601e-04 #> 17  4.118052e-05  3.243790e-05  3.366936e-05 -1.046447e-05 #> 22  5.011669e-05  1.201129e-04  8.975026e-05  2.188617e-04  outs <- phreg(Surv(time,status==9)~strata(vf,wmicat.4)+cluster(id),data=TRACE) summary(outs) #>  #>     n events #>  1878    958 #>  par(mfrow=c(1,2)) plot(outs)"},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":null,"dir":"Reference","previous_headings":"","what":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"Fits Cox model treatment weights $$ w()= \\sum_a (=)/\\pi(|X)$$,  $$\\pi(|X)=P(=|X)$$. Computes standard errors via influence functions returned IID argument. Propensity scores fitted using either logistic regression (glm) multinomial model (mlogit) treatment categories. treatment needs factor identified rhs \"treat.model\". Recurrent events can considered start,stop structure cluster(id) must specified. Robust standard errors computed cases.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"","code":"phreg_IPTW(   formula,   data,   treat.model = NULL,   treat.var = NULL,   weights = NULL,   estpr = 1,   pi0 = 0.5,   se.cluster = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"formula phreg data data frame risk averaging treat.model propensity score model (binary multinomial) treat.var 1/0 variable indicates treatment given propensity score computed weights may given, uses weights*w() weights estpr (=1, default) estimate propensity scores get infuence function contribution uncertainty pi0 fixed simple weights se.cluster compute GEE type standard errors additional cluster structure present ... arguments phreg call","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"Time-dependent propensity score weights can also computed treat.var used, must 1 time first (A_0) 2nd treatment (A_1), uses weights $$w_0(A_0) * w_1(A_1)^{t>T_r}$$ $$T_r$$ time 2nd randomization.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_IPTW.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"IPTW Cox, Inverse Probaibilty of Treatment Weighted Cox regression — phreg_IPTW","text":"","code":"library(mets) data <- mets:::simLT(0.7,100,beta=0.3,betac=0,ce=1,betao=0.3) dfactor(data) <- Z.f~Z out <- phreg_IPTW(Surv(time,status)~Z.f,data=data,treat.model=Z.f~X) summary(out) #>  #>    n events #>  100     45 #>  #>  100 clusters #> coeffients: #>      Estimate     S.E.  dU^-1/2 P-value #> Z.f1 -0.47160  0.25676  0.20863  0.0662 #>  #> exp(coeffients): #>      Estimate    2.5%  97.5% #> Z.f1  0.62400 0.37725 1.0321 #>"},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":null,"dir":"Reference","previous_headings":"","what":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"Efficient implementation Lu-Tsiatis improvement using baseline covariates, extended competing risks recurrent events. Results almost equivalent speffSurv function speff2trial function survival case. dynamic censoring augmentation regression also computed gain even censoring augmentation. Furhter, also deal twostage randomizations. function implemented deal recurrent events (start,stop) + cluster,  examples vignette.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"","code":"phreg_rct(   formula,   data,   cause = 1,   cens.code = 0,   typesR = c(\"R0\", \"R1\", \"R01\"),   typesC = c(\"C\", \"dynC\"),   weights = NULL,   augmentR0 = NULL,   augmentR1 = NULL,   augmentC = NULL,   treat.model = ~+1,   RCT = TRUE,   treat.var = NULL,   km = TRUE,   level = 0.95,   cens.model = NULL,   estpr = 1,   pi0 = 0.5,   base.augment = FALSE,   return.augmentR0 = FALSE,   mlogit = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"formula formula 'Surv' 'Event' outcome (see coxph) treatment (randomization 0/1) data data frame cause use competing risks, recurrent events data cens.code use competing risks, recurrent events data typesR augmentations used randomization typesC augmentations used censoring weights weights score equation augmentR0 formula randomization augmentation (~age+sex) augmentR1 formula randomization augmentation (~age+sex) augmentC formula censoring augmentation (~age+sex) treat.model propensity score model, default ~+1, assuming RCT study RCT false use propensity score adjustment marginal model treat.var case twostage randomization, variable 1 treatment times, start,stop default assumes one treatment first record km use Kaplan-Meier censoring weights (stratified treatment) level confidence intervals cens.model default censoring model ~strata(treatment) model can used make censoring martingales estpr estimates propensity scores pi0 possible fixed propensity scores randomizations base.augment TRUE covariate augment baselines (R0 augmentation) return.augmentR0 return augmentation data mlogit TRUE forces use function propensity scores, default binary treatment glm ... Additional arguments phreg function","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"Lu, Tsiatis (2008), Improving efficiency log-rank test using auxiliary covariates, Biometrika, 679–694 Scheike, Nerstroem Martinussen (2025), Randomized clinical trials proportional hazards model recurrent events.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_rct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lu-Tsiatis More Efficient Log-Rank for Randomized studies with baseline covariates — phreg_rct","text":"","code":"## Lu, Tsiatis simulation data <- mets:::simLT(0.7,100) dfactor(data) <- Z.f~Z  out <- phreg_rct(Surv(time,status)~Z.f,data=data,augmentR0=~X,augmentC=~factor(Z):X) summary(out) #>                 Estimate   Std.Err       2.5%     97.5%   P-value #> Marginal-Z.f1  0.1781155 0.2639114 -0.3391412 0.6953723 0.4997350 #> R0_C:Z.f1     -0.0175242 0.2071406 -0.4235122 0.3884638 0.9325790 #> R0_dynC:Z.f1   0.1109733 0.2022974 -0.2855223 0.5074689 0.5833039 #> attr(,\"class\") #> [1] \"summary.phreg_rct\""},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":null,"dir":"Reference","previous_headings":"","what":"Weibull-Cox regression — phreg_weibull","title":"Weibull-Cox regression — phreg_weibull","text":"Fits Cox-Weibull cumulative hazard given $$   \\Lambda(t) = \\lambda \\cdot t^s $$ \\(s\\) shape parameter,   \\(\\lambda\\) rate parameter. allow regression model   parameters $$\\lambda := \\exp(\\beta^\\top X)$$ $$s :=e   \\exp(\\gamma^\\top Z)$$ defined `formula` `shape.formula`   respectively.","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weibull-Cox regression — phreg_weibull","text":"","code":"phreg_weibull(   formula,   shape.formula = ~1,   data,   save.data = TRUE,   control = list() )"},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weibull-Cox regression — phreg_weibull","text":"formula Formula proportional hazards. right-handside must [Event] [Surv] object (right-censoring possibly delayed entry). shape.formula Formula shape parameter data data.frame save.data TRUE data.frame stored model object (predictions simulations) control control arguments optimization routine [stats::nlmbin]","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weibull-Cox regression — phreg_weibull","text":"`phreg.par` object","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Weibull-Cox regression — phreg_weibull","text":"parametrization","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Weibull-Cox regression — phreg_weibull","text":"Klaus Kähler Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/phreg_weibull.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weibull-Cox regression — phreg_weibull","text":"","code":"data(sTRACE, package=\"mets\") sTRACE$entry <- 0 fit1 <- phreg_weibull(Event(entry, time, status == 9) ~ age,              shape.formula = ~age, data = sTRACE) tt <- seq(0,10, length.out=100) pr1 <- predict(fit1, newdata = sTRACE[1, ], times = tt) fit2 <- phreg(Event(time, status == 9) ~ age, data = sTRACE) pr2 <- predict(fit2, newdata = sTRACE[1, ], se = FALSE) if (interactive()) {    plot(pr2$times, pr2$surv, type=\"s\")    lines(tt, pr1[,1,1], col=\"red\", lwd=2) }"},{"path":"http://kkholst.github.io/mets/reference/plack.cif.html","id":null,"dir":"Reference","previous_headings":"","what":"plack Computes concordance for or.cif based model, that is Plackett random effects model — plack.cif","title":"plack Computes concordance for or.cif based model, that is Plackett random effects model — plack.cif","text":".. content description (empty lines) ..","code":""},{"path":"http://kkholst.github.io/mets/reference/plack.cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plack Computes concordance for or.cif based model, that is Plackett random effects model — plack.cif","text":"","code":"plack.cif(cif1, cif2, object)"},{"path":"http://kkholst.github.io/mets/reference/plack.cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plack Computes concordance for or.cif based model, that is Plackett random effects model — plack.cif","text":"cif1 Cumulative incidence first argument. cif2 Cumulative incidence second argument. object .cif object dependence parameters.","code":""},{"path":"http://kkholst.github.io/mets/reference/plack.cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"plack Computes concordance for or.cif based model, that is Plackett random effects model — plack.cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/plot.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting the baselines of stratified Cox — plot.phreg","title":"Plotting the baselines of stratified Cox — plot.phreg","text":"Plotting baselines stratified Cox","code":""},{"path":"http://kkholst.github.io/mets/reference/plot.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting the baselines of stratified Cox — plot.phreg","text":"","code":"# S3 method for class 'phreg' plot(x, ...)"},{"path":"http://kkholst.github.io/mets/reference/plot.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting the baselines of stratified Cox — plot.phreg","text":"x phreg object ... Additional arguments baseplot funtion","code":""},{"path":"http://kkholst.github.io/mets/reference/plot.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plotting the baselines of stratified Cox — plot.phreg","text":"Klaus K. Holst, Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/plot.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plotting the baselines of stratified Cox — plot.phreg","text":"","code":"data(TRACE) dcut(TRACE) <- ~. out1 <- phreg(Surv(time,status==9)~vf+chf+strata(wmicat.4),data=TRACE)  par(mfrow=c(2,2)) plot(out1) plot(out1,stratas=c(0,3)) plot(out1,stratas=c(0,3),col=2:3,lty=1:2,se=TRUE) plot(out1,stratas=c(0),col=2,lty=2,se=TRUE,polygon=FALSE)  plot(out1,stratas=c(0),col=matrix(c(2,1,3),1,3),lty=matrix(c(1,2,3),1,3),se=TRUE,polygon=FALSE)"},{"path":"http://kkholst.github.io/mets/reference/plot_twin.html","id":null,"dir":"Reference","previous_headings":"","what":"Scatter plot function — plot_twin","title":"Scatter plot function — plot_twin","text":"Scatterplot contours (kernel) estimated density","code":""},{"path":"http://kkholst.github.io/mets/reference/plot_twin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scatter plot function — plot_twin","text":"","code":"plot_twin(   data,   marginal.args = list(),   kernsmooth.args = list(),   xlab,   ylab,   col = \"black\",   col2 = \"lightblue\",   alpha = 0.3,   grid = TRUE,   side.plot = TRUE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/plot_twin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scatter plot function — plot_twin","text":"data bivariate data plot (data.frame matrix 2 columns) marginal.args argumemts marginal estimator (`density` continuous data, `barplot` categorical ) kernsmooth.args arguments 2d-kernel smoother xlab x-axis label ylab y-axis label col color points col2 color contour / density plot alpha transparency level points grid grid added plot side.plot TRUE subplots marginal distributions added plot ... arguments lower level plot functions","code":""},{"path":"http://kkholst.github.io/mets/reference/plot_twin.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scatter plot function — plot_twin","text":"Klaus Kähler Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/plot_twin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scatter plot function — plot_twin","text":"","code":"data(\"twinbmi\", package=\"mets\") twinwide <- fast.reshape(twinbmi, id=\"tvparnr\",varying=c(\"bmi\")) datamz <- log(subset(twinwide, zyg==\"MZ\")[,c(\"bmi1\",\"bmi2\")])  # continuous variables plot_twin(datamz)   # categorical variables datamz2 <- datamz datamz2[, 1] <- cut(datamz[, 1], 4) datamz2[, 2] <- cut(datamz[, 2], 4) plot_twin(datamz2, color = TRUE)   # survival variables cens1 <- rbinom(nrow(datamz), 1, 0.5) cens2 <- rbinom(nrow(datamz), 1, 0.5) datamz2[, 1] <- Event(datamz[, 1], cens1) datamz2[, 2] <- suppressWarnings(Event(datamz[, 2], cens2)) plot_twin(datamz2)   rm(datamz, datamz2, cens1, cens2)"},{"path":"http://kkholst.github.io/mets/reference/pmvn.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate normal distribution function — pmvn","title":"Multivariate normal distribution function — pmvn","text":"Multivariate normal distribution function","code":""},{"path":"http://kkholst.github.io/mets/reference/pmvn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multivariate normal distribution function — pmvn","text":"","code":"pmvn(lower, upper, mu, sigma, cor = FALSE)"},{"path":"http://kkholst.github.io/mets/reference/pmvn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multivariate normal distribution function — pmvn","text":"lower lower limits upper upper limits mu mean vector sigma variance matrix vector correlation coefficients cor TRUE sigma treated standardized (correlation matrix)","code":""},{"path":"http://kkholst.github.io/mets/reference/pmvn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multivariate normal distribution function — pmvn","text":"","code":"lower <- rbind(c(0,-Inf),c(-Inf,0)) upper <- rbind(c(Inf,0),c(0,Inf)) mu <- rbind(c(1,1),c(-1,1)) sigma <- diag(2)+1 pmvn(lower=lower,upper=upper,mu=mu,sigma=sigma) #> [1] 0.1265479 0.5361516"},{"path":"http://kkholst.github.io/mets/reference/predict.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions from proportional hazards model — predict.phreg","title":"Predictions from proportional hazards model — predict.phreg","text":"Predictions proportional hazards model","code":""},{"path":"http://kkholst.github.io/mets/reference/predict.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictions from proportional hazards model — predict.phreg","text":"","code":"# S3 method for class 'phreg' predict(   object,   newdata,   times = NULL,   individual.time = FALSE,   tminus = FALSE,   se = TRUE,   robust = FALSE,   conf.type = \"log\",   conf.int = 0.95,   km = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/predict.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictions from proportional hazards model — predict.phreg","text":"object phreg object newdata data.frame times Time predict variable, default time-points object sorted individual.time use one (individual) time per subject, newdata times length makes predictions individual times. tminus make predictions T- strictly given times, useful IPCW techniques se standard errors upper lower confidence intervals. robust get robust se's also default functions (uses robse.cumhaz otherwise se.cumhaz). conf.type transformation suvival estimates, default log conf.int significance level km use Kaplan-Meier product-limit baseline $$S_{s0}(t)= (1 - dA_{s0}(t))$$, otherwise take exp cumulative baseline. ... Additional arguments plot functions","code":""},{"path":"http://kkholst.github.io/mets/reference/predictRisk.binreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Risk predictions to work with riskRegression package — predictRisk.binreg","title":"Risk predictions to work with riskRegression package — predictRisk.binreg","text":"Risk predictions work riskRegression package","code":""},{"path":"http://kkholst.github.io/mets/reference/predictRisk.binreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Risk predictions to work with riskRegression package — predictRisk.binreg","text":"","code":"# S3 method for class 'binreg' predictRisk(object, newdata, cause, times = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/predictRisk.binreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Risk predictions to work with riskRegression package — predictRisk.binreg","text":"object phreg/binreg/cifreg object newdata data.frame make new predictions cause cause (cif) predict times times predictions ... additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/predictRisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Risk predictions to work with riskRegression package — predictRisk","title":"Risk predictions to work with riskRegression package — predictRisk","text":"Risk predictions work riskRegression package","code":""},{"path":"http://kkholst.github.io/mets/reference/predictRisk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Risk predictions to work with riskRegression package — predictRisk","text":"","code":"predictRisk(object, ...)"},{"path":"http://kkholst.github.io/mets/reference/predictRisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Risk predictions to work with riskRegression package — predictRisk","text":"object phreg/binreg/cifreg object ... additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/predictRisk.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Risk predictions to work with riskRegression package — predictRisk","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/print.casewise.html","id":null,"dir":"Reference","previous_headings":"","what":"prints Concordance test — print.casewise","title":"prints Concordance test — print.casewise","text":"prints Concordance test","code":""},{"path":"http://kkholst.github.io/mets/reference/print.casewise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"prints Concordance test — print.casewise","text":"","code":"# S3 method for class 'casewise' print(x, digits = 3, ...)"},{"path":"http://kkholst.github.io/mets/reference/print.casewise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prints Concordance test — print.casewise","text":"x output casewise.test digits number digits ... Additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/print.casewise.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"prints Concordance test — print.casewise","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"Estimation probability k events recurrent events process terminal event, based also estimate variance recurrent events. estimator based cumulative incidence exceeding \"k\" events. contrast probability exceeding k events can also computed counting process integral.","code":""},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"","code":"prob.exceed.recurrent(   formula,   data,   cause = 1,   death.code = 2,   cens.code = 0,   exceed = NULL,   marks = NULL,   all.cifs = FALSE,   return.data = FALSE,   conf.type = c(\"log\", \"plain\"),   level = 0.95,   ... )"},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"formula formula data data-frame cause interest death.code status cens.code censoring codes exceed values (given observed values) marks may give jump-times exceed values needs specified .cifs true returns list fitted objects cif.exceed return.data true returns list data fitting different excess thresholds conf.type type confidence interval c(\"log\",\"plain\") level confidence intervals default 0.95 ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"Scheike, Eriksson, Tribler (2019), mean, variance correlation bivariate                                        recurrent events terminal event, JRSS-C","code":""},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/prob.exceed.recurrent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimation of probability of more that k events for recurrent events process — prob.exceed.recurrent","text":"","code":"library(mets) data(hfactioncpx12) dtable(hfactioncpx12,~status) #>  #> status #>    0    1    2  #>  617 1391  124  #>   oo <- prob.exceed.recurrent(Event(entry,time,status)~cluster(id),         hfactioncpx12,cause=1,death.code=2) plot(oo)  summary(oo,times=c(1,2,5)) #> $prob #>      times                 N<1 exceed>=1 exceed>=2 exceed>=3  exceed>=4 #> [1,]     1 0.9978807 0.5747460 0.4252540 0.2008652 0.1012955 0.04794006 #> [2,]     2 1.9967128 0.3925156 0.6074844 0.3509483 0.2205076 0.13989818 #> [3,]     5 3.9793816 0.1925999 0.8074001 0.5477499 0.3899373 0.29900312 #>       exceed>=5  exceed>=6   exceed>=7 #> [1,] 0.03153223 0.01371012 0.008229099 #> [2,] 0.10092792 0.05533511 0.035595440 #> [3,] 0.19615192 0.14357991 0.103037717 #>  #> $se #>      times                  N<1  exceed>=1  exceed>=2  exceed>=3   exceed>=4 #> [1,]     1 0.9978807 0.01827977 0.01827977 0.01481729 0.01116527 0.007907269 #> [2,]     2 1.9967128 0.01862412 0.01862412 0.01832598 0.01592337 0.013395849 #> [3,]     5 3.9793816 0.02129779 0.02129779 0.02413438 0.02515176 0.024909447 #>        exceed>=5   exceed>=6   exceed>=7 #> [1,] 0.006470709 0.004305759 0.003345689 #> [2,] 0.011774638 0.008883381 0.007151615 #> [3,] 0.020706361 0.019083117 0.016923719 #>  #> $lower #>      times                                                              #> [1,]     1 0.9978807 0.6091060 0.3908940 0.1738256 0.08161439 0.0346977 #> [2,]     2 1.9967128 0.4279431 0.5720569 0.3168070 0.19140637 0.1159594 #> [3,]     5 3.9793816 0.2332821 0.7667179 0.5024323 0.34362952 0.2539590 #>                                         #> [1,] 0.02109017 0.007408243 0.003709206 #> [2,] 0.08029840 0.040397144 0.024009120 #> [3,] 0.15949140 0.110652445 0.074677237 #>  #> $upper #>      times                                                              #> [1,]     1 0.9978807 0.5373658 0.4626342 0.2321109 0.1257227 0.06623635 #> [2,]     2 1.9967128 0.3548940 0.6451060 0.3887690 0.2540334 0.16877894 #> [3,]     5 3.9793816 0.1497591 0.8502409 0.5971550 0.4424855 0.35203666 #>                                      #> [1,] 0.0471443 0.02537272 0.01825676 #> [2,] 0.1268574 0.07579680 0.05277309 #> [3,] 0.2412392 0.18630579 0.14216877 #>"},{"path":"http://kkholst.github.io/mets/reference/prt.html","id":null,"dir":"Reference","previous_headings":"","what":"Prostate data set — prt","title":"Prostate data set — prt","text":"Prostate data set","code":""},{"path":"http://kkholst.github.io/mets/reference/prt.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Prostate data set — prt","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":null,"dir":"Reference","previous_headings":"","what":"Random effects model for competing risks data — random.cif","title":"Random effects model for competing risks data — random.cif","text":"Fits random effects  model describing dependence cumulative incidence curves subjects within cluster.  Given gamma distributed random effects assumed cumulative incidence curves indpendent, marginal cumulative incidence curves form $$ P(T \\leq t, cause=1 | x,z) = P_1(t,x,z) = 1- exp( -x^T (t) exp(z^T \\beta)) $$ allow regression structure random effects variances may depend cluster covariates.","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random effects model for competing risks data — random.cif","text":"","code":"random.cif(   cif,   data,   cause = NULL,   cif2 = NULL,   cause1 = 1,   cause2 = 1,   cens.code = NULL,   cens.model = \"KM\",   Nit = 40,   detail = 0,   clusters = NULL,   theta = NULL,   theta.des = NULL,   sym = 1,   step = 1,   same.cens = FALSE,   var.link = 0,   score.method = \"nr\",   entry = NULL,   trunkp = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random effects model for competing risks data — random.cif","text":"cif model object comp.risk function marginal cumulative incidence cause2, .e., event conditioned , whose odds comparision made respect data data.frame variables. cause specifies causes  related death times, value cens.code censoring value. cif2 specificies model cause2 different cause1. cause1 cause first coordinate. cause2 cause second coordinate. cens.code specificies code censoring NULL uses one marginal cif model. cens.model specified model use ICPW, KM Kaplan-Meier alternatively may \"cox\" Nit number iterations Newton-Raphson algorithm. detail 0 details printed iterations, 1 details given. clusters specifies cluster structure. theta specifies starting values cross-odds-ratio parameters model. theta.des specifies regression design cross-odds-ratio parameters. sym 1 symmetry 0 otherwise step specifies step size Newton-Raphson algorith.m .cens true censoring within clusters assumed variable, default independent censoring. var.link var.link=1 var log-scale. score.method default uses \"nlminb\" optimzer, alternatively, use \"nr\" algorithm. entry entry-age case delayed entry. two causes must given. trunkp gives probability survival delayed entry, related entry-ages given . ... extra arguments.","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random effects model for competing risks data — random.cif","text":"returns object type 'cor'. following arguments: theta estimate proportional odds parameters model. var.theta variance gamma. hess derivative used score. score scores final stage. score scores final stage. theta.iid matrix iid decomposition parametric effects.","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random effects model for competing risks data — random.cif","text":"Semiparametric Random Effects Model Multivariate Competing Risks Data, Scheike, Zhang, Sun, Jensen (2010), Biometrika. Cross odds ratio Modelling dependence Multivariate Competing Risks Data, Scheike Sun (2012), work progress.","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Random effects model for competing risks data — random.cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/random.cif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random effects model for competing risks data — random.cif","text":"","code":"## Reduce Ex.Timings  d <- simnordic.random(5000,delayed=TRUE,cordz=0.5,cormz=2,lam0=0.3,country=TRUE)  times <- seq(50,90,by=10)  add1 <- timereg::comp.risk(Event(time,cause)~-1+factor(country)+cluster(id),data=d,  times=times,cause=1,max.clust=NULL)   ### making group indidcator   mm <- model.matrix(~-1+factor(zyg),d)   out1<-random.cif(add1,data=d,cause1=1,cause2=1,theta=1,same.cens=TRUE)  summary(out1) #> Random effect variance for variation due to clusters #>  #> Cause 1 and cause 1 #>  #>  #>              Coef.        SE        z        P-val Cross odds ratio        SE #> intercept 1.104589 0.1416994 7.795297 6.439294e-15         2.104589 0.1416994   out2<-random.cif(add1,data=d,cause1=1,cause2=1,theta=1,        theta.des=mm,same.cens=TRUE)  summary(out2) #> Random effect variance for variation due to clusters #>  #> Cause 1 and cause 1 #>  #>  #>                   Coef.        SE        z        P-val Cross odds ratio #> factor(zyg)MZ 2.1534461 0.3028887 7.109694 1.163070e-12         3.153446 #> factor(zyg)DZ 0.3138631 0.1303997 2.406931 1.608721e-02         1.313863 #>                      SE #> factor(zyg)MZ 0.3028887 #> factor(zyg)DZ 0.1303997  ######################################### ##### 2 different causes #########################################   add2 <- timereg::comp.risk(Event(time,cause)~-1+factor(country)+cluster(id),data=d,                   times=times,cause=2,max.clust=NULL)  out3 <- random.cif(add1,data=d,cause1=1,cause2=2,cif2=add2,sym=1,same.cens=TRUE)  summary(out3) ## negative dependence #> Random effect variance for variation due to clusters #>  #> Cause 1 and cause 2 #>  #>  #>                Coef.         SE         z P-val Cross odds ratio         SE #> intercept -0.3341633 0.02629939 -12.70612     0        0.6658367 0.02629939   out4 <- random.cif(add1,data=d,cause1=1,cause2=2,cif2=add2,theta.des=mm,sym=1,same.cens=TRUE)  summary(out4) ## negative dependence #> Random effect variance for variation due to clusters #>  #> Cause 1 and cause 2 #>  #>  #>                    Coef.         SE          z        P-val Cross odds ratio #> factor(zyg)MZ -0.4502417 0.03083045 -14.603799 0.000000e+00        0.5497583 #> factor(zyg)DZ -0.2007758 0.04502517  -4.459191 8.226949e-06        0.7992242 #>                       SE #> factor(zyg)MZ 0.03083045 #> factor(zyg)DZ 0.04502517"},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of Piecewise constant hazard model (Cox). — rchaz","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"Simulates data piecwise constant baseline hazard can also Cox type. Censor data highest value break points.","code":""},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"","code":"rchaz(   cumhazard,   rr,   n = NULL,   entry = NULL,   cum.hazard = TRUE,   cause = 1,   extend = FALSE )"},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"cumhazard cumulative hazard, piece-constant rates periods defined first column input. rr relative risk simulations, alternatively rr=1 specify n n number simulation rr given entry delayed entry time simuations. cum.hazard specifies wheter input cumulative hazard rates. cause name cause extend extend piecewise constant constant rate. Default average rate time cumulative (TRUE), numeric uses given rate.","code":""},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"piecewise linear cumulative hazard inverse easy compute delayed entry x compute $$\\Lambda^{-1}(\\Lambda(x) + E/RR)$$, RR relative risks E exponential mean 1. quantity survival function $$P(T > t | T>x) = exp(-RR (\\Lambda(t) - \\Lambda(x)))$$.","code":""},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/rchaz.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of Piecewise constant hazard model (Cox). — rchaz","text":"","code":"library(mets) chaz <-  c(0,1,1.5,2,2.1) breaks <- c(0,10,   20,  30,   40) cumhaz <- cbind(breaks,chaz) n <- 10 X <- rbinom(n,1,0.5) beta <- 0.2 rrcox <- exp(X * beta)  pctime <- rchaz(cumhaz,n=10) pctimecox <- rchaz(cumhaz,rrcox,entry=runif(n))"},{"path":"http://kkholst.github.io/mets/reference/rchazC.html","id":null,"dir":"Reference","previous_headings":"","what":"Piecewise constant hazard distribution — rchazC","title":"Piecewise constant hazard distribution — rchazC","text":"Piecewise constant hazard distribution","code":""},{"path":"http://kkholst.github.io/mets/reference/rchazC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Piecewise constant hazard distribution — rchazC","text":"","code":"rchazC(base1, rr, entry)"},{"path":"http://kkholst.github.io/mets/reference/rchazC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Piecewise constant hazard distribution — rchazC","text":"base1 baseline rr relative risk terms entry entry times left truncation","code":""},{"path":"http://kkholst.github.io/mets/reference/rcrisk.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","title":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","text":"Simulates data piecwise constant baseline hazard can also Cox type. Censor data highest value break points either cumulatives, see also sim.phregs","code":""},{"path":"http://kkholst.github.io/mets/reference/rcrisk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","text":"","code":"rcrisk(   cumA,   cumB,   rr1 = NULL,   rr2 = NULL,   n = NULL,   cens = NULL,   rrc = NULL,   extend = TRUE,   causes = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/rcrisk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","text":"cumA cumulative hazard cause 1, list multiple cumulative hazards cumB cumulative hazard cause 2 NULL cumA list rr1 number simulations vector relative risk simuations, matrix columns equal number hazards list rr2 number simulations vector relative risk simuations. n number simulation rr given, must given rr given cens censor , rate cumumlative hazard rrc retlativ risk censoring. extend extend cumulative hazards largest end-point causes assign status values causes, vector integers ... arguments rchaz","code":""},{"path":"http://kkholst.github.io/mets/reference/rcrisk.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/rcrisk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of Piecewise constant hazard models with two causes (Cox). — rcrisk","text":"","code":"library(mets) data(bmt);  n <- 100 cox1 <- phreg(Surv(time,cause==1)~tcell+platelet,data=bmt) cox2 <- phreg(Surv(time,cause==2)~tcell+platelet,data=bmt)  X1 <- bmt[,c(\"tcell\",\"platelet\")] xid <- sample(1:nrow(X1),n,replace=TRUE) Z1 <- X1[xid,] Z2 <- X1[xid,] rr1 <- exp(as.matrix(Z1) %*% cox1$coef) rr2 <- exp(as.matrix(Z2) %*% cox2$coef)  d <-  rcrisk(cox1$cum,cox2$cum,rr1,rr2,cens=2/70) dd <- cbind(d,Z1)  d <-  rcrisk(cox1$cum,cox2$cum,rr1,rr2,cens=cbind(c(1,30,68),c(.01,1,3))) dd <- cbind(d,Z1)  par(mfrow=c(1,3)) scox0 <- phreg(Surv(time,status==0)~tcell+platelet,data=dd) plot(scox0); lines(cbind(c(1,30,68),c(.01,1,3)),col=2) ## scox1 <- phreg(Surv(time,status==1)~tcell+platelet,data=dd) scox2 <- phreg(Surv(time,status==2)~tcell+platelet,data=dd) plot(cox1); plot(scox1,add=TRUE,col=2) plot(cox2); plot(scox2,add=TRUE,col=2)  cbind(cox1$coef,scox1$coef,cox2$coef,scox2$coef) #>                [,1]        [,2]       [,3]      [,4] #> tcell    -0.4232606 -12.1354452  0.3991068 1.2674840 #> platelet -0.5654438  -0.3064046 -0.2461474 0.5558998  # 3 causes and censoring  d3 <-  rcrisk(list(cox1$cum,cox2$cum,cox1$cum),NULL,n=100,cens=cbind(c(1,30,68),c(.01,1,3))) dtable(d3,~status) #>  #> status #>  0  1  2  3  #> 19 37 13 31  #>"},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Recurrent events regression with terminal event — recreg","title":"Recurrent events regression with terminal event — recreg","text":"Fits Ghosh-Lin IPCW Cox-type model","code":""},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recurrent events regression with terminal event — recreg","text":"","code":"recreg(   formula,   data,   cause = 1,   death.code = 2,   cens.code = 0,   cens.model = ~1,   weights = NULL,   offset = NULL,   Gc = NULL,   wcomp = NULL,   marks = NULL,   augmentation.type = c(\"lindyn.augment\", \"lin.augment\"),   ... )"},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recurrent events regression with terminal event — recreg","text":"formula formula 'Event' outcome data data frame cause interest (1 default) death.code codes death (terminating event, 2 default) cens.code code censoring (0 default) cens.model stratified Cox model without covariates weights weights score equations offset offsets model Gc censoring weights time argument, default calculate Kaplan-Meier estimator, give G_c(T_i-) wcomp weights composite outcome, cause=c(1,3), might wcomp=c(1,2). marks mark value can specified, vector data-frame mark value can found events augmentation.type augmentation augmentation model given ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recurrent events regression with terminal event — recreg","text":"Cox type model : $$ E(dN_1(t)|X) = \\mu_0(t)dt exp(X^T \\beta) $$ solving Cox-type IPCW weighted score equations $$  \\int (Z - E(t)) w(t) dN_1(t) $$ $$w(t) = G(t) ((T_i \\wedge t < C_i)/G_c(T_i \\wedge t))$$ $$E(t) = S_1(t)/S_0(t)$$ $$S_j(t) = \\sum X_i^j w_i(t) \\exp(X_i^T \\beta)$$. iid decomposition beta's form $$ \\int (Z - E ) w(t) dM_1 + \\int q(s)/p(s) dM_c $$ returned iid. Events, deaths censorings specified via stop start structure Event call, via status vector cause (code), censoring-codes (cens.code) death-codes (death.code) indentifies . See example vignette.","code":""},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Recurrent events regression with terminal event — recreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/recreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recurrent events regression with terminal event — recreg","text":"","code":"## data with no ties library(mets) data(hfactioncpx12) hf <- hfactioncpx12 hf$x <- as.numeric(hf$treatment)  dd <- data.frame(treatment=levels(hf$treatment),id=1)  gl <- recreg(Event(entry,time,status)~treatment+cluster(id),data=hf,cause=1,death.code=2) summary(gl) #>  #>     n events #>  2132   1391 #>  #>  741 clusters #> coeffients: #>             Estimate      S.E.   dU^-1/2 P-value #> treatment1 -0.110404  0.078656  0.053776  0.1604 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.89547 0.76754 1.0447 #>  head(iid(gl)) #>      treatment1 #> 1 -1.266428e-04 #> 2 -6.112340e-04 #> 3  2.885192e-03 #> 4  1.308207e-03 #> 5  5.404664e-05 #> 6  2.229380e-03 pgl <- predict(gl,dd,se=1); plot(pgl,se=1)   ## censoring stratified after treatment  gls <- recreg(Event(entry,time,status)~treatment+cluster(id),data=hf, cause=1,death.code=2,cens.model=~strata(treatment)) summary(gls) #>  #>     n events #>  2132   1391 #>  #>  741 clusters #> coeffients: #>             Estimate      S.E.   dU^-1/2 P-value #> treatment1 -0.109509  0.078707  0.053777  0.1641 #>  #> exp(coeffients): #>            Estimate    2.5%  97.5% #> treatment1  0.89627 0.76815 1.0458 #>   glss <- recreg(Event(entry,time,status)~strata(treatment)+cluster(id),data=hf, cause=1,death.code=2,cens.model=~strata(treatment)) summary(glss) #>  #>     n events #>  2132   1391 #>  plot(glss)   ## IPCW at 2 years  ll2 <- recregIPCW(Event(entry,time,status)~treatment+cluster(id),data=hf, cause=1,death.code=2,time=2,cens.model=~strata(treatment)) summary(ll2) #>    n events #>  741   1052 #>  #>  741 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  0.452257  0.060901  0.332893  0.571621  0.0000 #> treatment1  -0.078348  0.093520 -0.261644  0.104948  0.4022 #>  #> exp(coeffients): #>             Estimate    2.5%  97.5% #> (Intercept)  1.57186 1.39500 1.7711 #> treatment1   0.92464 0.76979 1.1107 #>  #>   ll2i <- recregIPCW(Event(entry,time,status)~-1+treatment+cluster(id),data=hf, cause=1,death.code=2,time=2,cens.model=~strata(treatment)) summary(ll2i) #>    n events #>  741   1052 #>  #>  741 clusters #> coeffients: #>            Estimate  Std.Err     2.5%    97.5% P-value #> treatment0 0.452257 0.060901 0.332893 0.571621       0 #> treatment1 0.373909 0.070972 0.234806 0.513013       0 #>  #> exp(coeffients): #>            Estimate   2.5%  97.5% #> treatment0   1.5719 1.3950 1.7711 #> treatment1   1.4534 1.2647 1.6703 #>  #>"},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast recurrent marginal mean when death is possible — recurrentMarginal","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"Fast Marginal means recurrent events using Lin Ghosh (2000) standard errors. Fitting two models death recurent events combined prducte estimator $$ \\int_0^t  S(u|x=0) dR(u|x=0) $$ mean number recurrent events, $$ S(u|x=0) $$  probability survival baseline group, $$ dR(u|x=0) $$  hazard rate event among survivors baseline. $$ S(u|x=0) $$  estimated $$ exp(-\\Lambda_d(u|x=0) $$   $$\\Lambda_d(u|x=0) $$ cumulative baseline death.","code":""},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"","code":"recurrentMarginal(formula, data, cause = 1, ..., death.code = 2)"},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"formula Event object data data frame computation cause interest (1 default) ... Additional arguments lower level funtions death.code codes death (terminating event, 2 default)","code":""},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"Assumes ties sense jump times needs unique, particularly stratified version.","code":""},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"Cook, R. J. Lawless, J. F. (1997) Marginal analysis recurrent events terminating event. Statist. Med., 16, 911–924. Ghosh Lin (2002) Nonparametric Analysis Recurrent events death, Biometrics, 554–562.","code":""},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/recurrentMarginal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fast recurrent marginal mean when death is possible — recurrentMarginal","text":"","code":"library(mets) data(hfactioncpx12) hf <- hfactioncpx12 hf$x <- as.numeric(hf$treatment)   ##  to fit non-parametric models with just a baseline  xr <- phreg(Surv(entry,time,status==1)~cluster(id),data=hf) dr <- phreg(Surv(entry,time,status==2)~cluster(id),data=hf) par(mfrow=c(1,3)) plot(dr,se=TRUE) title(main=\"death\") plot(xr,se=TRUE) ### robust standard errors  rxr <-  robust.phreg(xr,fixbeta=1) plot(rxr,se=TRUE,robust=TRUE,add=TRUE,col=4)  ## marginal mean of expected number of recurrent events  ## out <- recurrentMarginalPhreg(xr,dr) ## summary(out,times=1:5)   ## marginal mean using formula   outN <- recurrentMarginal(Event(entry,time,status)~cluster(id),hf,cause=1,death.code=2) plot(outN,se=TRUE,col=2,add=TRUE) summary(outN,times=1:5)  #> [[1]] #>        new.time      mean         se   CI-2.5% CI-97.5% strata #> 608           1 0.8282358 0.04844543 0.7385251 0.928844      0 #> 1053          2 1.5139493 0.07039884 1.3820710 1.658412      0 #> 1282          3 2.0244982 0.08351867 1.8672476 2.194992      0 #> 1392          4 2.5004732 0.10843166 2.2967320 2.722288      0 #> 1392.1        5 2.5004732 0.10843166 2.2967320 2.722288      0 #>   ######################################################################## ###   with strata     ################################################## ######################################################################## out <- recurrentMarginal(Event(entry,time,status)~strata(treatment)+cluster(id),                          data=hf,cause=1,death.code=2) plot(out,se=TRUE,ylab=\"marginal mean\",col=1:2)   summary(out,times=1:5)  #> [[1]] #>       new.time      mean         se   CI-2.5% CI-97.5% strata #> 325          1 0.8737156 0.06783343 0.7503858 1.017315      0 #> 555          2 1.5718563 0.09572955 1.3949953 1.771140      0 #> 682          3 2.1184963 0.11385721 1.9066915 2.353829      0 #> 748          4 2.6815219 0.15451005 2.3951619 3.002118      0 #> 748.1        5 2.6815219 0.15451005 2.3951619 3.002118      0 #>  #> [[2]] #>       new.time      mean         se   CI-2.5%  CI-97.5% strata #> 284          1 0.7815557 0.06908585 0.6572305 0.9293989      1 #> 499          2 1.4534055 0.10315606 1.2646561 1.6703258      1 #> 601          3 1.9240624 0.12165771 1.6998008 2.1779119      1 #> 645          4 2.3134997 0.14963892 2.0380418 2.6261880      1 #> 645.1        5 2.3134997 0.14963892 2.0380418 2.6261880      1 #>"},{"path":"http://kkholst.github.io/mets/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. lava estimate, gof, IC, iid, twostage survival cluster, strata, Surv","code":""},{"path":"http://kkholst.github.io/mets/reference/resmean.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","title":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","text":"Restricted mean stratified Kaplan-Meier stratified Cox martingale standard error. Standard error computed using linear interpolation standard errors jump-times. Plots gives restricted mean times. Years lost can computed based decomposed years lost different causes using cif.yearslost function based integrating cumulative incidence functions. One particular feature functions restricted mean years-lost computed event times functions can plotted/viewed.  times given beyond last event time withn strata curves extrapolated using estimates cumulative incidence.","code":""},{"path":"http://kkholst.github.io/mets/reference/resmean.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","text":"","code":"resmean.phreg(x, times = NULL, covs = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/resmean.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","text":"x phreg object times possible times report restricted mean covs possible covariate Cox model ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/resmean.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/resmean.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restricted mean for stratified Kaplan-Meier or Cox model with martingale standard errors — resmean.phreg","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(408)*0.001 out1 <- phreg(Surv(time,cause!=0)~strata(tcell,platelet),data=bmt)  rm1 <- resmean.phreg(out1,times=10*(1:6)) summary(rm1) #>                       strata times     rmean  se.rmean     lower     upper #> tcell.0..platelet.0        0    10  5.863343 0.2565969  5.381388  6.388463 #> tcell.0..platelet.1        1    10  7.631912 0.3423881  6.989500  8.333367 #> tcell.1..platelet.0        2    10  7.277574 0.7092954  6.012095  8.809422 #> tcell.1..platelet.1        3    10  7.670141 0.5624556  6.643307  8.855691 #> tcell.0..platelet.0.1      0    20  9.888956 0.5393853  8.886328 11.004709 #> tcell.0..platelet.1.1      1    20 13.506402 0.8000279 12.025973 15.169075 #> tcell.1..platelet.0.1      2    20 12.103008 1.5545678  9.409379 15.567744 #> tcell.1..platelet.1.1      3    20 12.787705 1.4675869 10.211842 16.013312 #> tcell.0..platelet.0.2      0    30 13.602944 0.8315415 12.067001 15.334388 #> tcell.0..platelet.1.2      1    30 18.901242 1.2693320 16.570173 21.560242 #> tcell.1..platelet.0.2      2    30 16.191174 2.4006207 12.108018 21.651283 #> tcell.1..platelet.1.2      3    30 17.766060 2.4422086 13.570031 23.259556 #> tcell.0..platelet.0.3      0    40 17.159956 1.1235963 15.093201 19.509717 #> tcell.0..platelet.1.3      1    40 23.883637 1.7373039 20.710175 27.543375 #> tcell.1..platelet.0.3      2    40 19.549213 3.2030923 14.179532 26.952350 #> tcell.1..platelet.1.3      3    40 22.433331 3.3838515 16.691610 30.150138 #> tcell.0..platelet.0.4      0    50 20.482470 1.4110537 17.895439 23.443490 #> tcell.0..platelet.1.4      1    50 28.330687 2.1961768 24.337306 32.979320 #> tcell.1..platelet.0.4      2    50 22.746016 4.0537083 16.040114 32.255460 #> tcell.1..platelet.1.4      3    50 26.115632 4.2306906 19.011101 35.875159 #> tcell.0..platelet.0.5      0    60 23.741477 1.7038099 20.626292 27.327148 #> tcell.0..platelet.1.5      1    60 32.771213 2.6865780 27.906873 38.483438 #> tcell.1..platelet.0.5      2    60 25.942819 4.9476027 17.851839 37.700870 #> tcell.1..platelet.1.5      3    60 29.671600 5.1599202 21.101639 41.722060 #>                       years.lost #> tcell.0..platelet.0     4.136657 #> tcell.0..platelet.1     2.368088 #> tcell.1..platelet.0     2.722426 #> tcell.1..platelet.1     2.329859 #> tcell.0..platelet.0.1  10.111044 #> tcell.0..platelet.1.1   6.493598 #> tcell.1..platelet.0.1   7.896992 #> tcell.1..platelet.1.1   7.212295 #> tcell.0..platelet.0.2  16.397056 #> tcell.0..platelet.1.2  11.098758 #> tcell.1..platelet.0.2  13.808826 #> tcell.1..platelet.1.2  12.233940 #> tcell.0..platelet.0.3  22.840044 #> tcell.0..platelet.1.3  16.116363 #> tcell.1..platelet.0.3  20.450787 #> tcell.1..platelet.1.3  17.566669 #> tcell.0..platelet.0.4  29.517530 #> tcell.0..platelet.1.4  21.669313 #> tcell.1..platelet.0.4  27.253984 #> tcell.1..platelet.1.4  23.884368 #> tcell.0..platelet.0.5  36.258523 #> tcell.0..platelet.1.5  27.228787 #> tcell.1..platelet.0.5  34.057181 #> tcell.1..platelet.1.5  30.328400 par(mfrow=c(1,2)) plot(rm1,se=1) plot(rm1,years.lost=TRUE,se=1)   ## comparing populations, can also be done using rmstIPCW via influence functions rm1 <- resmean.phreg(out1,times=40) e1 <- estimate(rm1) e1 #>    Estimate Std.Err  2.5% 97.5%   P-value #> p1    17.16   1.124 14.96 19.36 1.169e-52 #> p2    23.88   1.737 20.48 27.29 5.270e-43 #> p3    19.55   3.203 13.27 25.83 1.039e-09 #> p4    22.43   3.384 15.80 29.07 3.368e-11 estimate(e1,rbind(c(1,-1,0,0))) #>             Estimate Std.Err   2.5%  97.5%  P-value #> [p1] - [p2]   -6.724   2.069 -10.78 -2.669 0.001155 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0   ## years.lost decomposed into causes drm1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=10*(1:6)) par(mfrow=c(1,2)); plot(drm1,cause=1,se=1); plot(drm1,cause=2,se=1);  summary(drm1) #> $estimate #>                       strata times    intF_1     intF_2 se.intF_1 se.intF_2 #> tcell.0..platelet.0        0    10  3.117728  1.0189288 0.2487176 0.1703638 #> tcell.0..platelet.1        1    10  1.710936  0.6571526 0.3238412 0.1870601 #> tcell.1..platelet.0        2    10  1.876143  0.8462836 0.6339082 0.4726466 #> tcell.1..platelet.1        3    10  1.358636  0.9712223 0.5303228 0.3617247 #> tcell.0..platelet.0.1      0    20  7.517571  2.5934726 0.5441135 0.3861255 #> tcell.0..platelet.1.1      1    20  4.230955  2.2626433 0.7414105 0.5327638 #> tcell.1..platelet.0.1      2    20  4.568450  3.3285417 1.4876793 1.1718383 #> tcell.1..platelet.1.1      3    20  3.569521  3.6427738 1.3003137 1.1906605 #> tcell.0..platelet.0.2      0    30 12.105134  4.2919224 0.8508105 0.6161428 #> tcell.0..platelet.1.2      1    30  6.884188  4.2145701 1.1741030 0.9057072 #> tcell.1..platelet.0.2      2    30  7.260758  6.5480678 2.3532886 1.9703396 #> tcell.1..platelet.1.2      3    30  5.780405  6.4535344 2.0925102 2.0815222 #> tcell.0..platelet.0.3      0    40 16.718644  6.1214006 1.1626277 0.8509971 #> tcell.0..platelet.1.3      1    40  9.728017  6.3883460 1.6094995 1.2998361 #> tcell.1..platelet.0.3      2    40  9.953066 10.4977217 3.2212066 2.8144091 #> tcell.1..platelet.1.3      3    40  8.302374  9.2642951 2.8718027 2.9840891 #> tcell.0..platelet.0.4      0    50 21.367828  8.1497025 1.4766462 1.0945185 #> tcell.0..platelet.1.4      1    50 12.979249  8.6900648 2.0475172 1.7124440 #> tcell.1..platelet.0.4      2    50 12.645373 14.6086108 4.0899639 3.7302613 #> tcell.1..platelet.1.4      3    50 11.809312 12.0750558 3.6736890 3.8902225 #> tcell.0..platelet.0.5      0    60 26.017012 10.2415106 1.7930406 1.3471492 #> tcell.0..platelet.1.5      1    60 16.237003 10.9917837 2.5097421 2.1389931 #> tcell.1..platelet.0.5      2    60 15.337681 18.7194999 4.9591193 4.6873851 #> tcell.1..platelet.1.5      3    60 15.442583 14.8858164 4.5899631 4.7979021 #>                       total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell.0..platelet.0           4.136657    2.6664497     3.645382    0.7342160 #> tcell.0..platelet.1           2.368088    1.1806486     2.479401    0.3761574 #> tcell.1..platelet.0           2.722426    0.9675272     3.638049    0.2832191 #> tcell.1..platelet.1           2.329859    0.6321933     2.919824    0.4680527 #> tcell.0..platelet.0.1        10.111044    6.5233175     8.663364    1.9370986 #> tcell.0..platelet.1.1         6.493598    3.0010849     5.964836    1.4262364 #> tcell.1..platelet.0.1         7.896992    2.4131367     8.648800    1.6694832 #> tcell.1..platelet.1.1         7.212295    1.7479626     7.289331    1.9196070 #> tcell.0..platelet.0.2        16.397056   10.5473368    13.893011    3.2393246 #> tcell.0..platelet.1.2        11.098758    4.9281026     9.616693    2.7658642 #> tcell.1..platelet.0.2        13.808826    3.8467904    13.704570    3.6306405 #> tcell.1..platelet.1.2        12.233940    2.8433007    11.751512    3.4296609 #> tcell.0..platelet.0.3        22.840044   14.5884049    19.159946    4.6614042 #> tcell.0..platelet.1.3        16.116363    7.0338498    13.454127    4.2874053 #> tcell.1..platelet.0.3        20.450787    5.2780598    18.768926    6.2071231 #> tcell.1..platelet.1.3        17.566669    4.2147370    16.354380    4.9275936 #> tcell.0..platelet.0.4        29.517530   18.6610983    24.467159    6.2635995 #> tcell.0..platelet.1.4        21.669313    9.5272998    17.681914    5.9059153 #> tcell.1..platelet.0.4        27.253984    6.7084592    23.836393    8.8564030 #> tcell.1..platelet.1.4        23.884368    6.4184377    21.728005    6.4218095 #> tcell.0..platelet.0.5        36.258523   22.7297314    29.779715    7.9140457 #> tcell.0..platelet.1.5        27.228787   11.9932328    21.982420    7.5062836 #> tcell.1..platelet.0.5        34.057181    8.1384451    28.905332   11.4591489 #> tcell.1..platelet.1.5        30.328400    8.6242095    27.651622    7.9144140 #>                       upper_intF_2 #> tcell.0..platelet.0       1.414047 #> tcell.0..platelet.1       1.148055 #> tcell.1..platelet.0       2.528769 #> tcell.1..platelet.1       2.015313 #> tcell.0..platelet.0.1     3.472255 #> tcell.0..platelet.1.1     3.589556 #> tcell.1..platelet.0.1     6.636299 #> tcell.1..platelet.1.1     6.912770 #> tcell.0..platelet.0.2     5.686555 #> tcell.0..platelet.1.2     6.422080 #> tcell.1..platelet.0.2    11.809815 #> tcell.1..platelet.1.2    12.143506 #> tcell.0..platelet.0.3     8.038682 #> tcell.0..platelet.1.3     9.518803 #> tcell.1..platelet.0.3    17.754145 #> tcell.1..platelet.1.3    17.417663 #> tcell.0..platelet.0.4    10.603751 #> tcell.0..platelet.1.4    12.786710 #> tcell.1..platelet.0.4    24.096861 #> tcell.1..platelet.1.4    22.704967 #> tcell.0..platelet.0.5    13.253467 #> tcell.0..platelet.1.5    16.095756 #> tcell.1..platelet.0.5    30.579904 #> tcell.1..platelet.1.5    27.997971 #>   ## comparing populations, can also be done using rmstIPCW via influence functions drm1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=40) summary(drm1) #> $estimate #>                     strata times    intF_1    intF_2 se.intF_1 se.intF_2 #> tcell=0, platelet=0      0    40 16.718644  6.121401  1.162628 0.8509971 #> tcell=0, platelet=1      1    40  9.728017  6.388346  1.609500 1.2998361 #> tcell=1, platelet=0      2    40  9.953066 10.497722  3.221207 2.8144091 #> tcell=1, platelet=1      3    40  8.302374  9.264295  2.871803 2.9840891 #>                     total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell=0, platelet=0         22.84004    14.588405     19.15995     4.661404 #> tcell=0, platelet=1         16.11636     7.033850     13.45413     4.287405 #> tcell=1, platelet=0         20.45079     5.278060     18.76893     6.207123 #> tcell=1, platelet=1         17.56667     4.214737     16.35438     4.927594 #>                     upper_intF_2 #> tcell=0, platelet=0     8.038682 #> tcell=0, platelet=1     9.518803 #> tcell=1, platelet=0    17.754145 #> tcell=1, platelet=1    17.417663 #>  ## first cause  e1 <- estimate(drm1) estimate(e1,rbind(c(1,-1,0,0))) #>             Estimate Std.Err  2.5% 97.5%   P-value #> [p1] - [p2]    6.991   1.985 3.099 10.88 0.0004302 #>  #>  Null Hypothesis:  #>   [p1] - [p2] = 0"},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":null,"dir":"Reference","previous_headings":"","what":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"standard causal assumptions  can estimate average treatment effect E(Y(1) - Y(0)). need Consistency, ignorability ( Y(1), Y(0) indep given X), positivity.","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"","code":"resmeanATE(formula, data, model = \"exp\", outcome = c(\"rmst\", \"rmtl\"), ...)"},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"formula formula 'Event' outcome data data-frame model exp (\"exp\") identity link (\"lin\") outcome restricted mean time (rmst) restricted mean time lost (rmtl) ... Additional arguments pass binregATE","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"first covariate specification competing risks regression model must treatment effect factor. factor two levels uses mlogit propensity score modelling.  consider outcome mint(T;tau)  (epsion==cause1)(t- min(T;t)) gives years lost due cause \"cause\" depending number causes. default model exp(X^ beta) otherwise linear model used. Estimates ATE using standard binary double robust estimating equations IPCW censoring adjusted.","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"Scheike, T.  Holst, K. K. Restricted mean time lost survival competing risks data using mets R, WIP","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanATE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Average Treatment effect for Restricted Mean for censored competing risks data using IPCW — resmeanATE","text":"","code":"library(mets); data(bmt); bmt$event <- bmt$cause!=0; dfactor(bmt) <- tcell~tcell out <- resmeanATE(Event(time,event)~tcell+platelet,data=bmt,time=40,treat.model=tcell~platelet) summary(out) #>    n events #>  408    241 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  2.852563  0.062496  2.730074  2.975052  0.0000 #> tcell1       0.021286  0.122983 -0.219757  0.262329  0.8626 #> platelet     0.303306  0.090772  0.125396  0.481215  0.0008 #>  #> exp(coeffients): #>             Estimate     2.5%  97.5% #> (Intercept) 17.33214 15.33402 19.591 #> tcell1       1.02151  0.80271  1.300 #> platelet     1.35433  1.13360  1.618 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.25882  0.95918 17.37887 21.13877  0.0000 #> treat1    19.67316  2.22868 15.30502 24.04129  0.0000 #> treat:1-0  0.41434  2.41151 -4.31213  5.14081  0.8636 #>  #> Average Treatment effects (double robust) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    19.27793  0.95799 17.40030 21.15556  0.0000 #> treat1    20.34004  2.54146 15.35887 25.32122  0.0000 #> treat:1-0  1.06211  2.71020 -4.24979  6.37402  0.6951 #>  #>   out1 <- resmeanATE(Event(time,cause)~tcell+platelet,data=bmt,cause=1,time=40,                    treat.model=tcell~platelet) summary(out1) #>    n events #>  408    157 #>  #>  408 clusters #> coeffients: #>             Estimate  Std.Err     2.5%    97.5% P-value #> (Intercept)  2.80626  0.06962  2.66981  2.94271  0.0000 #> tcell1      -0.37413  0.24769 -0.85960  0.11133  0.1309 #> platelet    -0.49164  0.16493 -0.81490 -0.16837  0.0029 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 16.54790 14.43717 18.9672 #> tcell1       0.68788  0.42333  1.1178 #> platelet     0.61162  0.44268  0.8450 #>  #> Average Treatment effects (G-formula) : #>           Estimate  Std.Err     2.5%    97.5% P-value #> treat0    14.53165  0.95705 12.65587 16.40742  0.0000 #> treat1     9.99609  2.37815  5.33499 14.65718  0.0000 #> treat:1-0 -4.53556  2.57515 -9.58276  0.51164  0.0782 #>  #> Average Treatment effects (double robust) : #>            Estimate   Std.Err      2.5%     97.5% P-value #> treat0     14.51355   0.95800  12.63590  16.39120  0.0000 #> treat1      9.36465   2.41708   4.62727  14.10203  0.0001 #> treat:1-0  -5.14890   2.59798 -10.24084  -0.05696  0.0475 #>  #>   ratioATE(out,out1,h=function(x) log(x)) #> $ratioG #>            Estimate Std.Err    2.5%    97.5%   P-value #> [treat0]    -0.2816  0.1075 -0.4924 -0.07093 9.150e-33 #> [treat1]    -0.6771  0.3183 -1.3008 -0.05328 1.369e-07 #> [treat0].1  -0.3954  0.3363 -1.0545  0.26370 3.333e-05 #> [treat1].1   0.3954  0.3363 -0.2637  1.05454 7.221e-02 #>  #>  Null Hypothesis:  #>   [treat0] = 1 #>   [treat1] = 1 #>   [treat0] = 1 #>   [treat1] = 1  #>   #> chisq = 170.3366, df = 4, p-value < 2.2e-16 #>  #> $ratioDR #>            Estimate Std.Err    2.5%    97.5%   P-value #> [treat0]    -0.2839  0.1075 -0.4947 -0.07310 7.454e-33 #> [treat1]    -0.7756  0.3460 -1.4538 -0.09753 2.865e-07 #> [treat0].1  -0.4918  0.3619 -1.2011  0.21754 3.756e-05 #> [treat1].1   0.4918  0.3619 -0.2175  1.20109 1.602e-01 #>  #>  Null Hypothesis:  #>   [treat0] = 1 #>   [treat1] = 1 #>   [treat0] = 1 #>   [treat1] = 1  #>   #> chisq = 168.3765, df = 4, p-value < 2.2e-16 #>"},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":null,"dir":"Reference","previous_headings":"","what":"Restricted IPCW mean for censored survival data — resmeanIPCW","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"Simple fast version IPCW regression just one time-point thus fitting model $$E( min(T, t) | X ) = exp( X^T beta) $$ case competing risks data $$E( (epsilon=1) (t - min(T ,t)) | X ) = exp( X^T beta) $$ thus given years lost cause, see binreg arguments.","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"","code":"resmeanIPCW(formula, data, outcome = c(\"rmst\", \"rmtl\"), ...)"},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"formula formula outcome Event form data data frame outcome can either rmst regression ('rmst') years-lost regression  ('rmtl') ... Additional arguments binreg","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"status binary assumes survival setting default consider outcome Y=min(T,t), status two levels, computes years lost due specified cause, thus using response $$ Y = (t-min(T,t)) (status=cause) $$ Based binomial regresion IPCW response estimating equation: $$ X ( \\Delta(min(T,t)) Y /G_c(min(T,t)) - exp( X^T beta)) = 0 $$ IPCW adjusted responses. $$ \\Delta(min(T,t)) = ( min(T ,t) \\leq C ) $$ indicator uncensored.  Concretely, uncensored observations time t count event (type) t censoring time t . One therefore bit careful data constructed event times T equivalent t. Can also solve binomial regresion IPCW response estimating equation: $$ h(X) X ( \\Delta(min(T,t)) Y /G_c(min(T,t)) - exp( X^T beta)) = 0 $$ IPCW adjusted responses $h$ given argument together iid censoring h. using appropriately  h argument can also efficient IPCW estimator estimator. Variance based  $$ \\sum w_i^2 $$ also IPCW adjustment, naive.var variance known censoring model. Ydirect given solves : $$ X ( \\Delta(min(T,t)) Ydirect /G_c(min(T,t)) - exp( X^T beta)) = 0 $$ IPCW adjusted responses. actual influence (type=\"II\") function based augmenting $$ X \\int_0^t E(Y | T>s) /G_c(s) dM_c(s) $$ alternatively just solved directly (type=\"\") without additional terms. Censoring model may depend strata.","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"Scheike, T.  Holst, K. K. Restricted mean time lost survival competing risks data using mets R, WIP","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/resmeanIPCW.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restricted IPCW mean for censored survival data — resmeanIPCW","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(nrow(bmt))*0.001 # E( min(T;t) | X ) = exp( a+b X) with IPCW estimation  out <- resmeanIPCW(Event(time,cause!=0)~tcell+platelet+age,bmt,                 time=50,cens.model=~strata(platelet),model=\"exp\") summary(out) #>    n events #>  408    245 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  3.014393  0.065110  2.886780  3.142007  0.0000 #> tcell        0.106872  0.138216 -0.164027  0.377771  0.4394 #> platelet     0.246943  0.097333  0.056173  0.437712  0.0112 #> age         -0.185971  0.043566 -0.271359 -0.100583  0.0000 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 20.37672 17.93546 23.1503 #> tcell        1.11279  0.84872  1.4590 #> platelet     1.28011  1.05778  1.5492 #> age          0.83030  0.76234  0.9043 #>  #>   ## weighted GLM version   RMST out2 <- logitIPCW(Event(time,cause!=0)~tcell+platelet+age,bmt,             time=50,cens.model=~strata(platelet),model=\"exp\",outcome=\"rmst\") summary(out2) #>    n   events #>  408 7467.653 #>  #>  408 clusters #> coeffients: #>               Estimate    Std.Err       2.5%      97.5% P-value #> (Intercept)  3.0201981  0.0748709  2.8734539  3.1669423  0.0000 #> tcell        0.0249363  0.1868557 -0.3412942  0.3911667  0.8938 #> platelet     0.2530618  0.1262735  0.0055703  0.5005532  0.0451 #> age         -0.1803245  0.0526438 -0.2835045 -0.0771445  0.0006 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 20.49535 17.69804 23.7348 #> tcell        1.02525  0.71085  1.4787 #> platelet     1.28796  1.00559  1.6496 #> age          0.83500  0.75314  0.9258 #>  #>   ### time-lost outtl <- resmeanIPCW(Event(time,cause!=0)~tcell+platelet+age,bmt,                 time=50,cens.model=~strata(platelet),model=\"exp\",outcome=\"rmtl\") summary(outtl) #>    n events #>  408    245 #>  #>  408 clusters #> coeffients: #>              Estimate   Std.Err      2.5%     97.5% P-value #> (Intercept)  3.361764  0.047705  3.268264  3.455265  0.0000 #> tcell       -0.085417  0.127458 -0.335230  0.164396  0.5028 #> platelet    -0.239822  0.100764 -0.437317 -0.042327  0.0173 #> age          0.175688  0.044858  0.087769  0.263607  0.0001 #>  #> exp(coeffients): #>             Estimate     2.5%   97.5% #> (Intercept) 28.84003 26.26571 31.6667 #> tcell        0.91813  0.71517  1.1787 #> platelet     0.78677  0.64577  0.9586 #> age          1.19207  1.09174  1.3016 #>  #>   ### same as Kaplan-Meier for full censoring model  bmt$int <- with(bmt,strata(tcell,platelet)) out <- resmeanIPCW(Event(time,cause!=0)~-1+int,bmt,time=30,                              cens.model=~strata(platelet,tcell),model=\"lin\") estimate(out) #>                        Estimate Std.Err  2.5% 97.5%   P-value #> inttcell=0, platelet=0    13.60  0.8316 11.97 15.23 3.824e-60 #> inttcell=0, platelet=1    18.90  1.2696 16.41 21.39 4.002e-50 #> inttcell=1, platelet=0    16.19  2.4061 11.48 20.91 1.705e-11 #> inttcell=1, platelet=1    17.77  2.4536 12.96 22.58 4.464e-13 out1 <- phreg(Surv(time,cause!=0)~strata(tcell,platelet),data=bmt) rm1 <- resmean.phreg(out1,times=30) summary(rm1) #>                     strata times    rmean  se.rmean    lower    upper #> tcell=0, platelet=0      0    30 13.60295 0.8315412 12.06701 15.33440 #> tcell=0, platelet=1      1    30 18.90126 1.2693314 16.57019 21.56026 #> tcell=1, platelet=0      2    30 16.19122 2.4006180 12.10806 21.65132 #> tcell=1, platelet=1      3    30 17.76607 2.4422046 13.57005 23.25956 #>                     years.lost #> tcell=0, platelet=0   16.39705 #> tcell=0, platelet=1   11.09874 #> tcell=1, platelet=0   13.80878 #> tcell=1, platelet=1   12.23393  ### years lost regression outl <- resmeanIPCW(Event(time,cause!=0)~-1+int,bmt,time=30,outcome=\"years-lost\",                              cens.model=~strata(platelet,tcell),model=\"lin\") estimate(outl) #>                        Estimate Std.Err   2.5% 97.5%   P-value #> inttcell=0, platelet=0    16.40  0.8315 14.767 18.03 1.485e-86 #> inttcell=0, platelet=1    11.10  1.2693  8.611 13.59 2.255e-18 #> inttcell=1, platelet=0    13.81  2.4006  9.104 18.51 8.811e-09 #> inttcell=1, platelet=1    12.23  2.4423  7.447 17.02 5.467e-07  ## competing risks years-lost for cause 1   out <- resmeanIPCW(Event(time,cause)~-1+int,bmt,time=30,cause=1,                             cens.model=~strata(platelet,tcell),model=\"lin\") estimate(out) #>                        Estimate Std.Err   2.5%  97.5%   P-value #> inttcell=0, platelet=0   12.105  0.8508 10.438 13.773 6.162e-46 #> inttcell=0, platelet=1    6.884  1.1741  4.583  9.185 4.536e-09 #> inttcell=1, platelet=0    7.261  2.3533  2.648 11.873 2.033e-03 #> inttcell=1, platelet=1    5.780  2.0925  1.679  9.882 5.737e-03 ## same as integrated cumulative incidence  rmc1 <- cif.yearslost(Event(time,cause)~strata(tcell,platelet),data=bmt,times=30) summary(rmc1) #> $estimate #>                     strata times    intF_1   intF_2 se.intF_1 se.intF_2 #> tcell=0, platelet=0      0    30 12.105108 4.291939 0.8508088 0.6161454 #> tcell=0, platelet=1      1    30  6.884191 4.214551 1.1741028 0.9057053 #> tcell=1, platelet=0      2    30  7.260755 6.548026 2.3532855 1.9703338 #> tcell=1, platelet=1      3    30  5.780372 6.453554 2.0924973 2.0815288 #>                     total.years.lost lower_intF_1 upper_intF_1 lower_intF_2 #> tcell=0, platelet=0         16.39705    10.547314    13.892982     3.239337 #> tcell=0, platelet=1         11.09874     4.928105     9.616694     2.765849 #> tcell=1, platelet=0         13.80878     3.846791    13.704556     3.630610 #> tcell=1, platelet=1         12.23393     2.843285    11.751441     3.429671 #>                     upper_intF_2 #> tcell=0, platelet=0     5.686578 #> tcell=0, platelet=1     6.422058 #> tcell=1, platelet=0    11.809764 #> tcell=1, platelet=1    12.143543 #>"},{"path":"http://kkholst.github.io/mets/reference/rpch.html","id":null,"dir":"Reference","previous_headings":"","what":"Piecewise constant hazard distribution — rpch","title":"Piecewise constant hazard distribution — rpch","text":"Piecewise constant hazard distribution","code":""},{"path":"http://kkholst.github.io/mets/reference/rpch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Piecewise constant hazard distribution — rpch","text":"","code":"rpch(n, lambda = 1, breaks = c(0, Inf))"},{"path":"http://kkholst.github.io/mets/reference/rpch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Piecewise constant hazard distribution — rpch","text":"n sample size lambda rate parameters breaks time cut-points","code":""},{"path":"http://kkholst.github.io/mets/reference/rweibullcox.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate observations from a Weibull distribution — rweibullcox","title":"Simulate observations from a Weibull distribution — rweibullcox","text":"Simulate observations model cumulative hazard   given $$\\Lambda(t) = \\lambda\\cdot t^s$$ \\(\\lambda\\)   rate parameter \\(s\\) shape parameter.","code":""},{"path":"http://kkholst.github.io/mets/reference/rweibullcox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate observations from a Weibull distribution — rweibullcox","text":"","code":"rweibullcox(n, rate, shape)"},{"path":"http://kkholst.github.io/mets/reference/rweibullcox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate observations from a Weibull distribution — rweibullcox","text":"n (integer) number observations rate (numeric) rate parameter (can vector size n) shape (numeric) shape parameter (can vector size n)","code":""},{"path":"http://kkholst.github.io/mets/reference/rweibullcox.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate observations from a Weibull distribution — rweibullcox","text":"[stats::rweibull()] uses different parametrization   cumulative hazard given $$H(t) = (t/b)^,$$ .e., shape \\(:=s\\) scale paramter \\(b\\) related rate paramter \\(r\\) $$r := b^{-}$$","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/sim.cif.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of output from Cumulative incidence regression model — sim.cif","title":"Simulation of output from Cumulative incidence regression model — sim.cif","text":"Simulates data looks like fit fitted cumulative incidence model","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.cif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of output from Cumulative incidence regression model — sim.cif","text":"","code":"sim.cif(   cif,   n,   data = NULL,   Z = NULL,   rr = NULL,   strata = NULL,   drawZ = TRUE,   cens = NULL,   rrc = NULL,   cumstart = c(0, 0),   U = NULL,   pU = NULL,   type = NULL,   extend = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/sim.cif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of output from Cumulative incidence regression model — sim.cif","text":"cif output form prop.odds.subdist ccr (cmprsk), can also call invsubdist cumulative linear predictor n number simulations. data extract covariates simulations (draws observed covariates). Z use covariates simulation rather drawing new ones. rr possible vector relative risk cox model. strata possible vector strata drawZ random sample Z cens specifies censoring model, \".matrix\" uses cumulative hazard given, \".scalar\" uses rate exponential, given takes average rate simulated data cox model. rrc possible vector relative risk cox-type censoring. cumstart start cumulatives time 0 0. U uniforms use drawing timing cumulative incidence. pU uniforms use drawing event type (F1,F2,1-F1-F2). type model logistic,cloglog,rr extend extend piecewise constant constant rate. Default average rate time cumulative (TRUE), numeric uses given rate. ... arguments simsubdist (example Uniform variable realizations)","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.cif.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of output from Cumulative incidence regression model — sim.cif","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.cif.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of output from Cumulative incidence regression model — sim.cif","text":"","code":"library(mets) data(bmt) nsim <- 100  ## logit cumulative incidence regression model  cif <- cifreg(Event(time,cause)~platelet+age,data=bmt,cause=1) estimate(cif)   #>          Estimate Std.Err    2.5%    97.5%   P-value #> platelet  -0.5300 0.23329 -0.9873 -0.07275 0.0230973 #> age        0.3553 0.09611  0.1669  0.54365 0.0002187 plot(cif,col=1) simbmt <- sim.cif(cif,nsim,data=bmt) dtable(simbmt,~status) #>  #> status #>  0  1  #> 62 38  #>  scif <- cifreg(Event(time,status)~platelet+age,data=simbmt,cause=1) estimate(scif) #>          Estimate Std.Err    2.5%  97.5% P-value #> platelet   0.1401  0.4363 -0.7150 0.9953 0.74808 #> age        0.4783  0.2072  0.0722 0.8843 0.02097 plot(scif,add=TRUE,col=2)   ## Fine-Gray cloglog cumulative incidence regression model  cif <- cifregFG(Event(time,cause)~strata(tcell)+age,data=bmt,cause=1) estimate(cif)   #>     Estimate Std.Err   2.5%  97.5%   P-value #> age   0.3584 0.07883 0.2039 0.5129 5.469e-06 plot(cif,col=1) simbmt <- sim.cif(cif,nsim,data=bmt) scif <- cifregFG(Event(time,status)~strata(tcell)+age,data=simbmt,cause=1) estimate(scif) #>     Estimate Std.Err     2.5%  97.5% P-value #> age   0.3866  0.1936 0.007124 0.7661 0.04585 plot(scif,add=TRUE,col=2)   ################################################################ #  simulating several causes with specific cumulatives  ################################################################ cif1 <-  cifreg(Event(time,cause)~strata(tcell)+age,data=bmt,cause=1) cif2 <-  cifreg(Event(time,cause)~strata(platelet)+tcell+age,data=bmt,cause=2) cifss <-  list(cif1,cif2) simbmt <- sim.cifs(list(cif1,cif2),nsim,data=bmt,extend=0.005) dtable(simbmt,~status) #>  #> status #>  0  1  2  #> 49 41 10  #>  scif1 <-  cifreg(Event(time,status)~strata(tcell)+age,data=simbmt,cause=1) scif2 <-  cifreg(Event(time,status)~strata(platelet)+tcell+age,data=simbmt,cause=2) cbind(cif1$coef,scif1$coef)    #>          [,1]      [,2] #> age 0.4157306 0.6036799 ## can be off due to restriction F1+F2<= 1     cbind(cif2$coef,scif2$coef)    #>              [,1]      [,2] #> tcell  0.68379711 1.2507661 #> age   -0.03484497 0.9837646      par(mfrow=c(1,2))    ## Cause 1 follows the model     plot(cif1); plot(scif1,add=TRUE,col=1:2,lwd=2) # Cause 2:second cause is modified with restriction to satisfy F1+F2<= 1, so scaled down      plot(cif2); plot(scif2,add=TRUE,col=1:2,lwd=2)"},{"path":"http://kkholst.github.io/mets/reference/sim.phreg.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of output from Cox model. — sim.phreg","title":"Simulation of output from Cox model. — sim.phreg","text":"Simulates data looks like fit Cox model. Censor data automatically highest value event times using cumulative hazard.","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phreg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of output from Cox model. — sim.phreg","text":"","code":"sim.phreg(   cox,   n,   data = NULL,   Z = NULL,   rr = NULL,   strata = NULL,   entry = NULL,   extend = NULL,   cens = NULL,   rrc = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/sim.phreg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of output from Cox model. — sim.phreg","text":"cox output form coxph cox.aalen model fitting cox model. n number simulations. data extract covariates simulations (draws observed covariates). Z give design matrix instead data rr possible vector relative risk cox model. strata possible vector strata entry delayed entry variable simulation. extend extend possible stratified baselines largest end-point given takes average rate simulated data cox model. cens specifies censoring model, \".matrix\" uses cumulative hazard given, \".scalar\" uses rate exponential, rrc possible vector relative risk cox-type censoring. ... arguments rchaz, example entry-time.","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phreg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of output from Cox model. — sim.phreg","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phreg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of output from Cox model. — sim.phreg","text":"","code":"library(mets) data(sTRACE) nsim <- 100 coxs <-  phreg(Surv(time,status==9)~strata(chf)+vf+wmi,data=sTRACE) set.seed(100) sim3 <- sim.phreg(coxs,nsim,data=sTRACE) head(sim3) #>    entry         time status        rr id      time status chf vf wmi orig.id #> 1      0 0.0005400588      1 0.9366566  1 0.2452262      9   1  1 0.4     202 #> 45     0 7.8250000000      0 0.2405435  2 7.1560000      0   0  0 1.6     358 #> 46     0 7.8250000000      0 0.2405435  3 6.2800000      0   0  0 1.6     112 #> 47     0 2.3192112736      1 0.2012996  4 7.5980000      0   0  0 1.8     499 #> 48     0 2.8507169261      1 0.4104366  5 6.4740000      0   0  0 1.0     473 #> 49     0 7.8250000000      0 0.2874382  6 6.1510000      0   0  0 1.4     206 cc <-   phreg(Surv(time,status)~strata(chf)+vf+wmi,data=sim3) cbind(coxs$coef,cc$coef) #>           [,1]      [,2] #> vf   0.2907750  1.795377 #> wmi -0.8905339 -1.100672 plot(coxs,col=1); plot(cc,add=TRUE,col=2)   Z <- sim3[,c(\"vf\",\"chf\",\"wmi\")] strata <- sim3[,c(\"chf\")] rr <- exp(as.matrix(Z[,-2]) %*% coef(coxs)) sim4 <- sim.phreg(coxs,nsim,data=NULL,rr=rr,strata=strata) sim4 <- cbind(sim4,Z) cc <-   phreg(Surv(time,status)~strata(chf)+vf+wmi,data=sim4) cbind(coxs$coef,cc$coef) #>           [,1]        [,2] #> vf   0.2907750  0.05398059 #> wmi -0.8905339 -1.25463072 plot(coxs,col=1); plot(cc,add=TRUE,col=2)"},{"path":"http://kkholst.github.io/mets/reference/sim.phregs.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of cause specific from Cox models. — sim.phregs","title":"Simulation of cause specific from Cox models. — sim.phregs","text":"Simulates data looks like fit cause specific Cox models. Censor data automatically. censoring given  list causes give censoring looks like data.  Covariates drawn data-set replacement. gives covariates like data.  Calls sim.phregs","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phregs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of cause specific from Cox models. — sim.phregs","text":"","code":"sim.phregs(   coxs,   n,   data = NULL,   rr = NULL,   strata = NULL,   entry = NULL,   extend = NULL,   cens = NULL,   rrc = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/sim.phregs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of cause specific from Cox models. — sim.phregs","text":"coxs list cox models. n number simulations. data extract covariates simulations (draws observed covariates). rr possible vector relative risk cox model. strata possible vector strata entry delayed entry variable simulation. extend extend possible stratified baselines largest end-point cens specifies censoring model, NULL censoring   cause end last event type. \".matrix\" uses cumulative.  hazard given, \".scalar\" uses rate exponential,  given takes average rate simulated data cox model.  censoring can also given cause. rrc possible vector relative risk cox-type censoring. ... arguments rchaz, example entry-time","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phregs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of cause specific from Cox models. — sim.phregs","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.phregs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of cause specific from Cox models. — sim.phregs","text":"","code":"library(mets) data(bmt) nsim <- 100;   cox1 <- phreg(Surv(time,cause==1)~strata(tcell)+platelet+age,data=bmt) cox2 <- phreg(Surv(time,cause==2)~tcell+strata(platelet),data=bmt) coxs <- list(cox1,cox2) ## just calls sim.phregs ! dd <- sim.phregs(coxs,nsim,data=bmt,extend=c(0.001)) scox1 <- phreg(Surv(time,status==1)~strata(tcell)+platelet+age,data=dd) scox2 <- phreg(Surv(time,status==2)~tcell+strata(platelet),data=dd)  cbind(cox1$coef,scox1$coef) #>                [,1]       [,2] #> platelet -0.5215632 -0.8736568 #> age       0.4058943  0.4505475 cbind(cox2$coef,scox2$coef) #>            [,1]       [,2] #> tcell 0.4153706 0.04614013 par(mfrow=c(1,2)) plot(cox1); plot(scox1,add=TRUE);  plot(cox2); plot(scox2,add=TRUE);"},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"Simulation two-stage recurrent events data based Cox/Cox Cox/Ghosh-Lin structure","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"","code":"sim.recurrent(cox1,coxd=NULL,coxc=NULL,n=1,data=NULL, type=c(\"default\",\"cox-cox\",\"gl-cox\"),id=\"id\", varz=1,share=1,cens=0.001,scale1=1,scaled=1,dependence=NULL,...)"},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"cox1 cox/ghosh-lin recurrent events coxd cox terminal event (phreg) coxc possible cox censoring (phreg) n number id's data models fitted (draw covariates) type specify type simulation, default id name id variable varz dependence frailty share fit patly shared random effects model cens censoring rate exponential censoring scale1 scale baseline recurrent events model scaled scale baseline terminal event dependence dependence different NULL, uses simRecurrentList based models given ... Additional arguments simGLcox, nmin, nmax regulates linear approximation grid","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"Must specify two phreg objects, phreg recreg object, simulates data two-stage model","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"Scheike (2024), Twostage recurrent events models, review.","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/sim.recurrent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — sim.recurrent","text":"","code":"data(hfactioncpx12) hf <- hfactioncpx12 hf$x <- as.numeric(hf$treatment) n <- 100 xr <- phreg(Surv(entry,time,status==1)~x+cluster(id),data=hf) dr <- phreg(Surv(entry,time,status==2)~x+cluster(id),data=hf) simcoxcox <- sim.recurrent(xr,dr,n=n,data=hf) recGL <- recreg(Event(entry,time,status)~x+cluster(id),hf,death.code=2) simglcox <- sim.recurrent(recGL,dr,n=n,data=hf)"},{"path":"http://kkholst.github.io/mets/reference/simAalenFrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate from the Aalen Frailty model — simAalenFrailty","title":"Simulate from the Aalen Frailty model — simAalenFrailty","text":"Simulate observations Aalen Frailty model Gamma distributed frailty constant intensity.","code":""},{"path":"http://kkholst.github.io/mets/reference/simAalenFrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate from the Aalen Frailty model — simAalenFrailty","text":"","code":"simAalenFrailty(   n = 5000,   theta = 0.3,   K = 2,   beta0 = 1.5,   beta = 1,   cens = 1.5,   cuts = 0,   ... )"},{"path":"http://kkholst.github.io/mets/reference/simAalenFrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate from the Aalen Frailty model — simAalenFrailty","text":"n Number observations cluster theta Dependence paramter (variance frailty) K Number clusters beta0 Baseline (intercept) beta Effect (log hazard ratio) covariate cens Censoring rate cuts time cuts ... Additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/simAalenFrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate from the Aalen Frailty model — simAalenFrailty","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakes.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate from the Clayton-Oakes frailty model — simClaytonOakes","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakes","text":"Simulate observations Clayton-Oakes copula model piecewise constant marginals.","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakes","text":"","code":"simClaytonOakes(   K,   n,   eta,   beta,   stoptime,   lam = 1,   left = 0,   pairleft = 0,   trunc.prob = 0.5,   same = 0 )"},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakes","text":"K Number clusters n Number observations cluster eta variance beta Effect (log hazard ratio) covariate stoptime Stopping time lam constant hazard left Left truncation pairleft pairwise (1) left truncation individual (0) trunc.prob Truncation probability 1 left-truncation also univariate truncation","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakes.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakes","text":"Thomas Scheike Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakesWei.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate from the Clayton-Oakes frailty model — simClaytonOakesWei","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakesWei","text":"Simulate observations Clayton-Oakes copula model Weibull type baseline Cox marginals.","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakesWei.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakesWei","text":"","code":"simClaytonOakesWei(   K,   n,   eta,   beta,   stoptime,   weiscale = 1,   weishape = 2,   left = 0,   pairleft = 0 )"},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakesWei.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakesWei","text":"K Number clusters n Number observations cluster eta 1/variance beta Effect (log hazard ratio) covariate stoptime Stopping time weiscale weibull scale parameter weishape weibull shape parameter left Left truncation pairleft pairwise (1) left truncation individual (0)","code":""},{"path":"http://kkholst.github.io/mets/reference/simClaytonOakesWei.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate from the Clayton-Oakes frailty model — simClaytonOakesWei","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/simGLcox.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","text":"Simulation two-stage recurrent events data based Cox/Cox Cox/Ghosh-Lin structure. type=3 generate Cox/Cox twostage mode, type=2 generate Ghosh-Lin/Cox model. variance var.z=0, generates data without dependence frailty. model=\"twostage\" default generate data Ghosh-Lin/Cox model, type=3 generate data marginal Cox models (Cox/Cox). Simulation based linear aproximation hazard two-stage models based grid time-scale. Must sufficientyly fine.","code":""},{"path":"http://kkholst.github.io/mets/reference/simGLcox.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","text":"","code":"simGLcox(   n,   base1,   drcumhaz,   var.z = 0,   r1 = NULL,   rd = NULL,   rc = NULL,   fz = NULL,   fdz = NULL,   model = c(\"twostage\", \"frailty\", \"shared\"),   type = NULL,   share = 1,   cens = NULL,   nmin = 100,   nmax = 1000 )"},{"path":"http://kkholst.github.io/mets/reference/simGLcox.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","text":"n number id's base1 baseline cox/ghosh-lin models drcumhaz baseline terminal event var.z variance gamma frailty r1 relative risk term baseline rd relative risk term terminal event rc relative risk term censorings fz possible transformation (function) frailty term fdz possible transformation (function) frailty term death model twostage, frailty, shared (partly shared two-stage model) type type simulation, default decided based model share fit patly shared random effects model cens censoring rate exponential censoring nmin default 100, least nmin number rows two-baselines max(nmin,nrow(base1),nrow(drcumhaz)) points time-grid 0 maximum time base1 nmax default 1000, nmax points time-grid","code":""},{"path":"http://kkholst.github.io/mets/reference/simGLcox.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","text":"Must specify baselines recurrent events terminal event possible covariate effects.","code":""},{"path":"http://kkholst.github.io/mets/reference/simGLcox.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulation of two-stage recurrent events data based on Cox/Cox or Cox/Ghosh-Lin structure — simGLcox","text":"Scheike (2025), Two-stage recurrent events random effects models, LIDA, appear","code":""},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of illness-death model — simMultistate","title":"Simulation of illness-death model — simMultistate","text":"Simulation illness-death model","code":""},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of illness-death model — simMultistate","text":"","code":"simMultistate(   n,   cumhaz,   cumhaz2,   death.cumhaz,   death.cumhaz2,   rr = NULL,   rr2 = NULL,   rd = NULL,   rd2 = NULL,   rrc = NULL,   gap.time = FALSE,   max.recurrent = 100,   dependence = 0,   var.z = 0.22,   cor.mat = NULL,   cens = NULL,   extend = TRUE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of illness-death model — simMultistate","text":"n number id's cumhaz cumulative hazard going state 1 2. cumhaz2 cumulative hazard going state 2 1. death.cumhaz cumulative hazard death state 1. death.cumhaz2 cumulative hazard death state 2. rr relative risk adjustment cumhaz rr2 relative risk adjustment cumhaz2 rd relative risk adjustment death.cumhaz rd2 relative risk adjustment death.cumhaz2 rrc relative risk adjustment censoring gap.time true simulates gap-times specified cumulative hazard max.recurrent limits number recurrent events 100 dependence 0:independence; 1:share random effect variance var.z; 2:random effect exp(normal) correlation structure cor.mat; 3:additive gamma distributed random effects, z1= (z11+ z12)/2 mean 1 , z2= (z11^cor.mat(1,2)+ z13)/2, z3= (z12^(cor.mat(2,3)+z13^cor.mat(1,3))/2, z11 z12 z13 gamma mean variance 1 , first random effect z1 N1 second random effect z2 N2 third random effect death var.z variance random effects cor.mat correlation matrix var.z variance random effects cens rate censoring exponential distribution extend extend hazards max-time ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of illness-death model — simMultistate","text":"simMultistate different death intensities states 1 2 Must give cumulative hazards time-range","code":""},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of illness-death model — simMultistate","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/simMultistate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of illness-death model — simMultistate","text":"","code":"######################################## ## getting some rates to mimick  ######################################## library(mets) data(CPH_HPN_CRBSI) dr <- CPH_HPN_CRBSI$terminal base1 <- CPH_HPN_CRBSI$crbsi  base4 <- CPH_HPN_CRBSI$mechanical dr2 <- scalecumhaz(dr,1.5) cens <- rbind(c(0,0),c(2000,0.5),c(5110,3))  iddata <- simMultistate(100,base1,base1,dr,dr2,cens=cens) dlist(iddata,.~id|id<3,n=0) #> id: 1 #>   entry     time status rr death from to start     stop #> 1     0 170.1568      3  1     1    1  3     0 170.1568 #> ------------------------------------------------------------  #> id: 2 #>         entry      time status rr death from to     start      stop #> 2      0.0000  124.5735      2  1     0    1  2    0.0000  124.5735 #> 101  124.5735 1055.3495      1  1     0    2  1  124.5735 1055.3495 #> 171 1055.3495 1143.6886      3  1     1    1  3 1055.3495 1143.6886   ### estimating rates from simulated data   c0 <- phreg(Surv(start,stop,status==0)~+1,iddata) c3 <- phreg(Surv(start,stop,status==3)~+strata(from),iddata) c1 <- phreg(Surv(start,stop,status==1)~+1,subset(iddata,from==2)) c2 <- phreg(Surv(start,stop,status==2)~+1,subset(iddata,from==1)) ### par(mfrow=c(2,3)) plot(c0) lines(cens,col=2)  plot(c3,main=\"rates 1-> 3 , 2->3\") lines(dr,col=1,lwd=2) lines(dr2,col=2,lwd=2) ### plot(c1,main=\"rate 1->2\") lines(base1,lwd=2) ### plot(c2,main=\"rate 2->1\") lines(base1,lwd=2)"},{"path":"http://kkholst.github.io/mets/reference/simRecurrent.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","title":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","text":"Simulation recurrent events data based cumulative hazards event death process","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","text":"","code":"simRecurrent(   n,   cumhaz,   death.cumhaz = NULL,   r1 = NULL,   rd = NULL,   rc = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/simRecurrent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","text":"n number id's cumhaz cumulative hazard recurrent events death.cumhaz cumulative hazard death r1 potential relative risk adjustment rate rd potential relative risk adjustment rate rc potential relative risk adjustment rate ... Additional arguments simRecurrentList","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of recurrent events data based on cumulative hazards for event and death process — simRecurrent","text":"","code":"######################################## ## getting some rates to mimick  ######################################## library(mets) data(CPH_HPN_CRBSI) dr <- CPH_HPN_CRBSI$terminal base1 <- CPH_HPN_CRBSI$crbsi  base4 <- CPH_HPN_CRBSI$mechanical  ###################################################################### ### simulating simple model that mimicks data  ###################################################################### rr <- simRecurrent(5,base1) dlist(rr,.~id,n=0) #> id: 1 #>        entry      time status dtime fdeath death     start      stop #> 1     0.0000  479.9688      1  5110      0     0    0.0000  479.9688 #> 6   479.9688  772.2568      1  5110      0     0  479.9688  772.2568 #> 11  772.2568  957.6359      1  5110      0     0  772.2568  957.6359 #> 16  957.6359 1170.7139      1  5110      0     0  957.6359 1170.7139 #> 21 1170.7139 1484.9421      1  5110      0     0 1170.7139 1484.9421 #> 25 1484.9421 1898.2506      1  5110      0     0 1484.9421 1898.2506 #> 29 1898.2506 2573.3859      1  5110      0     0 1898.2506 2573.3859 #> 32 2573.3859 2866.1914      1  5110      0     0 2573.3859 2866.1914 #> 35 2866.1914 3131.7214      1  5110      0     0 2866.1914 3131.7214 #> 37 3131.7214 3328.1125      1  5110      0     0 3131.7214 3328.1125 #> 38 3328.1125 5110.0000      0  5110      0     0 3328.1125 5110.0000 #> ------------------------------------------------------------  #> id: 2 #>       entry     time status dtime fdeath death    start     stop #> 2     0.000 1817.737      1  5110      0     0    0.000 1817.737 #> 7  1817.737 3877.933      1  5110      0     0 1817.737 3877.933 #> 12 3877.933 5109.602      1  5110      0     0 3877.933 5109.602 #> 17 5109.602 5110.000      0  5110      0     0 5109.602 5110.000 #> ------------------------------------------------------------  #> id: 3 #>       entry     time status dtime fdeath death    start     stop #> 3     0.000 1394.052      1  5110      0     0    0.000 1394.052 #> 8  1394.052 1463.023      1  5110      0     0 1394.052 1463.023 #> 13 1463.023 2037.959      1  5110      0     0 1463.023 2037.959 #> 18 2037.959 2125.253      1  5110      0     0 2037.959 2125.253 #> 22 2125.253 2312.194      1  5110      0     0 2125.253 2312.194 #> 26 2312.194 3232.312      1  5110      0     0 2312.194 3232.312 #> 30 3232.312 3511.355      1  5110      0     0 3232.312 3511.355 #> 33 3511.355 5110.000      0  5110      0     0 3511.355 5110.000 #> ------------------------------------------------------------  #> id: 4 #>       entry     time status dtime fdeath death    start     stop #> 4     0.000 1531.245      1  5110      0     0    0.000 1531.245 #> 9  1531.245 1560.042      1  5110      0     0 1531.245 1560.042 #> 14 1560.042 2371.617      1  5110      0     0 1560.042 2371.617 #> 19 2371.617 3111.797      1  5110      0     0 2371.617 3111.797 #> 23 3111.797 3252.093      1  5110      0     0 3111.797 3252.093 #> 27 3252.093 3486.817      1  5110      0     0 3252.093 3486.817 #> 31 3486.817 3980.201      1  5110      0     0 3486.817 3980.201 #> 34 3980.201 4029.447      1  5110      0     0 3980.201 4029.447 #> 36 4029.447 5110.000      0  5110      0     0 4029.447 5110.000 #> ------------------------------------------------------------  #> id: 5 #>       entry     time status dtime fdeath death    start     stop #> 5     0.000 2037.976      1  5110      0     0    0.000 2037.976 #> 10 2037.976 2776.202      1  5110      0     0 2037.976 2776.202 #> 15 2776.202 2838.842      1  5110      0     0 2776.202 2838.842 #> 20 2838.842 3986.594      1  5110      0     0 2838.842 3986.594 #> 24 3986.594 3990.847      1  5110      0     0 3986.594 3990.847 #> 28 3990.847 5110.000      0  5110      0     0 3990.847 5110.000 rr <- simRecurrent(5,base1,death.cumhaz=dr) dlist(rr,.~id,n=0) #> id: 1 #>   entry     time status    dtime fdeath death start     stop #> 1     0 63.83977      0 63.83977      1     1     0 63.83977 #> ------------------------------------------------------------  #> id: 2 #>   entry     time status    dtime fdeath death start     stop #> 2     0 174.8384      0 174.8384      1     1     0 174.8384 #> ------------------------------------------------------------  #> id: 3 #>   entry   time status  dtime fdeath death start   stop #> 3     0 265.97      0 265.97      1     1     0 265.97 #> ------------------------------------------------------------  #> id: 4 #>          entry         time status    dtime fdeath death       start #> 4    0.0000000    0.3237961      1 1713.332      1     0   0.0000000 #> 6    0.3237961  855.0953903      1 1713.332      1     0   0.3237961 #> 8  855.0953903  956.5468201      1 1713.332      1     0 855.0953903 #> 10 956.5468201 1713.3321737      0 1713.332      1     1 956.5468201 #>            stop #> 4     0.3237961 #> 6   855.0953903 #> 8   956.5468201 #> 10 1713.3321737 #> ------------------------------------------------------------  #> id: 5 #>       entry     time status    dtime fdeath death    start     stop #> 5    0.0000 217.9460      1 752.5034      1     0   0.0000 217.9460 #> 7  217.9460 301.7446      1 752.5034      1     0 217.9460 301.7446 #> 9  301.7446 546.4851      1 752.5034      1     0 301.7446 546.4851 #> 11 546.4851 752.5034      0 752.5034      1     1 546.4851 752.5034  rr <- simRecurrent(100,base1,death.cumhaz=dr) par(mfrow=c(1,3)) showfitsim(causes=1,rr,dr,base1,base1) ###################################################################### ### simulating simple model  ### random effect for all causes (Z shared for death and recurrent)  ###################################################################### rr <- simRecurrent(100,base1,death.cumhaz=dr,dependence=1,var.z=0.4) dtable(rr,~death+status) #>  #>       status   0   1 #> death                #> 0             22 237 #> 1             78   0  ###################################################################### ### now with two event types and second type has same rate as death rate ###################################################################### set.seed(100) rr <- simRecurrentII(100,base1,base4,death.cumhaz=dr) dtable(rr,~death+status) #>  #>       status   0   1   2 #> death                    #> 0             10 295  39 #> 1             90   0   0 par(mfrow=c(2,2))  showfitsim(causes=2,rr,dr,base1,base4)  ###################################################################### ### now with three event types and two causes of death  ###################################################################### set.seed(100) cumhaz <- list(base1,base1,base4) drl <- list(dr,base4) rr <- simRecurrentList(100,cumhaz,death.cumhaz=drl,dependence=0) dtable(rr,~death+status) #>  #>       status   0   1   2   3 #> death                        #> 0              4 232 268  33 #> 1             70   0   0   0 #> 2             26   0   0   0 showfitsimList(rr,cumhaz,drl)"},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"Simulation recurrent events data based cumulative hazards","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"","code":"simRecurrentII(   n,   cumhaz,   cumhaz2,   death.cumhaz = NULL,   r1 = NULL,   r2 = NULL,   rd = NULL,   rc = NULL,   dependence = 0,   var.z = 1,   cor.mat = NULL,   cens = NULL,   gap.time = FALSE,   max.recurrent = 100,   ... )"},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"n number id's cumhaz cumulative hazard recurrent events cumhaz2 cumulative hazard recurrent events  type 2 death.cumhaz cumulative hazard death r1 potential relative risk adjustment rate r2 potential relative risk adjustment rate rd potential relative risk adjustment rate rc potential relative risk adjustment rate dependence 0:independence; 1:share random effect variance var.z; 2:random effect exp(normal) correlation structure cor.mat; 3:additive gamma distributed random effects, z1= (z11+ z12)/2 mean 1 , z2= (z11^cor.mat(1,2)+ z13)/2, z3= (z12^(cor.mat(2,3)+z13^cor.mat(1,3))/2, z11 z12 z13 gamma mean variance 1 , first random effect z1 N1 second random effect z2 N2 third random effect death var.z variance random effects cor.mat correlation matrix var.z variance random effects cens rate censoring exponential distribution gap.time true simulates gap-times specified cumulative hazard max.recurrent limits number recurrent events 100 ... Additional arguments simRecurrentList","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"Must give cumulative hazard death possibly two recurrent events. dependence can specified via random effects two recurrent events need share random effect, can also specified via zzr. terminal event may share random effect (dependence=1) (dependence=4) can specified via zzr.","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentII.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of recurrent events data based on cumulative hazards with two types of recurrent events — simRecurrentII","text":"","code":"######################################## ## getting some rates to mimick  ######################################## library(mets) data(CPH_HPN_CRBSI) dr <- CPH_HPN_CRBSI$terminal base1 <- CPH_HPN_CRBSI$crbsi  base4 <- CPH_HPN_CRBSI$mechanical  ###################################################################### ### simulating simple model that mimicks data  ###################################################################### rr <- simRecurrent(5,base1) dlist(rr,.~id,n=0) #> id: 1 #>         entry       time status dtime fdeath death      start       stop #> 1     0.00000   46.48577      1  5110      0     0    0.00000   46.48577 #> 6    46.48577  354.25536      1  5110      0     0   46.48577  354.25536 #> 11  354.25536  784.54874      1  5110      0     0  354.25536  784.54874 #> 16  784.54874 1532.83800      1  5110      0     0  784.54874 1532.83800 #> 21 1532.83800 2165.23317      1  5110      0     0 1532.83800 2165.23317 #> 26 2165.23317 3076.99011      1  5110      0     0 2165.23317 3076.99011 #> 31 3076.99011 3079.98994      1  5110      0     0 3076.99011 3079.98994 #> 35 3079.98994 3620.79709      1  5110      0     0 3079.98994 3620.79709 #> 37 3620.79709 3643.20854      1  5110      0     0 3620.79709 3643.20854 #> 39 3643.20854 3704.27821      1  5110      0     0 3643.20854 3704.27821 #> 41 3704.27821 3877.28789      1  5110      0     0 3704.27821 3877.28789 #> 43 3877.28789 5110.00000      0  5110      0     0 3877.28789 5110.00000 #> ------------------------------------------------------------  #> id: 2 #>        entry      time status dtime fdeath death     start      stop #> 2     0.0000  836.2402      1  5110      0     0    0.0000  836.2402 #> 7   836.2402 1582.7756      1  5110      0     0  836.2402 1582.7756 #> 12 1582.7756 1909.3792      1  5110      0     0 1582.7756 1909.3792 #> 17 1909.3792 2788.0148      1  5110      0     0 1909.3792 2788.0148 #> 22 2788.0148 4131.8937      1  5110      0     0 2788.0148 4131.8937 #> 27 4131.8937 4542.2007      1  5110      0     0 4131.8937 4542.2007 #> 32 4542.2007 5110.0000      0  5110      0     0 4542.2007 5110.0000 #> ------------------------------------------------------------  #> id: 3 #>        entry      time status dtime fdeath death     start      stop #> 3     0.0000  477.0110      1  5110      0     0    0.0000  477.0110 #> 8   477.0110  684.0066      1  5110      0     0  477.0110  684.0066 #> 13  684.0066  752.7554      1  5110      0     0  684.0066  752.7554 #> 18  752.7554  786.7790      1  5110      0     0  752.7554  786.7790 #> 23  786.7790 1523.6981      1  5110      0     0  786.7790 1523.6981 #> 28 1523.6981 1697.5033      1  5110      0     0 1523.6981 1697.5033 #> 33 1697.5033 2121.4121      1  5110      0     0 1697.5033 2121.4121 #> 36 2121.4121 2211.0749      1  5110      0     0 2121.4121 2211.0749 #> 38 2211.0749 2248.7271      1  5110      0     0 2211.0749 2248.7271 #> 40 2248.7271 3559.0995      1  5110      0     0 2248.7271 3559.0995 #> 42 3559.0995 3823.9527      1  5110      0     0 3559.0995 3823.9527 #> 44 3823.9527 4697.2929      1  5110      0     0 3823.9527 4697.2929 #> 45 4697.2929 4697.6391      1  5110      0     0 4697.2929 4697.6391 #> 46 4697.6391 5031.7334      1  5110      0     0 4697.6391 5031.7334 #> 47 5031.7334 5110.0000      0  5110      0     0 5031.7334 5110.0000 #> ------------------------------------------------------------  #> id: 4 #>       entry     time status dtime fdeath death    start     stop #> 4     0.000 1570.693      1  5110      0     0    0.000 1570.693 #> 9  1570.693 3057.897      1  5110      0     0 1570.693 3057.897 #> 14 3057.897 3389.234      1  5110      0     0 3057.897 3389.234 #> 19 3389.234 3913.541      1  5110      0     0 3389.234 3913.541 #> 24 3913.541 3976.522      1  5110      0     0 3913.541 3976.522 #> 29 3976.522 5110.000      0  5110      0     0 3976.522 5110.000 #> ------------------------------------------------------------  #> id: 5 #>       entry     time status dtime fdeath death    start     stop #> 5     0.000 1253.563      1  5110      0     0    0.000 1253.563 #> 10 1253.563 1947.437      1  5110      0     0 1253.563 1947.437 #> 15 1947.437 2075.897      1  5110      0     0 1947.437 2075.897 #> 20 2075.897 2211.924      1  5110      0     0 2075.897 2211.924 #> 25 2211.924 3991.204      1  5110      0     0 2211.924 3991.204 #> 30 3991.204 5084.684      1  5110      0     0 3991.204 5084.684 #> 34 5084.684 5110.000      0  5110      0     0 5084.684 5110.000 rr <- simRecurrent(5,base1,death.cumhaz=dr) dlist(rr,.~id,n=0) #> id: 1 #>        entry      time status    dtime fdeath death     start      stop #> 1     0.0000  350.6289      1 2996.717      1     0    0.0000  350.6289 #> 6   350.6289  362.1943      1 2996.717      1     0  350.6289  362.1943 #> 10  362.1943  884.5432      1 2996.717      1     0  362.1943  884.5432 #> 12  884.5432 1546.1629      1 2996.717      1     0  884.5432 1546.1629 #> 14 1546.1629 1701.7343      1 2996.717      1     0 1546.1629 1701.7343 #> 16 1701.7343 2996.7166      0 2996.717      1     1 1701.7343 2996.7166 #> ------------------------------------------------------------  #> id: 2 #>   entry     time status    dtime fdeath death start     stop #> 2     0 209.9771      0 209.9771      1     1     0 209.9771 #> ------------------------------------------------------------  #> id: 3 #>      entry      time status    dtime fdeath death    start      stop #> 3   0.0000  509.7645      1 4193.438      1     0   0.0000  509.7645 #> 7 509.7645 4193.4377      0 4193.438      1     1 509.7645 4193.4377 #> ------------------------------------------------------------  #> id: 4 #>        entry      time status    dtime fdeath death     start      stop #> 4     0.0000  298.0493      1 2335.534      1     0    0.0000  298.0493 #> 8   298.0493  482.9285      1 2335.534      1     0  298.0493  482.9285 #> 11  482.9285  490.5588      1 2335.534      1     0  482.9285  490.5588 #> 13  490.5588  630.2717      1 2335.534      1     0  490.5588  630.2717 #> 15  630.2717  750.7360      1 2335.534      1     0  630.2717  750.7360 #> 17  750.7360  768.4209      1 2335.534      1     0  750.7360  768.4209 #> 18  768.4209 1510.5186      1 2335.534      1     0  768.4209 1510.5186 #> 19 1510.5186 1936.9710      1 2335.534      1     0 1510.5186 1936.9710 #> 20 1936.9710 2263.3131      1 2335.534      1     0 1936.9710 2263.3131 #> 21 2263.3131 2335.5340      0 2335.534      1     1 2263.3131 2335.5340 #> ------------------------------------------------------------  #> id: 5 #>      entry      time status    dtime fdeath death    start      stop #> 5   0.0000  358.8214      1 4797.555      1     0   0.0000  358.8214 #> 9 358.8214 4797.5555      0 4797.555      1     1 358.8214 4797.5555  rr <- simRecurrent(100,base1,death.cumhaz=dr) par(mfrow=c(1,3)) showfitsim(causes=1,rr,dr,base1,base1) ###################################################################### ### simulating simple model  ### random effect for all causes (Z shared for death and recurrent)  ###################################################################### rr <- simRecurrent(100,base1,death.cumhaz=dr,dependence=1,var.z=0.4) dtable(rr,~death+status) #>  #>       status   0   1 #> death                #> 0             26 221 #> 1             74   0  ###################################################################### ### now with two event types and second type has same rate as death rate ###################################################################### set.seed(100) rr <- simRecurrentII(100,base1,base4,death.cumhaz=dr) dtable(rr,~death+status) #>  #>       status   0   1   2 #> death                    #> 0             10 295  39 #> 1             90   0   0 par(mfrow=c(2,2))  showfitsim(causes=2,rr,dr,base1,base4)  ###################################################################### ### now with three event types and two causes of death  ###################################################################### set.seed(100) cumhaz <- list(base1,base1,base4) drl <- list(dr,base4) rr <- simRecurrentList(100,cumhaz,death.cumhaz=drl,dependence=0) dtable(rr,~death+status) #>  #>       status   0   1   2   3 #> death                        #> 0              4 232 268  33 #> 1             70   0   0   0 #> 2             26   0   0   0 showfitsimList(rr,cumhaz,drl)"},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"Simulation recurrent events data based cumulative hazards","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"","code":"simRecurrentTS(   n,   cumhaz,   cumhaz2,   death.cumhaz = NULL,   nu = rep(1, 3),   share1 = 0.3,   vargamD = 2,   vargam12 = 0.5,   gap.time = FALSE,   max.recurrent = 100,   cens = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"n number id's cumhaz cumulative hazard recurrent events cumhaz2 cumulative hazard recurrent events  type 2 death.cumhaz cumulative hazard death nu powers random effects nu > -1/shape share1 random effect death splits two parts vargamD variance random effect  death vargam12 shared random effect N1 N2 gap.time true simulates gap-times specified cumulative hazard max.recurrent limits number recurrent events 100 cens rate censoring exponential distribution ... Additional arguments lower level funtions","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"Model constructed marginals specified form linear approximations cumulative hazards specific form make equivalent marginals integrating survivors. Therefore E(dN_1 | D>t) = cumhaz, E(dN_2 | D>t) = cumhaz2,  hazard death death.cumhazard Must give hazard death two recurrent events.  Hazard death death.cumhazard  two event types dependence can specified two recurrent events need share random effect. Random effect  death Z.death=(Zd1+Zd2), Z1=(Zd1^nu1) Z12,  Z2=(Zd2^nu2) Z12^nu3 $$Z.death=Zd1+Zd2$$  gamma distributions $$Zdj$$  gamma distribution  mean parameters (sharej), vargamD,  share2=1-share1 $$Z12$$  gamma distribution mean 1 variance vargam12","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/simRecurrentTS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation of recurrent events data based on cumulative hazards: Two-stage model — simRecurrentTS","text":"","code":"######################################## ## getting some rates to mimick  ######################################## data(CPH_HPN_CRBSI) dr <- CPH_HPN_CRBSI$terminal base1 <- CPH_HPN_CRBSI$crbsi  base4 <- CPH_HPN_CRBSI$mechanical  rr <- simRecurrentTS(1000,base1,base4,death.cumhaz=dr) dtable(rr,~death+status) #>  #>       status    0    1    2 #> death                       #> 0             138 2394  384 #> 1             861    0    0 showfitsim(causes=2,rr,dr,base1,base4)"},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary for dependence models for competing risks — summary.cor","title":"Summary for dependence models for competing risks — summary.cor","text":"Computes concordance casewise concordance dependence models competing risks models type cor.cif, rr.cif .cif given cumulative incidences different dependence measures object.","code":""},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary for dependence models for competing risks — summary.cor","text":"","code":"# S3 method for class 'cor' summary(object, marg.cif = NULL, marg.cif2 = NULL, digits = 3, ...)"},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary for dependence models for competing risks — summary.cor","text":"object object cor.cif rr.cif .cif dependence competing risks data two causes. marg.cif number gives cumulative incidence one time point concordance casewise concordance computed. marg.cif2 cumulative incidence cause 2 concordance casewise concordance computed. Default marg.cif. digits digits output. ... Additional arguments.","code":""},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary for dependence models for competing risks — summary.cor","text":"prints summary dependence model. casewise gives casewise concordance , probability cause 2 (related cif2) given cause 1 (related cif1) \toccured. concordance gives concordance , probability cause 2 (related cif2) cause 1 (related cif1). cif1 cumulative incidence cause1. cif2 cumulative incidence cause1.","code":""},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Summary for dependence models for competing risks — summary.cor","text":"Cross odds ratio Modelling dependence Multivariate Competing Risks Data, Scheike Sun (2012), Biostatistics. Semiparametric Random Effects Model Multivariate Competing Risks Data, Scheike, Zhang, Sun, Jensen (2010), Biometrika.","code":""},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summary for dependence models for competing risks — summary.cor","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/summary.cor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary for dependence models for competing risks — summary.cor","text":"","code":"## library(\"timereg\") ## data(\"multcif\",package=\"mets\") # simulated data  ## multcif$cause[multcif$cause==0] <- 2 ##   ## times=seq(0.1,3,by=0.1) # to speed up computations use only these time-points ## add <- timereg::comp.risk(Event(time,cause)~+1+cluster(id), ##                           data=multcif,n.sim=0,times=times,cause=1) ### ## out1<-cor.cif(add,data=multcif,cause1=1,cause2=1,theta=log(2+1)) ## summary(out1) ##  ## pad <- predict(add,X=1,se=0,uniform=0) ## summary(out1,marg.cif=pad)"},{"path":"http://kkholst.github.io/mets/reference/summaryGLM.html","id":null,"dir":"Reference","previous_headings":"","what":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","title":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","text":"Reporting glm binomial link  glm predictions","code":""},{"path":"http://kkholst.github.io/mets/reference/summaryGLM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","text":"","code":"summaryGLM(object, id = NULL, fun = NULL, ...)"},{"path":"http://kkholst.github.io/mets/reference/summaryGLM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","text":"object glm output id possible id cluster corrected standard errors fun possible function non-standard predictions based object ... arguments estimate lava example level=0.95","code":""},{"path":"http://kkholst.github.io/mets/reference/summaryGLM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/summaryGLM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reporting OR (exp(coef)) from glm with binomial link and glm predictions — summaryGLM","text":"","code":"data(sTRACE) sTRACE$id <- sample(1:100,nrow(sTRACE),replace=TRUE)  model <- glm(I(status==9)~sex+factor(diabetes)+age,data=sTRACE,family=binomial) summaryGLM(model) #> $coef #>                   Estimate Std.Err     2.5%   97.5%   P-value #> (Intercept)       -6.65169 0.82284 -8.26442 -5.0389 6.278e-16 #> sex                0.25832 0.22418 -0.18107  0.6977 2.492e-01 #> factor(diabetes)1  0.63305 0.30486  0.03553  1.2306 3.785e-02 #> age                0.09591 0.01099  0.07436  0.1175 2.707e-18 #>  #> $or #>                      Estimate         2.5%      97.5% #> (Intercept)       0.001291843 0.0002575172 0.00648057 #> sex               1.294751030 0.8343769033 2.00914026 #> factor(diabetes)1 1.883341432 1.0361638586 3.42317957 #> age               1.100659282 1.0771942608 1.12463545 #>  #> $fout #> NULL #>  summaryGLM(model,id=sTRACE$id) #> $coef #>                   Estimate Std.Err     2.5%   97.5%   P-value #> (Intercept)       -6.65169 0.77938 -8.17925 -5.1241 1.407e-17 #> sex                0.25832 0.21740 -0.16779  0.6844 2.348e-01 #> factor(diabetes)1  0.63305 0.27780  0.08856  1.1775 2.268e-02 #> age                0.09591 0.01019  0.07593  0.1159 5.032e-21 #>  #> $or #>                      Estimate        2.5%      97.5% #> (Intercept)       0.001291843 0.000280412 0.00595145 #> sex               1.294751030 0.845534854 1.98262700 #> factor(diabetes)1 1.883341432 1.092604538 3.24634836 #> age               1.100659282 1.078886728 1.12287122 #>  #> $fout #> NULL #>   nd <- data.frame(sex=c(0,1),age=67,diabetes=1) predictGLM(model,nd) #> $coef #>                   Estimate Std.Err     2.5%   97.5%   P-value #> (Intercept)       -6.65169 0.82284 -8.26442 -5.0389 6.278e-16 #> sex                0.25832 0.22418 -0.18107  0.6977 2.492e-01 #> factor(diabetes)1  0.63305 0.30486  0.03553  1.2306 3.785e-02 #> age                0.09591 0.01099  0.07436  0.1175 2.707e-18 #>  #> $pred #>     Estimate      2.5%     97.5% #> p1 0.6004375 0.4494731 0.7344613 #> p2 0.6605188 0.5194651 0.7778730 #>"},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":null,"dir":"Reference","previous_headings":"","what":"Twostage survival model for multivariate survival data — survival.twostage","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"Fits Clayton-Oakes bivariate Plackett models bivariate survival data using marginals Cox form. dependence can modelled via Regression design dependence parameter. Random effects, additive gamma model. clusters contain two subjects, use composite likelihood based pairwise bivariate models, full MLE see twostageMLE. two-stage model constructed given gamma distributed random effects assumed survival functions indpendent, marginal survival functions Cox form (additive form) $$ P(T > t| x) = S(t|x)= exp( -exp(x^T \\beta) A_0(t) ) $$ One possibility model variance within clusters via regression design, one can specify regression structure independent gamma distributed random effect cluster, variance given $$  \\theta = h( z_j^T \\alpha) $$ \\(z\\) specified theta.des, possible link function var.link=1 use exponential link \\(h(x)=exp(x)\\), var.link=0 identity link \\(h(x)=x\\). reported standard errors based estimated information likelihood assuming marginals known (unlike twostageMLE additive gamma model ). Can also fit structured additive gamma random effects model, ACE, ADE model survival data.  case random.design specificies random effects subject within cluster. matrix 1's 0's dimension n x d.  d random effects. cluster two subjects, let random.design rows  \\(v_1\\) \\(v_2\\). random effects subject 1 $$v_1^T (Z_1,...,Z_d)$$, d random effects. random effect associated parameter \\((\\lambda_1,...,\\lambda_d)\\). construction subjects 1's random effect Gamma distributed mean \\(\\lambda_j/v_1^T \\lambda\\) variance \\(\\lambda_j/(v_1^T \\lambda)^2\\). Note random effect \\(v_1^T (Z_1,...,Z_d)\\) mean 1 variance \\(1/(v_1^T \\lambda)\\). asssumed  \\(lamtot=v_1^T \\lambda\\) fixed within clusters ACE model . Based parameters relative contribution (heritability, h) equivalent  expected values random effects: \\(\\lambda_j/v_1^T \\lambda\\) DEFAULT parametrization (var.par=1) uses variances random effecs $$ \\theta_j  = \\lambda_j/(v_1^T \\lambda)^2 $$ alternative parametrizations one can specify parameters relate \\(\\lambda_j\\) argument var.par=0. types models basic model assumptions given random effects clusters survival distributions within cluster independent ' form $$ P(T > t| x,z) = exp( -Z \\cdot Laplace^{-1}(lamtot,lamtot,S(t|x)) ) $$ inverse laplace gamma distribution mean 1 variance 1/lamtot. parameters \\((\\lambda_1,...,\\lambda_d)\\) related parameters model regression construction \\(pard\\) (d x k), links \\(d\\) \\(\\lambda\\) parameters (k) underlying \\(\\theta\\) parameters $$ \\lambda = theta.des  \\theta $$ using theta.des specify low-dimension association. Default diagonal matrix. can used make structural assumptions variances random-effects needed ACE model example. case.control option can used pair specification pairwise parts estimating equations. assumed second subject pair proband.","code":""},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"","code":"survival.twostage(   margsurv,   data = parent.frame(),   method = \"nr\",   detail = 0,   clusters = NULL,   silent = 1,   weights = NULL,   theta = NULL,   theta.des = NULL,   var.link = 1,   baseline.iid = 1,   model = \"clayton.oakes\",   marginal.trunc = NULL,   marginal.survival = NULL,   strata = NULL,   se.clusters = NULL,   numDeriv = 1,   random.design = NULL,   pairs = NULL,   dim.theta = NULL,   numDeriv.method = \"simple\",   additive.gamma.sum = NULL,   var.par = 1,   no.opt = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"margsurv Marginal model data data frame method Scoring method \"nr\", lava NR optimizer detail Detail clusters Cluster variable silent Debug information weights Weights theta Starting values variance components theta.des design dependence parameters, pairs given indeces theta-design pair, given pairs column 5 var.link Link function variance:  exp-link. baseline.iid adjust baseline estimation, using phreg function data. model model marginal.trunc marginal left truncation probabilities marginal.survival optional vector marginal survival probabilities strata strata fitting, see example se.clusters clusters se calculation iid numDeriv get numDeriv version second derivative, otherwise uses sum squared scores pair random.design random effect design additive gamma model, pairs given indeces pairs random.design rows given columns 3:4 pairs matrix rows indeces (two-columns) pairs considered pairwise composite score, useful case-control sampling marginal known. dim.theta dimension theta parameter pairs situation. numDeriv.method uses simple speed things second derivative important. additive.gamma.sum two.stage=0, specification lamtot models via matrix multiplied onto parameters theta (dimensions=(number random effects x number theta parameters), null sums parameters. var.par 1 default parametrization variances random effects, var.par=0 specifies \\(\\lambda_j\\)'s used parameters. .opt optimizng ... Additional arguments maximizer NR lava. ascertained sampling","code":""},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"Twostage estimation additive gamma frailty models survival data. Scheike (2019), work progress Shih Louis (1995) Inference association parameter copula models bivariate survival data, Biometrics, (1995). Glidden (2000), Two-Stage estimator dependence parameter Clayton Oakes model, LIDA, (2000). Measuring early late dependence bivariate twin data Scheike, Holst, Hjelmborg (2015), LIDA Estimating heritability cause specific mortality based twins studies Scheike, Holst, Hjelmborg (2014), LIDA Additive Gamma frailty models competing risks data, Biometrics (2015) Eriksson Scheike (2015),","code":""},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/survival.twostage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Twostage survival model for multivariate survival data — survival.twostage","text":"","code":"library(mets) data(diabetes)  # Marginal Cox model  with treat as covariate margph <- phreg(Surv(time,status)~treat+cluster(id),data=diabetes) ### Clayton-Oakes, MLE fitco1<-twostageMLE(margph,data=diabetes,theta=1.0) summary(fitco1) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                 Coef.        SE       z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543033 2.68883 0.007170289    0.322645 0.08127892 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### Plackett model mph <- phreg(Surv(time,status)~treat+cluster(id),data=diabetes) fitp <- survival.twostage(mph,data=diabetes,theta=3.0,Nit=40,                clusters=diabetes$id,var.link=1,model=\"plackett\") summary(fitp) #> Dependence parameter for Odds-Ratio (Plackett) model  #> With log-link  #> $estimates #>             log-Coef.        SE        z        P-val Spearman Corr.         SE #> dependence1   1.14188 0.3026537 3.772891 0.0001613666      0.3648216 0.08869229 #>  #> $or #>             Estimate Std.Err  2.5% 97.5%   P-value #> dependence1    3.133  0.9481 1.274 4.991 0.0009528 #>  #> $type #> [1] \"plackett\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### Clayton-Oakes fitco2 <- survival.twostage(mph,data=diabetes,theta=0.0,detail=0,                  clusters=diabetes$id,var.link=1,model=\"clayton.oakes\") summary(fitco2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>              log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.0484957 0.3718487 -0.1304178 0.8962359    0.322645 0.08126576 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9527  0.3542 0.2584 1.647 0.007161 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" fitco3 <- survival.twostage(margph,data=diabetes,theta=1.0,detail=0,                  clusters=diabetes$id,var.link=0,model=\"clayton.oakes\") summary(fitco3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                 Coef.     SE        z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543 2.688855 0.007169754    0.322645 0.08127816 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### without covariates but with stratafied marg <- phreg(Surv(time,status)~+strata(treat)+cluster(id),data=diabetes) fitpa <- survival.twostage(marg,data=diabetes,theta=1.0,                 clusters=diabetes$id,model=\"clayton.oakes\") summary(fitpa) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>               log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.05684062 0.3721207 -0.1527478 0.8785971    0.320824 0.08108359 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9447  0.3516 0.2557 1.634 0.007203 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  fitcoa <- survival.twostage(marg,data=diabetes,theta=1.0,clusters=diabetes$id,                  model=\"clayton.oakes\") summary(fitcoa) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>               log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.05684062 0.3721207 -0.1527478 0.8785971    0.320824 0.08108359 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.9447  0.3516 0.2557 1.634 0.007203 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### Piecewise constant cross hazards ratio modelling ########################################################  d <- subset(simClaytonOakes(2000,2,0.5,0,stoptime=2,left=0),!truncated) udp <- piecewise.twostage(c(0,0.5,2),data=d,method=\"optimize\",                           id=\"cluster\",timevar=\"time\",                           status=\"status\",model=\"clayton.oakes\",silent=0) #> Data-set  1 out of  4  #>   Number of joint events: 530 of  2000 #> Data-set  2 out of  4  #>   Number of joint events: 265 of  1218 #> Data-set  3 out of  4  #>   Number of joint events: 246 of  1197 #> Data-set  4 out of  4  #>   Number of joint events: 613 of  945 summary(udp) #> [1] 1 #> Dependence parameter for Clayton-Oakes model  #> log-coefficient for dependence parameter (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  0.655 (0.067)  0.621 (0.101) #> 0.5 - 2  0.629 (0.097)  0.747 (0.058) #>  #> Kendall's tau (SE)  #>          0 - 0.5        0.5 - 2       #> 0 - 0.5  0.49  (0.017)  0.482 (0.025) #> 0.5 - 2  0.484 (0.024)  0.513 (0.015) #>    ## Reduce Ex.Timings ### Same model using the strata option, a bit slower ######################################################## ## makes the survival pieces for different areas in the plane ##ud1=surv.boxarea(c(0,0),c(0.5,0.5),data=d,id=\"cluster\",timevar=\"time\",status=\"status\") ##ud2=surv.boxarea(c(0,0.5),c(0.5,2),data=d,id=\"cluster\",timevar=\"time\",status=\"status\") ##ud3=surv.boxarea(c(0.5,0),c(2,0.5),data=d,id=\"cluster\",timevar=\"time\",status=\"status\") ##ud4=surv.boxarea(c(0.5,0.5),c(2,2),data=d,id=\"cluster\",timevar=\"time\",status=\"status\")  ## everything done in one call ud <- piecewise.data(c(0,0.5,2),data=d,timevar=\"time\",status=\"status\",id=\"cluster\") ud$strata <- factor(ud$strata); ud$intstrata <- factor(ud$intstrata)  ## makes strata specific id variable to identify pairs within strata ## se's computed based on the id variable across strata \"cluster\" ud$idstrata <- ud$id+(as.numeric(ud$strata)-1)*2000  marg2 <- timereg::aalen(Surv(boxtime,status)~-1+factor(num):factor(intstrata),                data=ud,n.sim=0,robust=0) tdes <- model.matrix(~-1+factor(strata),data=ud) fitp2 <- survival.twostage(marg2,data=ud,se.clusters=ud$cluster,clusters=ud$idstrata,                 model=\"clayton.oakes\",theta.des=tdes,step=0.5) summary(fitp2) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>                           log-Coef.         SE        z        P-val #> factor(strata)0-0.5,0-0.5 0.8276088 0.06812882 12.14770 0.0000000000 #> factor(strata)0-0.5,0.5-2 0.4049929 0.11343320  3.57032 0.0003565449 #> factor(strata)0.5-2,0-0.5 0.4176728 0.11408832  3.66096 0.0002512718 #> factor(strata)0.5-2,0.5-2 0.6903006 0.05740192 12.02574 0.0000000000 #>                           Kendall tau         SE #> factor(strata)0-0.5,0-0.5   0.5335648 0.01695545 #> factor(strata)0-0.5,0.5-2   0.4284558 0.02777768 #> factor(strata)0.5-2,0-0.5   0.4315636 0.02798774 #> factor(strata)0.5-2,0.5-2   0.4992884 0.01435045 #>  #> $vargam #>             Estimate Std.Err  2.5% 97.5%   P-value #> dependence1    2.288  0.1559 1.982 2.593 8.908e-49 #> dependence2    1.499  0.1701 1.166 1.833 1.189e-18 #> dependence3    1.518  0.1732 1.179 1.858 1.865e-18 #> dependence4    1.994  0.1145 1.770 2.219 5.715e-68 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### now fitting the model with symmetry, i.e. strata 2 and 3 same effect ud$stratas <- ud$strata; ud$stratas[ud$strata==\"0.5-2,0-0.5\"] <- \"0-0.5,0.5-2\" tdes2 <- model.matrix(~-1+factor(stratas),data=ud) fitp3 <- survival.twostage(marg2,data=ud,clusters=ud$idstrata,se.cluster=ud$cluster,                 model=\"clayton.oakes\",theta.des=tdes2,step=0.5) summary(fitp3) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>                            log-Coef.         SE         z        P-val #> factor(stratas)0-0.5,0-0.5 0.8276088 0.06812882 12.147705 0.000000e+00 #> factor(stratas)0-0.5,0.5-2 0.4110824 0.08945478  4.595422 4.318746e-06 #> factor(stratas)0.5-2,0.5-2 0.6903006 0.05740192 12.025741 0.000000e+00 #>                            Kendall tau         SE #> factor(stratas)0-0.5,0-0.5   0.5335648 0.01695545 #> factor(stratas)0-0.5,0.5-2   0.4299477 0.02192471 #> factor(stratas)0.5-2,0.5-2   0.4992884 0.01435045 #>  #> $vargam #>             Estimate Std.Err  2.5% 97.5%   P-value #> dependence1    2.288  0.1559 1.982 2.593 8.908e-49 #> dependence2    1.508  0.1349 1.244 1.773 5.177e-29 #> dependence3    1.994  0.1145 1.770 2.219 5.715e-68 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  ### same model using strata option, a bit slower fitp4 <- survival.twostage(marg2,data=ud,clusters=ud$cluster,se.cluster=ud$cluster,                 model=\"clayton.oakes\",theta.des=tdes2,step=0.5,strata=ud$strata) summary(fitp4) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>                            log-Coef.         SE         z        P-val #> factor(stratas)0-0.5,0-0.5 0.8276088 0.06812882 12.147705 0.000000e+00 #> factor(stratas)0-0.5,0.5-2 0.4110824 0.08945478  4.595422 4.318746e-06 #> factor(stratas)0.5-2,0.5-2 0.6903006 0.05740192 12.025741 0.000000e+00 #>                            Kendall tau         SE #> factor(stratas)0-0.5,0-0.5   0.5335648 0.01695545 #> factor(stratas)0-0.5,0.5-2   0.4299477 0.02192471 #> factor(stratas)0.5-2,0.5-2   0.4992884 0.01435045 #>  #> $vargam #>             Estimate Std.Err  2.5% 97.5%   P-value #> dependence1    2.288  0.1559 1.982 2.593 8.908e-49 #> dependence2    1.508  0.1349 1.244 1.773 5.177e-29 #> dependence3    1.994  0.1145 1.770 2.219 5.715e-68 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"    ## Reduce Ex.Timings ### structured random effects model additive gamma ACE ### simulate structured two-stage additive gamma ACE model data <- simClaytonOakes.twin.ace(4000,2,1,0,3) out <- twin.polygen.design(data,id=\"cluster\") pardes <- out$pardes pardes #>      [,1] [,2] #> [1,]  1.0    0 #> [2,]  0.5    0 #> [3,]  0.5    0 #> [4,]  0.5    0 #> [5,]  0.0    1 des.rv <- out$des.rv head(des.rv) #>   MZ DZ DZns1 DZns2 env #> 1  1  0     0     0   1 #> 2  1  0     0     0   1 #> 3  1  0     0     0   1 #> 4  1  0     0     0   1 #> 5  1  0     0     0   1 #> 6  1  0     0     0   1 aa <- phreg(Surv(time,status)~x+cluster(cluster),data=data,robust=0) ts <- survival.twostage(aa,data=data,clusters=data$cluster,detail=0,          theta=c(2,1),var.link=0,step=0.5,          random.design=des.rv,theta.des=pardes) summary(ts) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                Coef.        SE         z P-val Kendall tau         SE #> dependence1 1.886703 0.1355750 13.916305     0    0.485425 0.01794927 #> dependence2 1.018832 0.1090384  9.343785     0    0.337492 0.02392940 #>  #> $type #> [1] \"clayton.oakes\" #>  #> $h #>      Estimate Std.Err   2.5%  97.5%   P-value #> [1,]   0.6493 0.03797 0.5749 0.7238 1.416e-65 #> [2,]   0.3507 0.03797 0.2762 0.4251 2.569e-20 #>  #> $vare #> NULL #>  #> $vartot #>    Estimate Std.Err  2.5% 97.5%    P-value #> p1    2.906 0.09363 2.722 3.089 1.902e-211 #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/reference/survivalG.html","id":null,"dir":"Reference","previous_headings":"","what":"G-estimator for Cox and Fine-Gray model — survivalG","title":"G-estimator for Cox and Fine-Gray model — survivalG","text":"Computes G-estimator $$ \\hat S(t,=) = n^{-1} \\sum_i \\hat S(t,=,Z_i) $$ Cox model based phreg og Fine-Gray model based cifreg function. Gives influence functions risk estimates SE's based  .  first covariate factor contrast computed, continuous considered covariate values given Avalues.","code":""},{"path":"http://kkholst.github.io/mets/reference/survivalG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"G-estimator for Cox and Fine-Gray model — survivalG","text":"","code":"survivalG(   x,   data,   time = NULL,   Avalues = NULL,   varname = NULL,   same.data = TRUE,   First = FALSE )"},{"path":"http://kkholst.github.io/mets/reference/survivalG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"G-estimator for Cox and Fine-Gray model — survivalG","text":"x phreg cifreg object data data frame risk averaging, must part data used fitting unless .data=FALSE. subset data treated model fitted cluster(id) time estimate Avalues values compare first covariate varname given averages variable, default first variable .data assumes data used fitting survival model averaging. First use first record G-averaging, example start,stop structure used phreg","code":""},{"path":"http://kkholst.github.io/mets/reference/survivalG.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"G-estimator for Cox and Fine-Gray model — survivalG","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/survivalG.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"G-estimator for Cox and Fine-Gray model — survivalG","text":"","code":"library(mets) data(bmt); bmt$time <- bmt$time+runif(408)*0.001 bmt$event <- (bmt$cause!=0)*1; bmt$id <- 1:408 dfactor(bmt) <- tcell.f~tcell  fg1 <- cifreg(Event(time,cause)~tcell.f+platelet+age,bmt,cause=1,               cox.prep=TRUE,propodds=NULL) summary(survivalG(fg1,bmt,50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.4332 0.02749 0.3793 0.4870 6.316e-56 #> risk1   0.2726 0.05861 0.1577 0.3875 3.297e-06 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err   2.5%    97.5% P-value #> ps0  -0.1605  0.0635 -0.285 -0.03607 0.01147 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%       97.5%    P-value #> [ps0] -0.4630061 0.2211506 -0.8964533 -0.02955888 0.03629353 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.6293888 0.4080142 0.9708737  #>   ss <- phreg(Surv(time,event)~tcell.f+platelet+age,bmt)  summary(survivalG(ss,bmt,50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%    P-value #> risk0   0.6539 0.02708 0.6008 0.7069 8.837e-129 #> risk1   0.5639 0.05971 0.4469 0.6810  3.573e-21 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err    2.5%   97.5% P-value #> ps0 -0.08992 0.06291 -0.2132 0.03338  0.1529 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%      97.5%   P-value #> [ps0] -0.1479471 0.1095497 -0.3626606 0.06676628 0.1768548 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.8624767 0.6958226 1.0690456  #>  #> Average Treatment effect:  survival-difference (G-estimator) : #>       Estimate   Std.Err        2.5%     97.5%   P-value #> ps0 0.08992126 0.0629098 -0.03337969 0.2132222 0.1528985 #>  #> Average Treatment effect: 1-G (survival)-ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err        2.5%     97.5%   P-value #> [ps0] 0.2309406 0.1503721 -0.06378329 0.5256645 0.1245888 #> ratio:  #>  Estimate      2.5%     97.5%  #> 1.2597844 0.9382083 1.6915826  #>   ss <- phreg(Surv(time,event)~strata(tcell.f)+platelet+age,bmt)  summary(survivalG(ss,bmt,50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%    P-value #> risk0   0.6441 0.02727 0.5906 0.6975 2.397e-123 #> risk1   0.6172 0.07125 0.4776 0.7568  4.611e-18 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err    2.5%  97.5% P-value #> ps0 -0.02687 0.07622 -0.1763 0.1225  0.7244 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>          Estimate   Std.Err       2.5%     97.5%  P-value #> [ps0] -0.04261856 0.1228491 -0.2833984 0.1981613 0.728653 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.9582769 0.7532197 1.2191590  #>  #> Average Treatment effect:  survival-difference (G-estimator) : #>       Estimate    Std.Err      2.5%    97.5%   P-value #> ps0 0.02687251 0.07621848 -0.122513 0.176258 0.7244093 #>  #> Average Treatment effect: 1-G (survival)-ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%     97.5%   P-value #> [ps0] 0.07278456 0.2010782 -0.3213214 0.4668906 0.7173734 #> ratio:  #>  Estimate      2.5%     97.5%  #> 1.0754988 0.7251901 1.5950269  #>   sst <- survivalGtime(ss,bmt,n=50) #> Warning: NaNs produced #> Warning: NaNs produced #> Warning: NaNs produced #> Warning: NaNs produced plot(sst)   fg1t <- survivalGtime(fg1,bmt,n=50) plot(fg1t)   #among treated: must specify id to link influence functions ss <- phreg(Surv(time,event)~tcell.f+platelet+age+cluster(id),bmt)  summary(survivalG(ss,subset(bmt,tcell==1),50)) #> G-estimator : #>       Estimate Std.Err   2.5%  97.5%   P-value #> risk0   0.6662 0.03407 0.5995 0.7330 3.661e-85 #> risk1   0.5749 0.05749 0.4622 0.6876 1.518e-23 #>  #> Average Treatment effect: difference (G-estimator) : #>     Estimate Std.Err    2.5%   97.5% P-value #> ps0 -0.09134 0.06417 -0.2171 0.03442  0.1546 #>  #> Average Treatment effect: ratio (G-estimator) : #> log-ratio:  #>         Estimate   Std.Err       2.5%      97.5%   P-value #> [ps0] -0.1474634 0.1081918 -0.3595155 0.06458876 0.1728887 #> ratio:  #>  Estimate      2.5%     97.5%  #> 0.8628941 0.6980145 1.0667203  #>  #> Average Treatment effect:  survival-difference (G-estimator) : #>       Estimate    Std.Err        2.5%     97.5%   P-value #> ps0 0.09134406 0.06416628 -0.03441954 0.2171077 0.1545761 #>  #> Average Treatment effect: 1-G (survival)-ratio (G-estimator) : #> log-ratio:  #>        Estimate   Std.Err        2.5%     97.5%   P-value #> [ps0] 0.2419052 0.1620237 -0.07565548 0.5594659 0.1354311 #> ratio:  #>  Estimate      2.5%     97.5%  #> 1.2736735 0.9271356 1.7497378  #>"},{"path":"http://kkholst.github.io/mets/reference/test.conc.html","id":null,"dir":"Reference","previous_headings":"","what":"Concordance test Compares two concordance estimates — test.conc","title":"Concordance test Compares two concordance estimates — test.conc","text":".. content description (empty lines) ..","code":""},{"path":"http://kkholst.github.io/mets/reference/test.conc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concordance test Compares two concordance estimates — test.conc","text":"","code":"test.conc(conc1, conc2, same.cluster = FALSE)"},{"path":"http://kkholst.github.io/mets/reference/test.conc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concordance test Compares two concordance estimates — test.conc","text":"conc1 Concordance estimate group 1 conc2 Concordance estimate group 2 .cluster FALSE groups independent, otherwise estimates based data.","code":""},{"path":"http://kkholst.github.io/mets/reference/test.conc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Concordance test Compares two concordance estimates — test.conc","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/tetrachoric.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate parameters from odds-ratio — tetrachoric","title":"Estimate parameters from odds-ratio — tetrachoric","text":"Calculate tetrachoric correlation probabilities odds-ratio","code":""},{"path":"http://kkholst.github.io/mets/reference/tetrachoric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate parameters from odds-ratio — tetrachoric","text":"","code":"tetrachoric(P, OR, approx = 0, ...)"},{"path":"http://kkholst.github.io/mets/reference/tetrachoric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate parameters from odds-ratio — tetrachoric","text":"P Joint probabilities marginals (given) Odds-ratio approx TRUE approximation tetrachoric correlation used ... Additional arguments","code":""},{"path":"http://kkholst.github.io/mets/reference/tetrachoric.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate parameters from odds-ratio — tetrachoric","text":"","code":"tetrachoric(0.3,1.25) # Marginal p1=p2=0.3, OR=2 #> [1] 0.08173353 P <- matrix(c(0.1,0.2,0.2,0.5),2) prod(diag(P))/prod(lava::revdiag(P)) #> [1] 1.25 ##mets:::assoc(P) tetrachoric(P) #> [1] 0.08173353 or2prob(2,0.1) #>            [,1]       [,2] #> [1,] 0.01690481 0.08309519 #> [2,] 0.08309519 0.81690481 #> attr(,\"marg\") #> [1] 0.1 0.1 or2prob(2,c(0.1,0.2)) #>            [,1]      [,2] #> [1,] 0.03153416 0.1684658 #> [2,] 0.06846584 0.7315342 #> attr(,\"marg\") #> [1] 0.1 0.2"},{"path":"http://kkholst.github.io/mets/reference/ttpd.html","id":null,"dir":"Reference","previous_headings":"","what":"ttpd discrete survival data on interval form — ttpd","title":"ttpd discrete survival data on interval form — ttpd","text":"ttpd discrete survival data interval form","code":""},{"path":"http://kkholst.github.io/mets/reference/ttpd.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ttpd discrete survival data on interval form — ttpd","text":"Simulated data","code":""},{"path":"http://kkholst.github.io/mets/reference/twin.clustertrunc.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","title":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","text":"Estimation twostage model cluster truncation bivariate situation","code":""},{"path":"http://kkholst.github.io/mets/reference/twin.clustertrunc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","text":"","code":"twin.clustertrunc(   survformula,   data = parent.frame(),   theta.des = NULL,   clusters = NULL,   var.link = 1,   Nit = 10,   final.fitting = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/twin.clustertrunc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","text":"survformula Formula survival model aalen cox.aalen, limitiation model specification due call fast.reshape (example interactions * : work , expand prior call) data Data frame theta.des design dependence parameters two-stage model clusters clustering variable twins var.link exp link theta Nit number iteration final.fitting TRUE final estimation SE ... arguments marginal models ... Additional arguments lower level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/twin.clustertrunc.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/twin.clustertrunc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimation of twostage model with cluster truncation in bivariate situation — twin.clustertrunc","text":"","code":"library(\"timereg\") data(diabetes) v <- diabetes$time*runif(nrow(diabetes))*rbinom(nrow(diabetes),1,0.5) diabetes$v <- v  aout <- twin.clustertrunc(Surv(v,time,status)~1+treat+adult,      data=diabetes,clusters=\"id\") aout$two        ## twostage output #>  #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>              log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.1874448 0.3183693 -0.5887653 0.5560188   0.2930551 0.06595778 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%  P-value #> dependence1   0.8291   0.264 0.3117 1.346 0.001684 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" par(mfrow=c(2,2)) plot(aout$marg) ## marginal model output  out <- twin.clustertrunc(Surv(v,time,status)~1+prop(treat)+prop(adult),      data=diabetes,clusters=\"id\") out$two        ## twostage output #>  #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> With log-link  #> $estimates #>               log-Coef.        SE          z     P-val Kendall tau         SE #> dependence1 -0.06020013 0.2998662 -0.2007567 0.8408889   0.3200924 0.06526085 #>  #> $vargam #>             Estimate Std.Err   2.5% 97.5%   P-value #> dependence1   0.9416  0.2823 0.3882 1.495 0.0008535 #>  #> $type #> [1] \"clayton.oakes\" #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\" plot(out$marg) ## marginal model output"},{"path":"http://kkholst.github.io/mets/reference/twinbmi.html","id":null,"dir":"Reference","previous_headings":"","what":"BMI data set — twinbmi","title":"BMI data set — twinbmi","text":"BMI data set","code":""},{"path":"http://kkholst.github.io/mets/reference/twinbmi.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"BMI data set — twinbmi","text":"Self-reported BMI-values 11,411 subjects tvparnr: twin id bmi: BMI (m/kg^2) age: Age gender: (male/female) zyg: zygosity, MZ:=mz, DZ(sex):=dz, DZ(opposite sex):=os","code":""},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":null,"dir":"Reference","previous_headings":"","what":"Classic twin model for quantitative traits — twinlm","title":"Classic twin model for quantitative traits — twinlm","text":"Fits classical twin model quantitative traits.","code":""},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classic twin model for quantitative traits — twinlm","text":"","code":"twinlm(   formula,   data,   id,   zyg,   DZ,   group = NULL,   group.equal = FALSE,   strata = NULL,   weights = NULL,   type = c(\"ace\"),   twinnum = \"twinnum\",   binary = FALSE,   ordinal = 0,   keep = weights,   estimator = NULL,   constrain = TRUE,   control = list(),   messages = 1,   ... )"},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classic twin model for quantitative traits — twinlm","text":"formula Formula specifying effects covariates response data data.frame one observation pr row. addition column zygosity (DZ MZ given factor) individual much specified well twin id variable giving unique pair numbers/factors twin pair id name column dataset containing twin-id variable. zyg name column dataset containing zygosity variable DZ Character defining level zyg variable corresponding dyzogitic twins. argument missing, reference level (.e. first level) interpreted dyzogitic twins group Optional. Variable name defining group interaction analysis (e.g., gender) group.equal TRUE marginals groups asummed strata Strata variable name weights Weights matrix needed chosen estimator. use Inverse Probability Weights type Character defining type analysis performed. Can subset \"aced\" (additive genetic factors, common environmental factors, unique environmental factors, dominant genetic factors). choices : \"0\" (\"sat\"): Saturated model twin 1 twin 2 within twin        pair may different marginal distribution. \"1\" (\"flex\",\"zyg\"): Within twin pairs marginal distribution        , marginal distribution may differ MZ DZ        twins. free correlation structure within MZ DZ twins. \"2\" (\"u\", \"eqmarg\"): individuals marginals        free correlation structure within MZ DZ twins. default value additive polygenic model type=\"ace\". twinnum name column dataset numbering twins (1,2). exist data automatically created. binary TRUE liability model fitted. Note right-hand-side formula factor, character vector, og logical variable, liability model automatically chosen (wrapper bptwin function). ordinal non-zero (number bins) liability model fitted. keep Vector variables data specified formula, added data.frame SEM estimator Choice estimator/model constrain Development argument control Control argument parsed optimization routine messages Control amount messages shown ... Additional arguments parsed lower-level functions","code":""},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classic twin model for quantitative traits — twinlm","text":"Returns object class twinlm.","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Classic twin model for quantitative traits — twinlm","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/twinlm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Classic twin model for quantitative traits — twinlm","text":"","code":"## Simulate data set.seed(1) d <- twinsim(1000,b1=c(1,-1),b2=c(),acde=c(1,1,0,1)) ## E(y|z1,z2) = z1 - z2. var(A) = var(C) = var(E) = 1  ## E.g to fit the data to an ACE-model without any confounders we simply write ace <- twinlm(y ~ 1, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\") ace #>        Estimate Std. Error Z value  Pr(>|z|) #> y     -0.019439   0.041817 -0.4649     0.642 #> sd(A)  0.902004   0.203739  4.4273 9.544e-06 #> sd(C)  1.137025   0.132852  8.5586 < 2.2e-16 #> sd(E)  1.728992   0.037408 46.2194 < 2.2e-16 #>  #> MZ-pairs DZ-pairs  #>     1000     1000  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.15966  0.01867 0.30065 #> C 0.25370  0.13920 0.36820 #> E 0.58664  0.53677 0.63650 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.15966  0.01867 0.30065 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.41336  0.36229 0.46196 #> Correlation within DZ: 0.33353  0.27933 0.38561 #>  #> 'log Lik.' -8779.953 (df=4) #> AIC: 17567.91  #> BIC: 17590.31  ## An AE-model could be fitted as ae <- twinlm(y ~ 1, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ae\") ## LRT: lava::compare(ae,ace) #>  #> \t- Likelihood ratio test - #>  #> data:   #> chisq = 17.207, df = 1, p-value = 3.353e-05 #> sample estimates: #> log likelihood (model 1) log likelihood (model 2)  #>                -8788.556                -8779.953  #>  ## AIC AIC(ae)-AIC(ace) #> [1] 15.20656 ## To adjust for the covariates we simply alter the formula statement ace2 <- twinlm(y ~ x1+x2, data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ace\") ## Summary/GOF summary(ace2) #>        Estimate Std. Error  Z value Pr(>|z|) #> y     -0.026049   0.034844  -0.7476   0.4547 #> sd(A)  1.066060   0.072890  14.6256   <2e-16 #> sd(C)  0.980740   0.073569  13.3309   <2e-16 #> sd(E)  0.979980   0.021887  44.7736   <2e-16 #> y~x1   1.006963   0.021900  45.9807   <2e-16 #> y~x2  -0.993802   0.021962 -45.2512   <2e-16 #>  #> MZ-pairs DZ-pairs  #>     1000     1000  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.37156  0.27300 0.47012 #> C 0.31446  0.22643 0.40250 #> E 0.31398  0.28381 0.34414 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.37156  0.27300 0.47012 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.68602  0.65467 0.71502 #> Correlation within DZ: 0.50024  0.45538 0.54257 #>  #> 'log Lik.' -7449.697 (df=6) #> AIC: 14911.39  #> BIC: 14945   ## Reduce Ex.Timings ## An interaction could be analyzed as: ace3 <- twinlm(y ~ x1+x2 + x1:I(x2<0), data=d, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ace\") ace3 #>                     Estimate Std. Error  Z value Pr(>|z|) #> y                  -0.026089   0.034847  -0.7487   0.4541 #> sd(A)               1.065529   0.072975  14.6012   <2e-16 #> sd(C)               0.981354   0.073598  13.3340   <2e-16 #> sd(E)               0.980018   0.021889  44.7711   <2e-16 #> y~x1                1.010637   0.030279  33.3778   <2e-16 #> y~x2               -0.993859   0.021964 -45.2498   <2e-16 #> y~x1:I(x2 < 0)TRUE -0.007626   0.043403  -0.1757   0.8605 #>  #> MZ-pairs DZ-pairs  #>     1000     1000  #>  #> Variance decomposition: #>   Estimate 2.5%    97.5%   #> A 0.37117  0.27253 0.46981 #> C 0.31484  0.22673 0.40296 #> E 0.31399  0.28382 0.34415 #>  #>  #>                          Estimate 2.5%    97.5%   #> Broad-sense heritability 0.37117  0.27253 0.46981 #>  #>                        Estimate 2.5%    97.5%   #> Correlation within MZ: 0.68601  0.65466 0.71501 #> Correlation within DZ: 0.50043  0.45553 0.54279 #>  #> 'log Lik.' -7449.682 (df=7) #> AIC: 14913.36  #> BIC: 14952.57  ## Categorical variables are also supported  d2 <- transform(d,x2cat=cut(x2,3,labels=c(\"Low\",\"Med\",\"High\"))) ace4 <- twinlm(y ~ x1+x2cat, data=d2, DZ=\"DZ\", zyg=\"zyg\", id=\"id\", type=\"ace\")"},{"path":"http://kkholst.github.io/mets/reference/twinsim.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate twin data — twinsim","title":"Simulate twin data — twinsim","text":"Simulate twin data linear normal ACE/ADE/AE model.","code":""},{"path":"http://kkholst.github.io/mets/reference/twinsim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate twin data — twinsim","text":"","code":"twinsim(   nMZ = 100,   nDZ = nMZ,   b1 = c(),   b2 = c(),   mu = 0,   acde = c(1, 1, 0, 1),   randomslope = NULL,   threshold = 0,   cens = FALSE,   wide = FALSE,   ... )"},{"path":"http://kkholst.github.io/mets/reference/twinsim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate twin data — twinsim","text":"nMZ Number monozygotic twin pairs nDZ Number dizygotic twin pairs b1 Effect covariates (labelled x1,x2,...) type 1. One distinct covariate value twin/individual. b2 Effect covariates (labelled g1,g2,...) type 2. One covariate value twin pair. mu Intercept parameter. acde Variance random effects (order ,C,D,E) randomslope Logical indicating wether include random slopes variance components w.r.t. x1,x2,... threshold Treshold used define binary outcome y0 cens Logical variable indicating whether censor outcome wide Logical indicating wide data format returned ... Additional arguments parsed lower-level functions","code":""},{"path":[]},{"path":"http://kkholst.github.io/mets/reference/twinsim.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate twin data — twinsim","text":"Klaus K. Holst","code":""},{"path":"http://kkholst.github.io/mets/reference/twinstut.html","id":null,"dir":"Reference","previous_headings":"","what":"Stutter data set — twinstut","title":"Stutter data set — twinstut","text":"Based nation-wide questionnaire answers 33,317 Danish twins","code":""},{"path":"http://kkholst.github.io/mets/reference/twinstut.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Stutter data set — twinstut","text":"tvparnr: twin-pair id zyg: zygosity, MZ:=mz, DZ(sex):=dz, DZ(opposite sex):=os stutter: stutter status (yes/) age: age nr: number within twin-pair","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":null,"dir":"Reference","previous_headings":"","what":"Twostage survival model fitted by pseudo MLE — twostageMLE","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"Fits Clayton-Oakes clustered  survival data using marginals Cox form likelihood dependence parameter Glidden (2000). dependence can modelled via  Regression design dependence parameter. allow regression structure indenpendent gamma distributed random effects  variances may depend cluster covariates. $$  \\theta = h( z_j^T \\alpha) $$ \\(z\\) specified theta.des . link function can exp var.link=1","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"","code":"twostageMLE(   margsurv,   data = parent.frame(),   theta = NULL,   theta.des = NULL,   var.link = 0,   method = \"NR\",   no.opt = FALSE,   weights = NULL,   se.cluster = NULL,   ... )"},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"margsurv Marginal model phreg data data frame theta Starting values variance components theta.des design dependence parameters, pairs given (pairs) x (numer parameters)  x (max number random effects) matrix var.link Link function variance  1 uses exp link method type opitmizer, default Newton-Raphson \"NR\" .opt optimize, example get score iid specific theta weights cluster specific weights, given length equivalent data-set, weights score equations se.cluster specifies influence functions summed squared computing variance. Note id marginal model used construct MLE, scores can summed se.cluster argument. ... arguments passed  optimizer","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"Measuring early late dependence bivariate twin data Scheike, Holst, Hjelmborg (2015), LIDA Twostage modelling additive gamma frailty models survival data. Scheike Holst, working paper Shih Louis (1995) Inference association parameter copula models bivariate survival data, Biometrics, (1995). Glidden (2000), Two-Stage estimator dependence parameter Clayton Oakes model, LIDA, (2000).","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"Thomas Scheike","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageMLE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Twostage survival model fitted by pseudo MLE — twostageMLE","text":"","code":"data(diabetes) dd <- phreg(Surv(time,status==1)~treat+cluster(id),diabetes) oo <- twostageMLE(dd,data=diabetes) summary(oo) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                 Coef.        SE       z       P-val Kendall tau         SE #> dependence1 0.9526614 0.3543033 2.68883 0.007170289    0.322645 0.08127892 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\"  theta.des <- model.matrix(~-1+factor(adult),diabetes)  oo <-twostageMLE(dd,data=diabetes,theta.des=theta.des) summary(oo) #> Dependence parameter for Clayton-Oakes model #> Variance of Gamma distributed random effects  #> $estimates #>                    Coef.        SE        z      P-val Kendall tau         SE #> factor(adult)1 0.9117633 0.4000030 2.279391 0.02264381   0.3131310 0.09435851 #> factor(adult)2 1.0570600 0.7014182 1.507032 0.13180233   0.3457767 0.15010636 #>  #> $type #> NULL #>  #> attr(,\"class\") #> [1] \"summary.mets.twostage\""},{"path":"http://kkholst.github.io/mets/reference/twostageREC.html","id":null,"dir":"Reference","previous_headings":"","what":"Fittting of Two-stage recurrent events random effects model based on Cox/Cox or Cox/Ghosh-Lin marginals — twostageREC","title":"Fittting of Two-stage recurrent events random effects model based on Cox/Cox or Cox/Ghosh-Lin marginals — twostageREC","text":"Fittting Two-stage recurrent events random effects model based Cox/Cox Cox/Ghosh-Lin marginals. Random effects model fore recurrent events terminal  event. Marginal models fitted first given twostageREC function.","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageREC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fittting of Two-stage recurrent events random effects model based on Cox/Cox or Cox/Ghosh-Lin marginals — twostageREC","text":"","code":"twostageREC(   margsurv,   recurrent,   data = parent.frame(),   theta = NULL,   model = c(\"full\", \"shared\", \"non-shared\"),   ghosh.lin = NULL,   theta.des = NULL,   var.link = 0,   method = \"NR\",   no.opt = FALSE,   weights = NULL,   se.cluster = NULL,   fnu = NULL,   nufix = 0,   nu = NULL,   numderiv = 1,   derivmethod = c(\"simple\", \"Richardson\"),   ... )"},{"path":"http://kkholst.github.io/mets/reference/twostageREC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fittting of Two-stage recurrent events random effects model based on Cox/Cox or Cox/Ghosh-Lin marginals — twostageREC","text":"margsurv marginal model terminal event recurrent marginal model recurrent events data used fitting theta starting value total variance gamma frailty model can fully shared \"full\", partly shared \"shared\", non-shared random effect acts recurrent events ghosh.lin force use ghosh.lin marginals based recurrent (taking baseline coefficients) theta.des regression design variance var.link possible link  function 1 exponential link method NR .opt optimize weights possible weights se.cluster combine influence functions naive variance based clusters GEE style fnu function make transformation nu (amount shared) nufix fix amount shared nu starting value amount shared numderiv uses numerical derivatives derivatives derivmethod method numerical derivative ... arguments ","code":""},{"path":"http://kkholst.github.io/mets/reference/twostageREC.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fittting of Two-stage recurrent events random effects model based on Cox/Cox or Cox/Ghosh-Lin marginals — twostageREC","text":"Scheike (2026), Two-stage recurrent events random effects models, LIDA, appear","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-139","dir":"Changelog","previous_headings":"","what":"mets 1.3.9","title":"mets 1.3.9","text":"Vignette updates phreg_weibull: allows regression design rate shape parameter. Replaces phreg.par Event methods updated","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-138","dir":"Changelog","previous_headings":"","what":"mets 1.3.8","title":"mets 1.3.8","text":"CRAN release: 2025-10-21 Updated vignette simulation survival data percentage years lost due cause regression rmtlRatio, binregRatio","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-137","dir":"Changelog","previous_headings":"","what":"mets 1.3.7","title":"mets 1.3.7","text":"CRAN release: 2025-08-30 -alive estimation : WA_recurrent marks handle composite outcomes renaming IIDbaseline iidBaseline making method phreg/cifreg/recreg rownames influence functions ratioBinreg percentage regression RMTL due cause plot_twin function","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-136","dir":"Changelog","previous_headings":"","what":"mets 1.3.6","title":"mets 1.3.6","text":"CRAN release: 2025-04-23 -alive estimation : WA_recurrent Marks medical cost models : recreg recregIPCW New default augmentation binreg resmeanIPCW type=“II”, type=“” simple outcome IPCW plot, summary, predict functions New call recurentMarginal based formula, old version recurentMarginalPhreg","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-135","dir":"Changelog","previous_headings":"","what":"mets 1.3.5","title":"mets 1.3.5","text":"CRAN release: 2025-01-11 sim.phreg sim.recurrent simulations Combining binreg, binregATE resmeanIPCW resmeanATE","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-134","dir":"Changelog","previous_headings":"","what":"mets 1.3.4","title":"mets 1.3.4","text":"CRAN release: 2024-02-16 Maintenance release","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-133","dir":"Changelog","previous_headings":"","what":"mets 1.3.3","title":"mets 1.3.3","text":"CRAN release: 2023-12-04 Lu-Tsiatis efficient logrank test dynamic censoring augmentation: phreg_rct Inverse Probability treatment weighted Cox model: phreg_IPTW Twostage randomization survival outcome: binregTSR","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-132","dir":"Changelog","previous_headings":"","what":"mets 1.3.2","title":"mets 1.3.2","text":"CRAN release: 2023-01-17 Extension recreg (Ghosh-Lin model) deal composite outcomes. Recurrent events regression IPCW adjustment fixed time point: recregIPCW Efficient Ghosh-Lin modelling using dynamic regression augmentation.","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-131","dir":"Changelog","previous_headings":"","what":"mets 1.3.1","title":"mets 1.3.1","text":"CRAN release: 2022-10-02 Maintenance release","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-130","dir":"Changelog","previous_headings":"","what":"mets 1.3.0","title":"mets 1.3.0","text":"CRAN release: 2022-09-05 Efficient IPCW binary data: Effbinreg IPCW restricted mean survival regression: resmeanIPCW Lin-Ying additive hazards model fast robust standard errors: aalenMets mediator weighted survival mediation robust standard errors: mediatorSurv Examples updated dutility function longer casts warnings handling formulas Efficient estimation recurrent events mean: recurrentMarginalAIPCW Average treatment effect competing risks binary data: logitATE, binregATE Recurrent events regression IPCW adjustment (Ghosh-Lin model) : recreg","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-1281","dir":"Changelog","previous_headings":"","what":"mets 1.2.8.1","title":"mets 1.2.8.1","text":"CRAN release: 2020-09-28 Maintenance release","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-128","dir":"Changelog","previous_headings":"","what":"mets 1.2.8","title":"mets 1.2.8","text":"CRAN release: 2020-08-27 Augmentation binomial regression model: BinAugmentCifstrata Augmentation Fine-Gray model: FG_AugmentCifstrata Double Fine-Gray model two causes. Likelihood evaluation mvn uses Moore-Penrose pseudo-inverese (threshold set via lava.options(itol=...) Vignette updates","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-1271","dir":"Changelog","previous_headings":"","what":"mets 1.2.7.1","title":"mets 1.2.7.1","text":"CRAN release: 2020-03-04 Maintenance release","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-126","dir":"Changelog","previous_headings":"","what":"mets 1.2.6","title":"mets 1.2.6","text":"CRAN release: 2019-08-05 Fine-Gray model cloglog link (1-F_1(t,x)) Logit link confidence bands baseline confidenence bands cumulative incidence two cox’s Piecewise constant hazard: rpch, ppch Test-version multinomial regression model (via phreg): mlogit Simulation illness-death model: simMultistate Haplotype modelling discrete time--pregnancy models: haplo.surv.discrete Interval censoring discrete time logit-survival model: interval.logitsurv.discrete Binomial Regression competing risks data censoring one time point : binreg","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-125","dir":"Changelog","previous_headings":"","what":"mets 1.2.5","title":"mets 1.2.5","text":"CRAN release: 2018-11-20 plotting functionality robust standard errors phreg robust se’s marginal Cox model twostage survival model multivariate competing risks recurrent events gof robust standard errors clustered case twostageMLE fast twostage fitting clustered survival data robust standard errors. standard errors twostage models now also uncertainty Cox baseline cumulative score process test gof now also marginal Cox models","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-124","dir":"Changelog","previous_headings":"","what":"mets 1.2.4","title":"mets 1.2.4","text":"CRAN release: 2018-08-13 functions km (Kaplan-Meier) cif (cumulative incidence probability) robust standard errors. computation probability exceeding “k” events recurrents processs computation probability exceeding “k1” “k2” events bivariate recurrents processseses dspline simple spline decomposition data frame rmvn, dmvn: RNG density multivariate normal distribution varying correlation coefficients.","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-1231","dir":"Changelog","previous_headings":"","what":"mets 1.2.3.1","title":"mets 1.2.3.1","text":"CRAN release: 2018-04-30 starting values updated twinlm method","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-123","dir":"Changelog","previous_headings":"","what":"mets 1.2.3","title":"mets 1.2.3","text":"CRAN release: 2018-02-09 twinlm now supports ordinal outcomes optimized strata calculations phreg optimized robust standard errors phreg weights offsets phreg weights argument added lifetable gof phreg fast cumulative residuals (Lin, Wei, Ying) graphical gof phreg recurrent events function marginal mean standard errors simulating recurrent events possibly two recurrent events death covariance calculation recurrent events data related bootstrap","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-122","dir":"Changelog","previous_headings":"","what":"mets 1.2.2","title":"mets 1.2.2","text":"CRAN release: 2017-03-30 Vignettes updated Compatibility lava version 1.5","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-121","dir":"Changelog","previous_headings":"","what":"mets 1.2.1","title":"mets 1.2.1","text":"CRAN release: 2017-02-26 New documentation/vignettes Additional examples unit tests lifecourse plot function: lifecourse block sampling function: dsample","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-120","dir":"Changelog","previous_headings":"","what":"mets 1.2.0","title":"mets 1.2.0","text":"Namespace cleaning (twostage)… Dependency R>=3.3 radix algorithm Case-Control sampling twostage model. Two-stage additive gamma survival model. Additive random effects two-stage survival model via pairwise composite likelihood. Simulation family ace survival model. Function computing Kendall’s tau pairs additive gamma random effects model via simulations. Two-stage additive gamma binomial model. Additive random effects binomial model via pairwise composite likelihood. Simulation family ace model. Function computing pairwise concordance pairs additive gamma random effects model. Updated divide.conquer Extra unit tests force..cens argument IPWC methods dsort dreshape dcut drm, drename, ddrop, dkeep, dsubset drelevel dlag dfactor, dnumeric Data aggregation dby, dby2 dscalar, deval, daggregate dmean, dsd, dsum, dquantile, dcor dtable, dcount Data summaries dhead, dtail, dsummary, dprint, dlist, dlevels, dunique","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-111","dir":"Changelog","previous_headings":"","what":"mets 1.1.1","title":"mets 1.1.1","text":"CRAN release: 2015-05-29 Support left-truncation","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-110","dir":"Changelog","previous_headings":"","what":"mets 1.1.0","title":"mets 1.1.0","text":"CRAN release: 2015-02-16 fast.approx ‘type’ argument scoreMV lifetable updated new survpois function (piecewise constant hazard)","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-10","dir":"Changelog","previous_headings":"","what":"mets 1.0","title":"mets 1.0","text":"CRAN release: 2014-11-18 New functions biprobit.time, binomial.twostage.time. Automatically samples time points (approximately equidistant) last double jump time. Intial support left truncation. contrast argument added biprobit.time. ipw removed (namespace) biprobit optimized tabular data (non-continuous covariates). Regression design dependence parameter (tetrachoric correlation) now possible. predict method implemented biprobit arc-sinus transformation used probability estimates updated output bptwin relative recurrence risk + log-estimates iid method bptwin (influence function) survival probabilities start end intervals added lifetable new function ‘jumptimes’ extracting jump times possibly sample (equidistant) fast.pattern updated handle two categories demos added mets package divide.conquer function, folds function","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-028","dir":"Changelog","previous_headings":"","what":"mets 0.2.8","title":"mets 0.2.8","text":"CRAN release: 2014-05-13 Normal orthant probabilities via ‘pmvn’ (vectorized) Parametric proportional hazards models via ‘phreg.par’ twinlm.time function censored twin data. Wraps ‘ipw’ function now also supports parametric survival models via phreg.par. ‘grouptable’ tabulating twin-data. Relative recurrence risk ratios now reported bptwin/twinlm. Grandom.cif stable","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-027","dir":"Changelog","previous_headings":"","what":"mets 0.2.7","title":"mets 0.2.7","text":"CRAN release: 2014-02-20 Adapted changes ‘timereg::comp.risk’ cluster.index ‘mat’ argument stacking rows matrix according cluster-variable New lava-estimator: ‘normal’, ordinal data (cumulative probit) fast.reshape robust. Now also supports ‘varying arguments type ’varying=-c(…)’ choosing everything except ‘…’.","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-026","dir":"Changelog","previous_headings":"","what":"mets 0.2.6","title":"mets 0.2.6","text":"CRAN release: 2013-12-17 C++ source code cleanup Optimization fast.reshape","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-025","dir":"Changelog","previous_headings":"","what":"mets 0.2.5","title":"mets 0.2.5","text":"CRAN release: 2013-12-16 New datasets: dermalridges, dermalridgesMZ Grouped analysis updated twinlm (e.g. sex limitation model) Confidence limits genetic environmental effects now based standard (symmetric) Wald confidence limits. (use ‘transform’ argument summary method apply logit-transform) Improved output twinlm","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-024","dir":"Changelog","previous_headings":"","what":"mets 0.2.4","title":"mets 0.2.4","text":"CRAN release: 2013-07-13 fast.reshape :labelnum option wide long format (see example) Compilation flags removed Makevars files","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-023","dir":"Changelog","previous_headings":"","what":"mets 0.2.3","title":"mets 0.2.3","text":"CRAN release: 2013-05-22 fast.reshape bug-fix (column names)","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-022","dir":"Changelog","previous_headings":"","what":"mets 0.2.2","title":"mets 0.2.2","text":"CRAN release: 2013-05-21 Better starting values twinlm Fixed claytonaokes.cpp New fast cox ph regression: phreg Updated two-stage estimator Improved fast.reshape","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-021","dir":"Changelog","previous_headings":"","what":"mets 0.2.1","title":"mets 0.2.1","text":"CRAN release: 2013-04-24 fast.reshape easy.binomial.twostage","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-014","dir":"Changelog","previous_headings":"","what":"mets 0.1.4","title":"mets 0.1.4","text":"Fixed cor.cpp New datasets: twinstut, twinbmi, prtsim","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-013","dir":"Changelog","previous_headings":"","what":"mets 0.1.3","title":"mets 0.1.3","text":"twinlm moved mets package, wraps bptwin function","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-012","dir":"Changelog","previous_headings":"","what":"mets 0.1.2","title":"mets 0.1.2","text":"code clean-minor bug-fixes","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-011","dir":"Changelog","previous_headings":"","what":"mets 0.1.1","title":"mets 0.1.1","text":"Random effects CIF models moved MultiComp mets new data sets: np, multcif Documentation via roxygen2 bug fixes","code":""},{"path":"http://kkholst.github.io/mets/news/index.html","id":"mets-010","dir":"Changelog","previous_headings":"","what":"mets 0.1.0","title":"mets 0.1.0","text":"Initialization new package ‘mets’ implementation Clayton-Oakes model piecewise constant marginal hazards, bivariate probit random effects model (Liability model) twin-data.","code":""}]
